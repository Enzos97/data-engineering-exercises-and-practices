# Ejercicio 10 - Pr√°ctica Northwind: Airflow + Sqoop + Hive + Spark

Este ejercicio integra **Apache Airflow** para orquestaci√≥n de workflows, **Apache Sqoop** para ingesti√≥n de datos desde PostgreSQL, **Apache Hive** para almacenamiento de datos estructurados y **Apache Spark** para procesamiento distribuido, utilizando datos reales de la base de datos Northwind.

## üéØ Objetivos

- Crear base de datos en Hive para almacenamiento de datos de Northwind
- Desarrollar scripts de automatizaci√≥n para ingesta de datos desde PostgreSQL con Sqoop
- Procesar datos con Spark para generar an√°lisis espec√≠ficos
- Orquestar todo el pipeline con Apache Airflow usando TaskGroups
- Implementar un flujo completo de ETL automatizado

## üìã Ejercicios Incluidos

### 1Ô∏è‚É£ **Crear Base de Datos en Hive**
- Crear base de datos `northwind_analytics` en Hive
- Configurar ubicaci√≥n de almacenamiento en HDFS
- Verificar la creaci√≥n correcta

### 2Ô∏è‚É£ **Script Sqoop: Importar Clientes**
- Crear script bash para importar datos de clientes con productos vendidos
- Realizar JOIN entre tablas `customers`, `orders` y `order_details`
- Campos: `customer_id`, `company_name`, `productos_vendidos`
- Formato: Parquet con compresi√≥n Snappy
- Destino: `/sqoop/ingest/customers`
- Password almacenada en archivo seguro

### 3Ô∏è‚É£ **Script Sqoop: Importar Env√≠os**
- Crear script bash para importar datos de √≥rdenes con informaci√≥n de empresa
- Realizar JOIN entre tablas `orders` y `customers`
- Campos: `order_id`, `shipped_date`, `company_name`, `phone`
- Formato: Parquet con compresi√≥n Snappy
- Destino: `/sqoop/ingest/envios`
- Password almacenada en archivo seguro

### 4Ô∏è‚É£ **Script Sqoop: Importar Detalles de √ìrdenes**
- Crear script bash para importar detalles de √≥rdenes
- Tabla: `order_details`
- Campos: `order_id`, `unit_price`, `quantity`, `discount`
- Formato: Parquet con compresi√≥n Snappy
- Destino: `/sqoop/ingest/order_details`
- Password almacenada en archivo seguro

### 5Ô∏è‚É£ **Script Spark: Procesar Products Sold**
- Desarrollar script Python para procesamiento de datos de clientes
- Filtrar compa√±√≠as con productos vendidos mayor al promedio
- Insertar resultados en tabla Hive `products_sold`
- Base de datos: `northwind_analytics`

### 6Ô∏è‚É£ **Script Spark: Procesar Products Sent**
- Desarrollar script Python para procesamiento de √≥rdenes con descuento
- Realizar JOIN entre datos de env√≠os y detalles
- Calcular `unit_price_discount` y `total_price`
- Insertar resultados en tabla Hive `products_sent`
- Base de datos: `northwind_analytics`

### 7Ô∏è‚É£ **Orquestaci√≥n con Airflow**
- Crear DAG para automatizaci√≥n del pipeline completo
- Implementar TaskGroups para organizar etapas:
  - **Grupo Ingest**: Importaci√≥n de datos con Sqoop
  - **Grupo Process**: Procesamiento de datos con Spark
  - **Grupo Verify**: Verificaci√≥n de resultados en Hive
- Configurar dependencias y flujo de ejecuci√≥n
- Monitorear ejecuci√≥n del workflow

## üìÅ Estructura del Proyecto

```
ejercicio-10-practica-northwind-airflow-sqoop-spark/
‚îú‚îÄ‚îÄ README.md                           # Documentaci√≥n principal
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ sqoop_import_clientes.sh       # Script Sqoop para clientes
‚îÇ   ‚îú‚îÄ‚îÄ sqoop_import_envios.sh         # Script Sqoop para env√≠os
‚îÇ   ‚îú‚îÄ‚îÄ sqoop_import_order_details.sh  # Script Sqoop para detalles
‚îÇ   ‚îú‚îÄ‚îÄ spark_products_sold.py         # Procesamiento Spark de clientes
‚îÇ   ‚îú‚îÄ‚îÄ spark_products_sent.py         # Procesamiento Spark de env√≠os
‚îÇ   ‚îî‚îÄ‚îÄ README.md                      # Documentaci√≥n de scripts
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îú‚îÄ‚îÄ northwind_processing.py        # DAG de Airflow
‚îÇ   ‚îî‚îÄ‚îÄ README.md                      # Documentaci√≥n de Airflow
‚îú‚îÄ‚îÄ hive/
‚îÇ   ‚îú‚îÄ‚îÄ northwind-setup.sql            # Scripts SQL de Hive
‚îÇ   ‚îî‚îÄ‚îÄ README.md                      # Documentaci√≥n de Hive
‚îú‚îÄ‚îÄ images/                            # Capturas de pantalla
‚îÇ   ‚îî‚îÄ‚îÄ README.md                      # √çndice de im√°genes
‚îî‚îÄ‚îÄ ejercicios-resueltos.md            # Soluciones completas
```

## üöÄ Tecnolog√≠as Utilizadas

- **Apache Airflow** - Orquestaci√≥n de workflows
- **Apache Sqoop** - Ingesti√≥n de datos desde PostgreSQL
- **Apache Hive** - Data warehouse y consultas SQL
- **Apache Spark** - Procesamiento distribuido de datos
- **PySpark** - API de Python para Spark
- **HDFS** - Sistema de archivos distribuido
- **PostgreSQL** - Base de datos relacional fuente
- **Parquet** - Formato de almacenamiento columnar
- **Bash Scripting** - Automatizaci√≥n de procesos

## üìä Dataset Utilizado

- **Fuente**: Base de datos Northwind (PostgreSQL)
- **Tablas**: 
  - `customers` - Informaci√≥n de clientes
  - `orders` - √ìrdenes de compra
  - `order_details` - Detalles de cada orden
- **Descripci√≥n**: Base de datos de ejemplo cl√°sica para sistemas de gesti√≥n de pedidos

## üîß Requisitos Previos

- Contenedor de Hadoop ejecut√°ndose
- Apache Hive configurado y funcionando
- Apache Spark disponible en el ambiente
- Apache Airflow instalado y configurado
- Apache Sqoop instalado
- PostgreSQL con base de datos Northwind cargada
- Archivo de password en `/home/hadoop/password.txt`
- Conocimientos b√°sicos de SQL, Python y Bash

## üìñ Estructura del Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         INICIO                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ETAPA: INGEST                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ sqoop_import_clientes                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ sqoop_import_envios              (Paralelo)         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ sqoop_import_order_details                          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ETAPA: PROCESS                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ spark_products_sold                                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ spark_products_sent              (Paralelo)         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ETAPA: VERIFY                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ verify_products_sold                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ verify_products_sent             (Paralelo)         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         FIN                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Instrucciones de Ejecuci√≥n

### 1. Preparaci√≥n del Ambiente

```bash
# 1. Conectarse al contenedor Hadoop
docker exec -it edvai_hadoop bash
su hadoop

# 2. Crear archivo de password
echo "edvai" > /home/hadoop/password.txt
chmod 600 /home/hadoop/password.txt

# 3. Crear base de datos en Hive
hive -f /home/hadoop/hive/northwind-setup.sql
```

### 2. Copiar Scripts al Contenedor

```bash
# Copiar scripts al directorio de Hadoop
docker cp scripts/ edvai_hadoop:/home/hadoop/
docker cp airflow/northwind_processing.py edvai_hadoop:/home/hadoop/airflow/dags/

# Dar permisos de ejecuci√≥n
chmod +x /home/hadoop/scripts/*.sh
chmod +x /home/hadoop/scripts/*.py
```

### 3. Ejecutar el DAG en Airflow

```bash
# Activar Airflow webserver (si no est√° activo)
# Acceder a http://localhost:8080

# O ejecutar desde l√≠nea de comandos:
airflow dags trigger northwind_processing
```

### 4. Verificar Resultados

```bash
# Conectarse a Hive
beeline -u jdbc:hive2://localhost:10000

# Consultar resultados
USE northwind_analytics;
SELECT COUNT(*) FROM products_sold;
SELECT COUNT(*) FROM products_sent;
```

## üéØ Resultados Esperados

Al completar este ejercicio, habr√°s:

1. ‚úÖ Configurado una base de datos Hive para an√°lisis de Northwind
2. ‚úÖ Automatizado la ingesta de datos desde PostgreSQL a HDFS con Sqoop
3. ‚úÖ Procesado datos con Spark para generar insights de negocio
4. ‚úÖ Orquestado todo el pipeline con Apache Airflow usando TaskGroups
5. ‚úÖ Verificado la integridad de los datos procesados

## üìä Resultados del An√°lisis

### Tabla: products_sold
- **Contenido**: Clientes con productos vendidos mayor al promedio
- **Registros esperados**: ~33 clientes (de 89 totales)
- **Promedio**: ~24.21 productos vendidos

### Tabla: products_sent
- **Contenido**: Pedidos enviados que tuvieron descuento
- **Registros esperados**: ~803 detalles de pedidos
- **Precio promedio**: ~$627.52

## üìù Notas Importantes

- Los scripts de Sqoop usan archivos Parquet con compresi√≥n Snappy para optimizar almacenamiento
- Las tablas en Hive son manejadas autom√°ticamente por Spark (modo overwrite)
- El DAG de Airflow debe ejecutarse en orden: ingest ‚Üí process ‚Üí verify
- Los TaskGroups permiten ejecutar tareas en paralelo dentro de cada etapa
- La password de PostgreSQL se almacena de forma segura en un archivo con permisos 600

## üîó Referencias

- [Apache Airflow TaskGroups](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html#taskgroups)
- [Apache Sqoop User Guide](https://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html)
- [Apache Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Apache Hive Language Manual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual)

## üìß Contacto

Para consultas o problemas, contactar al equipo de Data Engineering.

---

**Autor**: Hadoop Team  
**Fecha**: 2025-11-12  
**Versi√≥n**: 1.0

