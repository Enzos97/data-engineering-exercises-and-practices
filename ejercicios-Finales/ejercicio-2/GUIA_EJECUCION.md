# ğŸš€ GuÃ­a de EjecuciÃ³n Paso a Paso - Car Rental Analytics

Esta guÃ­a te permite ejecutar y probar cada paso del proyecto de forma individual.

## ğŸ“ Pre-requisitos

```bash
# 1. Verificar que estÃ©s en el contenedor Hadoop
docker exec -it edvai_hadoop bash
su hadoop

# 2. Verificar que los servicios estÃ©n corriendo
jps

# Debe mostrar:
# - NameNode
# - DataNode
# - ResourceManager
# - NodeManager
```

---

## ğŸ”· **PASO 1: Copiar Archivos al Contenedor**

### Desde tu mÃ¡quina local (fuera del contenedor):

```bash
# Navegar al directorio del proyecto
cd ejercicios-Finales/ejercicio-2

# Copiar scripts
docker cp scripts/download_data.sh edvai_hadoop:/home/hadoop/scripts/
docker cp scripts/process_car_rental.py edvai_hadoop:/home/hadoop/scripts/

# Copiar archivos Hive
docker cp hive/car_rental_setup.sql edvai_hadoop:/home/hadoop/hive/
docker cp hive/queries.sql edvai_hadoop:/home/hadoop/hive/

# Copiar DAGs Airflow
docker cp airflow/car_rental_parent_dag.py edvai_hadoop:/home/hadoop/airflow/dags/
docker cp airflow/car_rental_child_dag.py edvai_hadoop:/home/hadoop/airflow/dags/
```

### Verificar archivos copiados:

```bash
# Entrar al contenedor
docker exec -it edvai_hadoop bash
su hadoop

# Verificar archivos
ls -la /home/hadoop/scripts/download_data.sh
ls -la /home/hadoop/scripts/process_car_rental.py
ls -la /home/hadoop/hive/car_rental_setup.sql
ls -la /home/hadoop/hive/queries.sql
ls -la /home/hadoop/airflow/dags/car_rental_parent_dag.py
ls -la /home/hadoop/airflow/dags/car_rental_child_dag.py
```

### Dar permisos de ejecuciÃ³n:

```bash
chmod +x /home/hadoop/scripts/download_data.sh
```

---

## ğŸ”· **PASO 2: Crear Tabla en Hive (Punto 1)**

```bash
# Ejecutar script SQL
hive -f /home/hadoop/hive/car_rental_setup.sql
```

### âœ… Verificar que funcionÃ³:

```bash
# Listar bases de datos
hive -e "SHOW DATABASES;" | grep car_rental

# Listar tablas
hive -e "USE car_rental_db; SHOW TABLES;"

# Ver estructura de la tabla
hive -e "USE car_rental_db; DESCRIBE car_rental_analytics;"
```

### ğŸ“Š Salida esperada:

```
car_rental_db

car_rental_analytics

fueltype                string
rating                  int
rentertripstaken        int
reviewcount             int
city                    string
state_name              string
owner_id                int
rate_daily              int
make                    string
model                   string
year                    int
```

---

## ğŸ”· **PASO 3: Descargar Archivos (Punto 2)**

```bash
# Ejecutar script de descarga
bash /home/hadoop/scripts/download_data.sh
```

### âœ… Verificar que funcionÃ³:

```bash
# Verificar archivos en HDFS
hdfs dfs -ls -h /car_rental/raw/
```

### ğŸ“Š Salida esperada:

```
-rw-r--r--   1 hadoop supergroup      XXX /car_rental/raw/CarRentalData.csv
-rw-r--r--   1 hadoop supergroup      XXX /car_rental/raw/georef_usa_states.csv
```

### Ver preview de datos:

```bash
# Ver primeras lÃ­neas de CarRentalData
hdfs dfs -cat /car_rental/raw/CarRentalData.csv | head -3

# Ver primeras lÃ­neas de georef
hdfs dfs -cat /car_rental/raw/georef_usa_states.csv | head -3
```

---

## ğŸ”· **PASO 4: Procesar Datos con Spark (Punto 3)**

```bash
# Ejecutar procesamiento Spark
spark-submit /home/hadoop/scripts/process_car_rental.py
```

### âœ… Verificar que funcionÃ³:

```bash
# Verificar total de registros en Hive
hive -e "USE car_rental_db; SELECT COUNT(*) as total FROM car_rental_analytics;"
```

### ğŸ“Š Salida esperada:

```
Total MapReduce CPU Time Spent: 0 msec
OK
10000  (o el nÃºmero de registros procesados)
Time taken: X seconds
```

### Ver muestra de datos:

```bash
# Ver primeras 5 filas
hive -e "USE car_rental_db; SELECT * FROM car_rental_analytics LIMIT 5;"
```

---

## ğŸ”· **PASO 5: Verificar Transformaciones**

### 5.1 Verificar que Texas fue excluido:

```bash
hive -e "USE car_rental_db; 
SELECT COUNT(*) as registros_texas 
FROM car_rental_analytics 
WHERE state_name = 'Texas';"
```

**Resultado esperado:** `0`

---

### 5.2 Verificar que no hay rating nulos:

```bash
hive -e "USE car_rental_db; 
SELECT COUNT(*) as rating_nulos 
FROM car_rental_analytics 
WHERE rating IS NULL;"
```

**Resultado esperado:** `0`

---

### 5.3 Verificar que fuelType estÃ¡ en minÃºsculas:

```bash
hive -e "USE car_rental_db; 
SELECT DISTINCT fuelType 
FROM car_rental_analytics 
ORDER BY fuelType;"
```

**Resultado esperado:**
```
diesel
electric
gasoline
hybrid
other
```

---

### 5.4 Ver estadÃ­sticas generales:

```bash
hive -e "USE car_rental_db; 
SELECT 
    'Total registros' as metrica, 
    CAST(COUNT(*) as STRING) as valor 
FROM car_rental_analytics
UNION ALL
SELECT 
    'Estados Ãºnicos', 
    CAST(COUNT(DISTINCT state_name) as STRING) 
FROM car_rental_analytics
UNION ALL
SELECT 
    'Ciudades Ãºnicas', 
    CAST(COUNT(DISTINCT city) as STRING) 
FROM car_rental_analytics
UNION ALL
SELECT 
    'Marcas Ãºnicas', 
    CAST(COUNT(DISTINCT make) as STRING) 
FROM car_rental_analytics;"
```

---

## ğŸ”· **PASO 6: Ejecutar Consultas de Negocio (Punto 5)**

### 5a. Alquileres ecolÃ³gicos con rating >= 4

```bash
hive -e "USE car_rental_db; 
SELECT COUNT(*) as total_alquileres_ecologicos 
FROM car_rental_analytics 
WHERE (fuelType = 'hybrid' OR fuelType = 'electric') 
  AND rating >= 4;"
```

---

### 5b. 5 estados con menor cantidad de alquileres

```bash
hive -e "USE car_rental_db; 
SELECT 
    state_name,
    COUNT(*) as total_alquileres
FROM car_rental_analytics
WHERE state_name IS NOT NULL
GROUP BY state_name
ORDER BY total_alquileres ASC
LIMIT 5;"
```

---

### 5c. 10 modelos mÃ¡s rentados (con marca)

```bash
hive -e "USE car_rental_db; 
SELECT 
    make as marca,
    model as modelo,
    COUNT(*) as total_alquileres
FROM car_rental_analytics
WHERE make IS NOT NULL AND model IS NOT NULL
GROUP BY make, model
ORDER BY total_alquileres DESC
LIMIT 10;"
```

---

### 5d. Alquileres por aÃ±o (2010-2015)

```bash
hive -e "USE car_rental_db; 
SELECT 
    year as aÃ±o_fabricacion,
    COUNT(*) as total_alquileres
FROM car_rental_analytics
WHERE year BETWEEN 2010 AND 2015
GROUP BY year
ORDER BY year;"
```

---

### 5e. 5 ciudades con mÃ¡s alquileres ecolÃ³gicos

```bash
hive -e "USE car_rental_db; 
SELECT 
    city as ciudad,
    state_name as estado,
    COUNT(*) as total_alquileres_ecologicos
FROM car_rental_analytics
WHERE (fuelType = 'hybrid' OR fuelType = 'electric')
  AND city IS NOT NULL
GROUP BY city, state_name
ORDER BY total_alquileres_ecologicos DESC
LIMIT 5;"
```

---

### 5f. Promedio de reviews por tipo de combustible

```bash
hive -e "USE car_rental_db; 
SELECT 
    fuelType as tipo_combustible,
    ROUND(AVG(reviewCount), 2) as promedio_reviews,
    COUNT(*) as total_vehiculos
FROM car_rental_analytics
WHERE fuelType IS NOT NULL
GROUP BY fuelType
ORDER BY promedio_reviews DESC;"
```

---

## ğŸ”· **PASO 7: Ejecutar DAGs de Airflow (Punto 4)**

### OpciÃ³n A: Desde UI de Airflow

1. Abrir navegador: `http://localhost:8080`
2. Buscar: `car_rental_parent_dag`
3. Activar toggle (ON)
4. Click en botÃ³n â–¶ "Trigger DAG"
5. Monitorear en vista "Graph" o "Grid"

---

### OpciÃ³n B: Desde CLI

```bash
# Listar DAGs
airflow dags list | grep car_rental

# Ejecutar DAG padre (esto ejecutarÃ¡ tambiÃ©n el hijo)
airflow dags trigger car_rental_parent_dag

# Ver estado
airflow dags list-runs -d car_rental_parent_dag

# Ver logs del DAG padre
airflow tasks logs car_rental_parent_dag download_and_ingest $(date +%Y-%m-%d)

# Ver logs del DAG hijo
airflow tasks logs car_rental_child_dag spark_process_data $(date +%Y-%m-%d)
```

---

## ğŸ”· **PASO 8: Guardar Resultados (Para el Informe)**

### Capturar resultados de consultas:

```bash
# Crear directorio para resultados
mkdir -p /home/hadoop/car_rental_results

# Guardar cada consulta en un archivo
hive -e "USE car_rental_db; 
SELECT COUNT(*) as total_alquileres_ecologicos 
FROM car_rental_analytics 
WHERE (fuelType = 'hybrid' OR fuelType = 'electric') AND rating >= 4;" \
> /home/hadoop/car_rental_results/5a_ecologicos.txt

hive -e "USE car_rental_db; 
SELECT state_name, COUNT(*) as total 
FROM car_rental_analytics 
WHERE state_name IS NOT NULL 
GROUP BY state_name 
ORDER BY total ASC 
LIMIT 5;" \
> /home/hadoop/car_rental_results/5b_estados_menor.txt

# ... (repetir para todas las consultas)
```

### Copiar resultados al host:

```bash
# Desde tu mÃ¡quina local
docker cp edvai_hadoop:/home/hadoop/car_rental_results ./resultados/
```

---

## âœ… Checklist Final

Marca cada item cuando lo completes:

- [ ] âœ… Archivos copiados al contenedor
- [ ] âœ… Base de datos `car_rental_db` creada
- [ ] âœ… Tabla `car_rental_analytics` creada
- [ ] âœ… 2 archivos descargados y en HDFS
- [ ] âœ… Procesamiento Spark completado
- [ ] âœ… Datos cargados en Hive
- [ ] âœ… Texas excluido (verificado)
- [ ] âœ… Rating nulos eliminados (verificado)
- [ ] âœ… fuelType en minÃºsculas (verificado)
- [ ] âœ… Consulta 5a ejecutada
- [ ] âœ… Consulta 5b ejecutada
- [ ] âœ… Consulta 5c ejecutada
- [ ] âœ… Consulta 5d ejecutada
- [ ] âœ… Consulta 5e ejecutada
- [ ] âœ… Consulta 5f ejecutada
- [ ] âœ… DAG Padre ejecutado
- [ ] âœ… DAG Hijo ejecutado
- [ ] âœ… Capturas de pantalla tomadas

---

## ğŸ› Errores Comunes y Soluciones

### Error: `bash: hive: command not found`

```bash
export HIVE_HOME=/home/hadoop/hive
export PATH=$HIVE_HOME/bin:$PATH
```

---

### Error: `hdfs: command not found`

```bash
export HADOOP_HOME=/home/hadoop/hadoop
export PATH=$HADOOP_HOME/bin:$PATH
```

---

### Error: `spark-submit: command not found`

```bash
export SPARK_HOME=/home/hadoop/spark
export PATH=$SPARK_HOME/bin:$PATH
```

---

### Error: `File already exists in HDFS`

```bash
# Eliminar y volver a ejecutar
hdfs dfs -rm -r /car_rental/raw/
bash /home/hadoop/scripts/download_data.sh
```

---

### Error: `Table not found`

```bash
# Recrear tabla
hive -f /home/hadoop/hive/car_rental_setup.sql
```

---

## ğŸ“ Soporte

Si encuentras algÃºn error no listado aquÃ­:

1. Verifica los logs de Spark: `/home/hadoop/spark/logs/`
2. Verifica los logs de Airflow: `/home/hadoop/airflow/logs/`
3. Verifica los logs de Hive: `/tmp/hadoop/hive.log`

---

**Â¡Listo! Ahora puedes ejecutar cada paso y verificar que todo funcione correctamente** âœ…


