#!/usr/bin/env python3
"""
Script: spark_products_sold.py
DescripciÃ³n: Procesa datos de clientes y crea tabla products_sold en Hive
             con compaÃ±Ã­as que tienen productos vendidos mayor al promedio
Autor: Hadoop
Fecha: 2025-11-12
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, max, min
from pyspark.sql.types import StructType, StructField, StringType, LongType

def main():
    print("=== INICIANDO PROCESAMIENTO SPARK PARA PRODUCTS_SOLD ===")

    # Inicializar Spark Session con soporte para Hive
    spark = (
        SparkSession.builder
        .appName("NorthwindProductsSold")
        .config("spark.sql.adaptive.enabled", "true")
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
        .enableHiveSupport()
        .getOrCreate()
    )

    spark.sparkContext.setLogLevel("WARN")

    try:
        # Ruta de los datos en HDFS
        customers_path = "hdfs://172.17.0.2:9000/sqoop/ingest/customers"
        
        # Base de datos destino
        database_name = "northwind_analytics"
        table_name = "products_sold"
        
        print("1. ğŸ“‚ Leyendo datos de clientes desde HDFS...")
        
        # Leer archivos Parquet
        df_customers = spark.read.parquet(customers_path)
        
        print(f"   âœ… Datos cargados: {df_customers.count():,} registros")
        print("   ğŸ“Š Esquema de datos:")
        df_customers.printSchema()
        
        # Mostrar sample de datos
        print("   ğŸ” Muestra de datos originales:")
        df_customers.show(10, truncate=False)

        # ============================================
        # CALCULAR PROMEDIO Y FILTRAR
        # ============================================
        print("\n2. ğŸ“ˆ Calculando promedio de productos vendidos...")
        
        # Calcular el promedio
        avg_products = df_customers.select(avg("productos_vendidos")).collect()[0][0]
        print(f"   âœ… Promedio de productos vendidos: {avg_products:.2f}")
        
        # Filtrar compaÃ±Ã­as con productos vendidos mayor al promedio
        print("3. ğŸ” Filtrando compaÃ±Ã­as con productos vendidos > promedio...")
        df_filtered = df_customers.filter(col("productos_vendidos") > avg_products)
        
        print(f"   âœ… CompaÃ±Ã­as filtradas: {df_filtered.count():,} de {df_customers.count():,}")
        
        # Ordenar por productos vendidos (descendente)
        df_final = df_filtered.orderBy(col("productos_vendidos").desc())
        
        print("   ğŸ“‹ Resultados filtrados (ordenados por productos vendidos):")
        df_final.show(truncate=False)

        # ============================================
        # GUARDAR EN HIVE
        # ============================================
        print("\n4. ğŸ’¾ Creando/Actualizando tabla en Hive...")
        
        # Asegurarse que la base de datos existe
        spark.sql(f"CREATE DATABASE IF NOT EXISTS {database_name}")
        spark.sql(f"USE {database_name}")
        
        print(f"   âœ… Usando base de datos: {database_name}")
        
        # Guardar en tabla Hive
        df_final.write \
            .mode("overwrite") \
            .saveAsTable(f"{database_name}.{table_name}")
        
        print(f"   âœ… Tabla '{table_name}' creada/actualizada en Hive")
        
        # ============================================
        # VERIFICACIÃ“N
        # ============================================
        print("\n5. âœ… Verificando datos en Hive...")
        
        # Contar registros en la tabla
        count_result = spark.sql(f"SELECT COUNT(*) as total FROM {database_name}.{table_name}").collect()[0]["total"]
        print(f"   ğŸ“Š Total de registros en tabla Hive: {count_result:,}")
        
        # Mostrar datos de la tabla
        print("   ğŸ” Datos en tabla Hive:")
        spark.sql(f"SELECT * FROM {database_name}.{table_name} ORDER BY productos_vendidos DESC").show(truncate=False)
        
        # Mostrar estadÃ­sticas
        print("   ğŸ“ˆ EstadÃ­sticas finales:")
        df_final.agg(
            avg("productos_vendidos").alias("promedio"),
            max("productos_vendidos").alias("maximo"),
            min("productos_vendidos").alias("minimo")
        ).show()
        
        print("\nâœ… PROCESAMIENTO COMPLETADO EXITOSAMENTE")
        print(f"ğŸ“Š CompaÃ±Ã­as procesadas: {df_final.count():,}")
        print(f"ğŸ’¾ Tabla Hive: {database_name}.{table_name}")

    except Exception as e:
        print(f"âŒ ERROR durante el procesamiento: {str(e)}")
        import traceback
        traceback.print_exc()
        raise e

    finally:
        spark.stop()
        print("ğŸ›‘ SesiÃ³n de Spark cerrada")

if __name__ == "__main__":
    main()

