# Ejercicio 8 - PrÃ¡ctica Airflow + Hive + Spark

Este ejercicio integra **Apache Airflow** para orquestaciÃ³n de workflows, **Apache Hive** para almacenamiento de datos estructurados y **Apache Spark** para procesamiento distribuido, utilizando datos reales de taxis de Nueva York.

## ğŸ¯ Objetivos

- Crear tablas externas en Hive para almacenamiento de datos
- Desarrollar scripts de automatizaciÃ³n para ingesta de datos
- Procesar datos con Spark y aplicar filtros especÃ­ficos
- Orquestar todo el pipeline con Apache Airflow
- Implementar un flujo completo de ETL automatizado

## ğŸ“‹ Ejercicios Incluidos

### 1ï¸âƒ£ **ConfiguraciÃ³n de Base de Datos Hive**
- Crear base de datos `tripdata` en Hive
- Definir tabla externa `airport_trips` con esquema especÃ­fico
- Configurar almacenamiento en formato Parquet
- Verificar estructura y metadatos de la tabla

### 2ï¸âƒ£ **Script de Ingesta Automatizada**
- Crear script bash para descarga de archivos Parquet
- Implementar validaciones de conectividad y servicios
- Configurar subida automÃ¡tica a HDFS
- Incluir limpieza de archivos temporales

### 3ï¸âƒ£ **Procesamiento con Spark**
- Desarrollar script Python para procesamiento de datos
- Unir datasets de enero y febrero 2021
- Aplicar filtros especÃ­ficos (aeropuertos + pago en efectivo)
- Insertar resultados en tabla Hive

### 4ï¸âƒ£ **OrquestaciÃ³n con Airflow**
- Crear DAG para automatizaciÃ³n del pipeline
- Configurar tareas secuenciales y dependencias
- Implementar verificaciÃ³n de resultados
- Monitorear ejecuciÃ³n del workflow

## ğŸ“ Estructura del Ejercicio

```
ejercicio-8-practica-airflow-hive-spark/
â”œâ”€â”€ README.md                    # DocumentaciÃ³n principal
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_and_ingest.sh   # Script de descarga e ingesta
â”‚   â”œâ”€â”€ process_airport_trips.py # Procesamiento con Spark
â”‚   â””â”€â”€ README.md               # DocumentaciÃ³n de scripts
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ airport_trips_processing.py # DAG de Airflow
â”‚   â””â”€â”€ README.md               # DocumentaciÃ³n de Airflow
â”œâ”€â”€ hive/
â”‚   â”œâ”€â”€ hive-setup.sql          # Scripts SQL de Hive
â”‚   â””â”€â”€ README.md               # DocumentaciÃ³n de Hive
â”œâ”€â”€ images/                     # Capturas de pantalla
â”‚   â”œâ”€â”€ ejercicio1-create-tripdata.jpg
â”‚   â”œâ”€â”€ ejercicio2-describe-airporttrips.jpg
â”‚   â””â”€â”€ ejercicio5-airflow.jpg
â””â”€â”€ ejercicios-resueltos.md     # Resultados completos
```

## ğŸš€ TecnologÃ­as Utilizadas

- **Apache Airflow** - OrquestaciÃ³n de workflows
- **Apache Hive** - Data warehouse y consultas SQL
- **Apache Spark** - Procesamiento distribuido de datos
- **PySpark** - API de Python para Spark
- **HDFS** - Sistema de archivos distribuido
- **Parquet** - Formato de datos columnar
- **Bash Scripting** - AutomatizaciÃ³n de procesos

## ğŸ“Š Dataset Utilizado

- **Fuente**: NYC Taxi Data (Yellow Taxi)
- **Archivos**: 
  - `yellow_tripdata_2021-01.parquet` (~20.7 MB)
  - `yellow_tripdata_2021-02.parquet` (~20.8 MB)
- **Registro**: Enero y Febrero 2021
- **Diccionario**: [NYC TLC Data Dictionary](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf)

## ğŸ”§ Requisitos Previos

- Contenedor de Hadoop ejecutÃ¡ndose
- Apache Hive configurado y funcionando
- Apache Spark disponible en el ambiente
- Apache Airflow instalado y configurado
- Acceso a internet para descarga de archivos
- Conocimientos bÃ¡sicos de SQL, Python y Bash

## ğŸ“– GuÃ­as Adicionales

- **Scripts de AutomatizaciÃ³n**: `scripts/README.md`
- **ConfiguraciÃ³n de Airflow**: `airflow/README.md`
- **ConfiguraciÃ³n de Hive**: `hive/README.md`
- **Resultados de Ejercicios**: `ejercicios-resueltos.md`

## ğŸ¯ Resultados Esperados

Al completar este ejercicio, habrÃ¡s:

1. âœ… Configurado una base de datos Hive con tabla externa
2. âœ… Automatizado la descarga e ingesta de datos a HDFS
3. âœ… Procesado datos con Spark aplicando filtros especÃ­ficos
4. âœ… Orquestado todo el pipeline con Apache Airflow
5. âœ… Verificado la integridad de los datos procesados

## ğŸ“ˆ MÃ©tricas del Pipeline

- **Datos procesados**: ~2.7 millones de registros
- **Filtros aplicados**: Viajes a aeropuertos pagados en efectivo
- **Resultado final**: 1 registro filtrado e insertado
- **Tiempo de ejecuciÃ³n**: ~2-3 minutos (dependiendo del hardware)
