# Scripts - Ejercicio 9: Formula 1

Esta carpeta contiene los scripts necesarios para automatizar el pipeline de procesamiento de datos de Formula 1.

## ğŸ“ Archivos Incluidos

### 1ï¸âƒ£ **f1_download_and_ingest.sh**
Script de bash para descarga e ingesta automÃ¡tica de archivos CSV de Formula 1.

**Funcionalidades:**
- âœ… VerificaciÃ³n de servicios HDFS
- âœ… Descarga de 4 archivos CSV desde URLs pÃºblicas de S3
- âœ… ValidaciÃ³n de conectividad
- âœ… Subida automÃ¡tica a HDFS
- âœ… Limpieza de archivos temporales
- âœ… Logging detallado del proceso

**Archivos descargados:**
- `results.csv` - Resultados de carreras
- `drivers.csv` - InformaciÃ³n de corredores
- `constructors.csv` - InformaciÃ³n de constructores
- `races.csv` - InformaciÃ³n de carreras

**Uso:**
```bash
chmod +x f1_download_and_ingest.sh
./f1_download_and_ingest.sh
```

**UbicaciÃ³n HDFS resultante:**
```
/user/hadoop/f1/raw/
â”œâ”€â”€ results.csv
â”œâ”€â”€ drivers.csv
â”œâ”€â”€ constructors.csv
â””â”€â”€ races.csv
```

### 2ï¸âƒ£ **process_f1_data.py**
Script de Python para procesamiento de datos con Spark.

**Funcionalidades:**
- âœ… Lectura de archivos CSV desde HDFS (results, drivers, constructors, races)
- âœ… JOIN entre tablas para relacionar datos
- âœ… Punto 4a: Encuentra corredores con mayor cantidad de puntos en la historia
- âœ… Punto 4b: Encuentra constructores con mÃ¡s puntos en Spanish Grand Prix 1991
- âœ… GeneraciÃ³n de archivos CSV para tablas externas de Hive
- âœ… EstadÃ­sticas y validaciones de datos
- âœ… Guardado en ubicaciones HDFS correctas para tablas externas

**Uso:**
```bash
spark-submit process_f1_data.py
```

**Procesamiento realizado:**
1. Lee 4 archivos CSV desde `/user/hadoop/f1/raw/`
2. **Punto 4a**: JOIN results + drivers â†’ Agrupa por corredor â†’ Suma puntos â†’ Ordena descendente
3. **Punto 4b**: JOIN results + constructors + races â†’ Filtra Spanish GP 1991 â†’ Agrupa por constructor â†’ Suma puntos
4. Guarda resultados en CSV en ubicaciones de tablas externas

## ğŸ”§ ConfiguraciÃ³n Requerida

### Variables de Entorno
```bash
export HADOOP_HOME=/home/hadoop/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

### Servicios Necesarios
- **HDFS**: NameNode y DataNode ejecutÃ¡ndose
- **Hive**: Metastore configurado con base de datos `f1`
- **Spark**: Disponible en el PATH
- **Internet**: Para descarga de archivos desde S3

## ğŸ“Š URLs de Datos

Todos los archivos estÃ¡n disponibles en el bucket S3 pÃºblico:

- **results.csv**: `https://data-engineer-edvai-public.s3.amazonaws.com/results.csv`
- **drivers.csv**: `https://data-engineer-edvai-public.s3.amazonaws.com/drivers.csv`
- **constructors.csv**: `https://data-engineer-edvai-public.s3.amazonaws.com/constructors.csv`
- **races.csv**: `https://data-engineer-edvai-public.s3.amazonaws.com/races.csv`

## ğŸ¯ Resultados Esperados

### Descarga e Ingesta
- **Archivos descargados**: 4 archivos CSV
- **UbicaciÃ³n HDFS**: `/user/hadoop/f1/raw/`
- **Tiempo estimado**: 1-3 minutos (dependiendo de la velocidad de conexiÃ³n)

### Procesamiento Spark (Punto 4)
- **Archivos leÃ­dos**: 4 archivos CSV (results, drivers, constructors, races)
- **Resultado final**: 
  - Archivo CSV con todos los corredores ordenados por puntos totales (punto 4a)
  - Archivo CSV con constructores de Spanish Grand Prix 1991 ordenados por puntos (punto 4b)
- **UbicaciÃ³n HDFS destino**: 
  - `/user/hive/warehouse/f1.db/driver_results/` (para tabla externa driver_results)
  - `/user/hive/warehouse/f1.db/constructor_results/` (para tabla externa constructor_results)
- **Formato**: CSV con headers

## ğŸš¨ Troubleshooting

### Error: "Servicios HDFS no detectados"
```bash
# Verificar servicios
jps | grep -E "NameNode|DataNode"

# Iniciar servicios si es necesario
start-dfs.sh
```

### Error: "No hay conexiÃ³n a internet"
```bash
# Verificar conectividad
ping google.com
wget --spider https://data-engineer-edvai-public.s3.amazonaws.com/results.csv
```

### Error: "Error creando directorio HDFS"
```bash
# Verificar permisos y estado de HDFS
hdfs dfs -ls /
hdfs dfs -mkdir -p /user/hadoop/f1/raw
```

### Error: "Spark session failed"
```bash
# Verificar configuraciÃ³n de Spark
spark-submit --version
# Verificar configuraciÃ³n de Hive
hive --version
```

## ğŸ“ Logs y Monitoreo

Los scripts incluyen logging detallado que muestra:
- âœ… Estado de cada paso del proceso
- ğŸ“Š MÃ©tricas de rendimiento (tamaÃ±os de archivos, conteos)
- âŒ Errores especÃ­ficos con cÃ³digos de salida
- ğŸ“… Timestamps de inicio y finalizaciÃ³n

## ğŸ”„ IntegraciÃ³n con Airflow

Estos scripts estÃ¡n diseÃ±ados para ser ejecutados como tareas de Airflow:
- **BashOperator**: Para `f1_download_and_ingest.sh`
- **BashOperator**: Para `process_f1_data.py`
- **Dependencias**: Secuenciales con validaciones

## ğŸ“‹ Estructura de Datos

### results.csv
Contiene informaciÃ³n de resultados de carreras individuales:
- resultId, raceId, driverId, constructorId, points, etc.

### drivers.csv
Contiene informaciÃ³n de los corredores:
- driverId, driverRef, number, code, forename, surname, nationality, etc.

### constructors.csv
Contiene informaciÃ³n de los constructores:
- constructorId, constructorRef, name, nationality, url

### races.csv
Contiene informaciÃ³n de las carreras:
- raceId, year, round, circuitId, name, date, time, url

## ğŸ”— Referencias

- **Diccionario de datos**: [Kaggle - Formula 1 Dataset](https://www.kaggle.com/datasets/rohanrao/formula-1-world-championship-1950-2020)

