# üìä Data Engineering Exercises and Practices

Este repositorio contiene ejercicios pr√°cticos de **ingenier√≠a de datos** enfocados en la ingesta de datos usando **Hadoop**, **HDFS** y **Sqoop**. Incluye scripts automatizados para diferentes escenarios de ingesta.

---

## üéØ Objetivos

- Practicar la ingesta de datos desde fuentes externas hacia HDFS
- Automatizar procesos de ETL usando scripts bash
- Conectar bases de datos relacionales (PostgreSQL) con Hadoop usando Sqoop
- Gestionar archivos temporales y limpieza de datos

---

## üìÅ Estructura del Repositorio

```
data-engineering-exercises-and-practices/
‚îú‚îÄ‚îÄ ejercicio-3-practica-de-ingest-local/     # Ingesta desde fuentes externas
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ landing.sh                        # Script de ingesta local
‚îÇ   ‚îú‚îÄ‚îÄ images/                               # Capturas de pantalla
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ ejercicio-4-practica-ingest-sqoop/        # Ingesta desde PostgreSQL
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sqoop.sh                          # Script de comandos Sqoop
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ ejercicio-5-practica-sqoop-nifi/          # Pr√°ctica completa Sqoop + NiFi
‚îÇ   ‚îú‚îÄ‚îÄ practica-sqoop/                       # Ejercicios de Sqoop
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sqoop-exercises.sh            # Script completo de Sqoop
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ejercicios-resueltos.md           # Resultados de ejercicios
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ practica-nifi/                        # Ejercicios de NiFi
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nifi-flow-diagram.md              # Explicaci√≥n del flujo
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ images/                               # Diagramas de flujo
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ ejercicio-6-practica-ingest-gcp/          # Pr√°ctica de ingesta en GCP
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ upload_csvs.sh                    # Script de subida a GCS
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ images/                               # Capturas de buckets y transfer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bucket-multiregional-us.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bucket-regional-finlandia.jpg
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ storage-transfer-service.jpg
‚îÇ   ‚îú‚îÄ‚îÄ ejercicios-resueltos.md               # Resultados de ejercicios
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ ejercicio-7-practica-nifi-spark/          # Pr√°ctica NiFi + Spark
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ download_parquet.sh               # Script de descarga
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ nifi/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nifi-flow-guide.md                # Gu√≠a del flujo NiFi
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ spark/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spark-analysis.md                 # An√°lisis con PySpark
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ images/                               # Capturas de flujo NiFi
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ nifi-process.jpg
‚îÇ   ‚îú‚îÄ‚îÄ ejercicios-resueltos.md               # Resultados completos
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ ejercicio-8-practica-airflow-hive-spark/  # Pr√°ctica Airflow + Hive + Spark
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ download_and_ingest.sh            # Script de descarga e ingesta
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ process_airport_trips.py          # Procesamiento con Spark
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ airport_trips_processing.py       # DAG de Airflow
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ hive/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hive-setup.sql                    # Scripts SQL de Hive
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ images/                               # Capturas de pantalla
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ejercicio1-create-tripdata.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ejercicio2-describe-airporttrips.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ejercicio5-airflow.jpg
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ ejercicios-resueltos.md               # Resultados completos
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ ejercicio-9-practica-f1-airflow-hive-spark/ # Pr√°ctica F1: Airflow + Hive + Spark
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ f1_download_and_ingest.sh        # Script de descarga e ingesta F1
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ process_f1_data.py                # Procesamiento con Spark
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ f1_processing.py                  # DAG de Airflow
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ hive/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ f1-setup.sql                      # Scripts SQL de Hive
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ images/                               # Capturas de pantalla
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ describe-external-tables-punto2.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ total_drivers.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ total_constructors.png
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ airflow-Graph.png
‚îÇ   ‚îú‚îÄ‚îÄ ejercicios-resueltos.md               # Resultados completos
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ ejercicio-10-practica-northwind-airflow-sqoop-spark/ # Pr√°ctica Northwind: Airflow + Sqoop + Spark
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sqoop_import_clientes.sh          # Import Sqoop de clientes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sqoop_import_envios.sh            # Import Sqoop de env√≠os
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sqoop_import_order_details.sh     # Import Sqoop de detalles
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spark_products_sold.py            # Procesamiento Spark: products_sold
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spark_products_sent.py            # Procesamiento Spark: products_sent
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ northwind_processing.py           # DAG de Airflow con TaskGroups
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ hive/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ northwind-setup.sql               # Scripts SQL de Hive
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ images/                               # Capturas de pantalla
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ ejercicios-resueltos.md               # Resultados completos paso a paso
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ ejercicio-11-practica-titanic-nifi-airflow-hive/ # Pr√°ctica Titanic: NiFi + Airflow + Hive
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ingest.sh                         # Script de descarga de titanic.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ nifi/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core-site.xml                     # Configuraci√≥n Hadoop para NiFi
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hdfs-site.xml                     # Configuraci√≥n HDFS para NiFi
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ titanic_dag.py                    # DAG de procesamiento con Pandas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ hive/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ titanic-setup.sql                 # Scripts SQL de Hive
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ images/                               # Capturas de pantalla
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ ejercicios-resueltos.md               # Resultados completos paso a paso
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ README.md                                 # Este archivo
```

---

## üöÄ Ejercicios Incluidos

### 1Ô∏è‚É£ **Ejercicio 3: Ingesta Local con Hadoop**

**Descripci√≥n:** Descarga un archivo CSV desde GitHub, lo mueve a HDFS y limpia archivos temporales.

**Caracter√≠sticas:**
- ‚úÖ Descarga autom√°tica desde GitHub
- ‚úÖ Gesti√≥n de directorios temporales
- ‚úÖ Subida a HDFS
- ‚úÖ Limpieza autom√°tica de archivos

**Archivo:** `ejercicio-3-practica-de-ingest-local/scripts/landing.sh`

### 2Ô∏è‚É£ **Ejercicio 4: Ingesta con Sqoop**

**Descripci√≥n:** Conecta PostgreSQL con Hadoop usando Sqoop para importar datos.

**Caracter√≠sticas:**
- ‚úÖ Conexi√≥n JDBC con PostgreSQL
- ‚úÖ Listado de tablas
- ‚úÖ Consultas SQL directas
- ‚úÖ Importaci√≥n completa y filtrada
- ‚úÖ Formato Parquet

**Archivo:** `ejercicio-4-practica-ingest-sqoop/scripts/sqoop.sh`

### 3Ô∏è‚É£ **Ejercicio 5: Pr√°ctica Completa Sqoop + NiFi**

**Descripci√≥n:** Pr√°ctica integral que combina Sqoop para ingesta de datos y NiFi para procesamiento de flujos de datos.

**Caracter√≠sticas:**
- ‚úÖ **Sqoop**: 4 ejercicios completos con resultados reales
- ‚úÖ **NiFi**: Configuraci√≥n y flujo de procesamiento
- ‚úÖ **Scripts automatizados**: Comandos listos para ejecutar
- ‚úÖ **Documentaci√≥n detallada**: Gu√≠as paso a paso
- ‚úÖ **Diagramas visuales**: Flujos de NiFi explicados

**Archivos principales:**
- `ejercicio-5-practica-sqoop-nifi/practica-sqoop/scripts/sqoop-exercises.sh`
- `ejercicio-5-practica-sqoop-nifi/practica-nifi/README.md`
- `ejercicio-5-practica-sqoop-nifi/images/nifi-flow-diagram.jpg`

### 4Ô∏è‚É£ **Ejercicio 6: Pr√°ctica Ingest GCP**

**Descripci√≥n:** Pr√°ctica de ingesta de datos en Google Cloud Platform utilizando Google Cloud Storage y Storage Transfer Service.

**Caracter√≠sticas:**
- ‚úÖ **Buckets GCP**: Creaci√≥n de buckets regionales y multiregionales
- ‚úÖ **gsutil CLI**: Subida automatizada de archivos CSV
- ‚úÖ **Storage Transfer**: Migraci√≥n entre buckets
- ‚úÖ **Scripts automatizados**: Procesos de subida masiva
- ‚úÖ **Capturas visuales**: Evidencia de resultados

**Archivos principales:**
- `ejercicio-6-practica-ingest-gcp/scripts/upload_csvs.sh`
- `ejercicio-6-practica-ingest-gcp/ejercicios-resueltos.md`
- `ejercicio-6-practica-ingest-gcp/images/` (capturas de pantalla)

### 5Ô∏è‚É£ **Ejercicio 7: Pr√°ctica NiFi + Spark**

**Descripci√≥n:** Pr√°ctica integral que combina Apache NiFi para procesamiento de flujos de datos y Apache Spark para an√°lisis de datos, utilizando un dataset real de taxis de NYC.

**Caracter√≠sticas:**
- ‚úÖ **NiFi**: Flujo de procesamiento con GetFile y PutHDFS
- ‚úÖ **Spark**: An√°lisis de datos con PySpark y SQL
- ‚úÖ **Dataset real**: Datos de taxis amarillos de NYC
- ‚úÖ **Scripts automatizados**: Descarga y procesamiento
- ‚úÖ **6 consultas de an√°lisis**: Insights de negocio

**Archivos principales:**
- `ejercicio-7-practica-nifi-spark/scripts/download_parquet.sh`
- `ejercicio-7-practica-nifi-spark/nifi/nifi-flow-guide.md`
- `ejercicio-7-practica-nifi-spark/spark/spark-analysis.md`
- `ejercicio-7-practica-nifi-spark/ejercicios-resueltos.md`

### 6Ô∏è‚É£ **Ejercicio 8: Pr√°ctica Airflow + Hive + Spark**

**Descripci√≥n:** Pr√°ctica integral que integra Apache Airflow para orquestaci√≥n de workflows, Apache Hive para almacenamiento de datos estructurados y Apache Spark para procesamiento distribuido, utilizando datos reales de taxis de Nueva York.

**Caracter√≠sticas:**
- ‚úÖ **Hive**: Base de datos y tabla externa con formato Parquet
- ‚úÖ **Spark**: Procesamiento distribuido con filtros espec√≠ficos
- ‚úÖ **Airflow**: Orquestaci√≥n completa del pipeline ETL
- ‚úÖ **Dataset real**: NYC Taxi Data (2.7M registros)
- ‚úÖ **Pipeline automatizado**: Descarga ‚Üí Procesamiento ‚Üí Verificaci√≥n
- ‚úÖ **Filtros espec√≠ficos**: Aeropuertos + pago en efectivo

**Archivos principales:**
- `ejercicio-8-practica-airflow-hive-spark/scripts/download_and_ingest.sh`
- `ejercicio-8-practica-airflow-hive-spark/scripts/process_airport_trips.py`
- `ejercicio-8-practica-airflow-hive-spark/airflow/airport_trips_processing.py`
- `ejercicio-8-practica-airflow-hive-spark/hive/hive-setup.sql`
- `ejercicio-8-practica-airflow-hive-spark/ejercicios-resueltos.md`

### 7Ô∏è‚É£ **Ejercicio 9: Pr√°ctica F1: Airflow + Hive + Spark**

**Descripci√≥n:** Pr√°ctica integral que integra Apache Airflow para orquestaci√≥n de workflows, Apache Hive para almacenamiento de datos estructurados y Apache Spark para procesamiento distribuido, utilizando datos reales de Formula 1 World Championship (1950-2020).

**Caracter√≠sticas:**
- ‚úÖ **Hive**: Base de datos `f1` con 2 tablas externas (driver_results, constructor_results)
- ‚úÖ **Spark**: Procesamiento distribuido con JOINs y agregaciones
- ‚úÖ **Airflow**: Orquestaci√≥n completa del pipeline ETL
- ‚úÖ **Dataset real**: Formula 1 Data (26,759 resultados, 861 corredores)
- ‚úÖ **Pipeline automatizado**: Descarga ‚Üí Procesamiento ‚Üí Verificaci√≥n
- ‚úÖ **An√°lisis espec√≠ficos**: Top corredores por puntos y constructores en Spanish GP 1991

**Archivos principales:**
- `ejercicio-9-practica-f1-airflow-hive-spark/scripts/f1_download_and_ingest.sh`
- `ejercicio-9-practica-f1-airflow-hive-spark/scripts/process_f1_data.py`
- `ejercicio-9-practica-f1-airflow-hive-spark/airflow/f1_processing.py`
- `ejercicio-9-practica-f1-airflow-hive-spark/hive/f1-setup.sql`
- `ejercicio-9-practica-f1-airflow-hive-spark/ejercicios-resueltos.md`

### 8Ô∏è‚É£ **Ejercicio 10: Pr√°ctica Northwind: Airflow + Sqoop + Hive + Spark**

**Descripci√≥n:** Pr√°ctica completa de ETL que integra Apache Sqoop para ingesti√≥n desde PostgreSQL, Apache Hive para almacenamiento de datos estructurados, Apache Spark para procesamiento y an√°lisis, y Apache Airflow con TaskGroups para orquestaci√≥n avanzada de workflows, utilizando la base de datos Northwind.

**Caracter√≠sticas:**
- ‚úÖ **Sqoop**: Ingesti√≥n automatizada desde PostgreSQL con JOINs complejos
- ‚úÖ **Hive**: Base de datos `northwind_analytics` con tablas procesadas
- ‚úÖ **Spark**: An√°lisis de datos con filtros y agregaciones
- ‚úÖ **Airflow**: DAG con TaskGroups (ingest, process, verify)
- ‚úÖ **Dataset real**: Base de datos Northwind (89 clientes, 2,155 detalles de √≥rdenes)
- ‚úÖ **Pipeline completo**: 7 ejercicios integrados en un flujo automatizado
- ‚úÖ **Formato Parquet**: Compresi√≥n Snappy para optimizaci√≥n
- ‚úÖ **Seguridad**: Password almacenada en archivo seguro

**Pipeline ETL:**
1. **Etapa Ingest** (Sqoop ‚Üí HDFS):
   - Clientes con productos vendidos (89 registros)
   - √ìrdenes enviadas con informaci√≥n de empresa (809 registros)
   - Detalles de √≥rdenes (2,155 registros)

2. **Etapa Process** (Spark ‚Üí Hive):
   - `products_sold`: Clientes con ventas > promedio (33 registros)
   - `products_sent`: Pedidos con descuento y c√°lculo de precios (803 registros)

3. **Etapa Verify** (Hive Beeline):
   - Verificaci√≥n de conteos y calidad de datos

**Archivos principales:**
- `ejercicio-10-practica-northwind-airflow-sqoop-spark/scripts/sqoop_import_clientes.sh`
- `ejercicio-10-practica-northwind-airflow-sqoop-spark/scripts/sqoop_import_envios.sh`
- `ejercicio-10-practica-northwind-airflow-sqoop-spark/scripts/sqoop_import_order_details.sh`
- `ejercicio-10-practica-northwind-airflow-sqoop-spark/scripts/spark_products_sold.py`
- `ejercicio-10-practica-northwind-airflow-sqoop-spark/scripts/spark_products_sent.py`
- `ejercicio-10-practica-northwind-airflow-sqoop-spark/airflow/northwind_processing.py`
- `ejercicio-10-practica-northwind-airflow-sqoop-spark/hive/northwind-setup.sql`
- `ejercicio-10-practica-northwind-airflow-sqoop-spark/ejercicios-resueltos.md`

### 9Ô∏è‚É£ **Ejercicio 11: Pr√°ctica Titanic: NiFi + Airflow + Hive**

**Descripci√≥n:** Pr√°ctica completa que integra Apache NiFi para flujo de ingesta de datos, Apache Airflow para procesamiento y transformaci√≥n con Pandas, y Apache Hive para almacenamiento y an√°lisis SQL, utilizando el dataset del Titanic.

**Caracter√≠sticas:**
- ‚úÖ **NiFi**: Flujo completo de ingesta (GetFile ‚Üí PutFile ‚Üí GetFile ‚Üí PutHDFS)
- ‚úÖ **Airflow**: DAG con transformaciones usando Pandas
- ‚úÖ **Pandas**: Manipulaci√≥n de datos (remover columnas, rellenar nulos, promedios)
- ‚úÖ **Hive**: Almacenamiento estructurado y consultas anal√≠ticas
- ‚úÖ **Dataset real**: Titanic (891 pasajeros)
- ‚úÖ **Pipeline automatizado**: 8 ejercicios integrados
- ‚úÖ **An√°lisis de negocio**: Supervivencia por g√©nero, clase, edades

**Pipeline ETL:**
1. **Descarga** (Script Bash):
   - Descarga titanic.csv desde S3 a `/home/nifi/ingest`

2. **Ingesta NiFi** (Flujo de 4 procesadores):
   - GetFile: Lee desde `/home/nifi/ingest`
   - PutFile: Mueve a `/home/nifi/bucket`
   - GetFile: Lee desde bucket
   - PutHDFS: Ingesta en HDFS `/nifi`

3. **Procesamiento Airflow** (Transformaciones Pandas):
   - Remover columnas: SibSp, Parch
   - Rellenar edad con promedio por g√©nero
   - Cabin nulo ‚Üí 0
   - Limpiar comas en Name

4. **An√°lisis Hive** (Consultas SQL):
   - Sobrevivientes por g√©nero (male: 109, female: 233)
   - Sobrevivientes por clase (1ra: 136, 2da: 87, 3ra: 119)
   - Mayor edad sobreviviente (80 a√±os)
   - Menor edad sobreviviente (0.42 a√±os)

**Archivos principales:**
- `ejercicio-11-practica-titanic-nifi-airflow-hive/scripts/ingest.sh`
- `ejercicio-11-practica-titanic-nifi-airflow-hive/nifi/core-site.xml`
- `ejercicio-11-practica-titanic-nifi-airflow-hive/nifi/hdfs-site.xml`
- `ejercicio-11-practica-titanic-nifi-airflow-hive/airflow/titanic_dag.py`
- `ejercicio-11-practica-titanic-nifi-airflow-hive/hive/titanic-setup.sql`
- `ejercicio-11-practica-titanic-nifi-airflow-hive/ejercicios-resueltos.md`

---

## üîß Requisitos Previos

### Entorno de Desarrollo
- **Docker** con contenedores de Hadoop y PostgreSQL
- **Bash** para ejecutar scripts
- Acceso a terminal del contenedor Hadoop

### Contenedores Necesarios
```bash
# Contenedor Hadoop
docker exec -it edvai_hadoop bash
su hadoop

# Verificar PostgreSQL
docker inspect edvai_postgres
```

---

## üìã C√≥mo Ejecutar

### Ejercicio 3: Ingesta Local
```bash
# 1. Navegar al directorio
cd ejercicio-3-practica-de-ingest-local/scripts/

# 2. Dar permisos de ejecuci√≥n
chmod +x landing.sh

# 3. Ejecutar el script
./landing.sh

# 4. Verificar en HDFS
hdfs dfs -ls /ingest
```

### Ejercicio 4: Ingesta con Sqoop
```bash
# 1. Navegar al directorio
cd ejercicio-4-practica-ingest-sqoop/scripts/

# 2. Dar permisos de ejecuci√≥n
chmod +x sqoop.sh

# 3. Ejecutar el script
./sqoop.sh
```

### Ejercicio 5: Pr√°ctica Completa Sqoop + NiFi
```bash
# 1. Ejecutar ejercicios de Sqoop
cd ejercicio-5-practica-sqoop-nifi/practica-sqoop/scripts/
chmod +x sqoop-exercises.sh
./sqoop-exercises.sh

# 2. Configurar NiFi (ver documentaci√≥n)
cd ../../practica-nifi/
# Seguir gu√≠a en README.md para configuraci√≥n de NiFi
```

### Ejercicio 6: Pr√°ctica Ingest GCP
```bash
# 1. Ejecutar script de subida de archivos
cd ejercicio-6-practica-ingest-gcp/scripts/
chmod +x upload_csvs.sh
./upload_csvs.sh

# 2. Verificar archivos en buckets
gsutil ls gs://data-bucket-demo-1/
gsutil ls gs://demo-bucket-edvai/

# 3. Ver documentaci√≥n completa
cd ../
# Revisar README.md y ejercicios-resueltos.md
```

### Ejercicio 7: Pr√°ctica NiFi + Spark
```bash
# 1. Ejecutar script de descarga
cd ejercicio-7-practica-nifi-spark/scripts/
chmod +x download_parquet.sh
./download_parquet.sh

# 2. Configurar flujo en NiFi (ver documentaci√≥n)
cd ../nifi/
# Seguir gu√≠a en nifi-flow-guide.md

# 3. Ejecutar an√°lisis con Spark
cd ../spark/
# Seguir gu√≠a en spark-analysis.md

# 4. Ver resultados completos
cd ../
# Revisar ejercicios-resueltos.md
```

### Ejercicio 8: Pr√°ctica Airflow + Hive + Spark
```bash
# 1. Configurar Hive (base de datos y tabla)
cd ejercicio-8-practica-airflow-hive-spark/hive/
# Seguir gu√≠a en README.md para configuraci√≥n de Hive

# 2. Ejecutar scripts de procesamiento
cd ../scripts/
chmod +x download_and_ingest.sh
chmod +x process_airport_trips.py
./download_and_ingest.sh
spark-submit process_airport_trips.py

# 3. Configurar DAG de Airflow
cd ../airflow/
# Copiar airport_trips_processing.py a /home/hadoop/airflow/dags/

# 4. Ejecutar DAG en Airflow
# Acceder a interfaz web de Airflow y ejecutar DAG manualmente

# 5. Ver resultados completos
cd ../
# Revisar ejercicios-resueltos.md
```

### Ejercicio 9: Pr√°ctica F1: Airflow + Hive + Spark
```bash
# 1. Configurar Hive (base de datos y tablas externas)
cd ejercicio-9-practica-f1-airflow-hive-spark/hive/
# Seguir gu√≠a en README.md para configuraci√≥n de Hive
hive -f f1-setup.sql

# 2. Ejecutar scripts de procesamiento
cd ../scripts/
chmod +x f1_download_and_ingest.sh
chmod +x process_f1_data.py
./f1_download_and_ingest.sh
spark-submit process_f1_data.py

# 3. Configurar DAG de Airflow
cd ../airflow/
# Copiar f1_processing.py a /home/hadoop/airflow/dags/

# 4. Ejecutar DAG en Airflow
# Acceder a interfaz web de Airflow y ejecutar DAG manualmente

# 5. Ver resultados completos
cd ../
# Revisar ejercicios-resueltos.md
```

### Ejercicio 11: Pr√°ctica Titanic: NiFi + Airflow + Hive
```bash
# 1. Descargar datos (en contenedor NiFi)
docker exec -it nifi bash
/home/nifi/scripts/ingest.sh

# 2. Configurar archivos Hadoop en NiFi
docker cp core-site.xml nifi:/home/nifi/hadoop/
docker cp hdfs-site.xml nifi:/home/nifi/hadoop/

# 3. Configurar flujo en NiFi
# Acceder a https://localhost:8443/nifi
# Crear procesadores: GetFile ‚Üí PutFile ‚Üí GetFile ‚Üí PutHDFS
# (Seguir gu√≠a en nifi/README.md)

# 4. Preparar HDFS
docker exec -it edvai_hadoop bash
hdfs dfs -mkdir -p /nifi
hdfs dfs -chmod 777 /nifi

# 5. Crear tabla en Hive
cd ejercicio-11-practica-titanic-nifi-airflow-hive/hive/
hive -f titanic-setup.sql

# 6. Configurar DAG de Airflow
cd ../airflow/
# Copiar titanic_dag.py a /home/hadoop/airflow/dags/

# 7. Ejecutar DAG en Airflow
airflow dags trigger titanic_processing_dag

# 8. Ejecutar consultas anal√≠ticas en Hive
hive -f /home/hadoop/hive/titanic-setup.sql

# 9. Ver resultados completos
cd ../
# Revisar ejercicios-resueltos.md
```

---

## üìä Datos Utilizados

### Ejercicio 3
- **Fuente:** [starwars.csv](https://github.com/fpineyro/homework-0/blob/master/starwars.csv)
- **Destino:** `/ingest/` en HDFS
- **Formato:** CSV

### Ejercicio 4
- **Fuente:** Base de datos Northwind (PostgreSQL)
- **Tabla:** `region`
- **Destino:** `/sqoop/ingest/` en HDFS
- **Formato:** Parquet

### Ejercicio 5
- **Sqoop**: Base de datos Northwind (PostgreSQL)
  - **Tablas:** `orders`, `products`, `customers`
  - **Destino:** `/sqoop/ingest/` en HDFS
  - **Formato:** Parquet
- **NiFi**: Archivo `starwars.csv`
  - **Procesamiento:** CSV ‚Üí Avro
  - **Destino:** `/nifi/` en HDFS
  - **Formato:** Avro

### Ejercicio 6
- **GCP Storage**: Archivos CSV locales
  - **Fuente**: Directorio local con 5 archivos CSV
  - **Destino**: `gs://data-bucket-demo-1/` (US multiregional)
  - **Formato:** CSV
- **Storage Transfer**: Migraci√≥n entre buckets
  - **Origen**: `gs://data-bucket-demo-1/`
  - **Destino**: `gs://demo-bucket-edvai/` (Finlandia regional)
  - **Formato:** CSV

### Ejercicio 7
- **NiFi**: Archivo Parquet de NYC Taxi Data
  - **Fuente**: S3 (https://data-engineer-edvai-public.s3.amazonaws.com/)
  - **Procesamiento**: GetFile ‚Üí PutHDFS
  - **Destino**: `/nifi/` en HDFS
  - **Formato:** Parquet
- **Spark**: An√°lisis de datos
  - **Fuente**: HDFS `/nifi/yellow_tripdata_2021-01.parquet`
  - **Procesamiento**: PySpark + SQL
  - **Resultados**: 6 consultas de an√°lisis
  - **Formato:** Parquet

### Ejercicio 8
- **Hive**: Base de datos y tabla externa
  - **Base de datos**: `tripdata`
  - **Tabla**: `airport_trips` (EXTERNAL TABLE)
  - **Formato**: Parquet
  - **Ubicaci√≥n**: `/user/hive/warehouse/tripdata.db/airport_trips`
- **Spark**: Procesamiento de datos NYC Taxi
  - **Fuente**: HDFS `/user/hadoop/tripdata/raw/` (2 archivos Parquet)
  - **Procesamiento**: PySpark con filtros espec√≠ficos
  - **Filtros**: Aeropuertos + pago en efectivo (payment_type = 2)
  - **Resultado**: 1 registro filtrado e insertado
  - **Formato:** Parquet
- **Airflow**: Orquestaci√≥n del pipeline
  - **DAG**: `airport_trips_processing`
  - **Tareas**: Descarga ‚Üí Procesamiento ‚Üí Verificaci√≥n
  - **Resultado**: Pipeline automatizado completo

### Ejercicio 9
- **Hive**: Base de datos y tablas externas
  - **Base de datos**: `f1`
  - **Tablas**: `driver_results`, `constructor_results` (EXTERNAL TABLE)
  - **Formato**: CSV
  - **Ubicaci√≥n**: `/user/hive/warehouse/f1.db/driver_results/` y `/user/hive/warehouse/f1.db/constructor_results/`
- **Spark**: Procesamiento de datos Formula 1
  - **Fuente**: HDFS `/user/hadoop/f1/raw/` (4 archivos CSV: results, drivers, constructors, races)
  - **Procesamiento**: PySpark con JOINs y agregaciones
  - **Punto 4a**: Top corredores por puntos totales (861 corredores)
  - **Punto 4b**: Constructores en Spanish Grand Prix 1991 (17 constructores)
  - **Resultado**: Archivos CSV generados para tablas externas
  - **Formato:** CSV
- **Airflow**: Orquestaci√≥n del pipeline
  - **DAG**: `f1_processing`
  - **Tareas**: Descarga ‚Üí Procesamiento ‚Üí Verificaci√≥n (2 tablas)
  - **Resultado**: Pipeline automatizado completo
- **Fuente de datos**: S3 p√∫blico
  - **results.csv**: 26,759 registros
  - **drivers.csv**: 861 registros
  - **constructors.csv**: 212 registros
  - **races.csv**: 1,125 registros

### Ejercicio 11
- **NiFi**: Flujo de ingesta completo
  - **Descarga**: Script bash con wget desde S3
  - **Procesamiento**: GetFile ‚Üí PutFile ‚Üí GetFile ‚Üí PutHDFS
  - **Origen**: `/home/nifi/ingest/titanic.csv`
  - **Intermedio**: `/home/nifi/bucket/titanic.csv`
  - **Destino**: `/nifi/titanic.csv` en HDFS
  - **Formato:** CSV
- **Airflow + Pandas**: Transformaciones de datos
  - **Fuente**: HDFS `/nifi/titanic.csv`
  - **Procesamiento**: Python con Pandas
  - **Transformaciones**: Remover columnas, rellenar nulos, limpiar datos
  - **Destino**: Hive `titanic_db.titanic_processed`
  - **Formato:** CSV ‚Üí Tabla Hive
- **Hive**: Almacenamiento y an√°lisis
  - **Base de datos**: `titanic_db`
  - **Tablas**: `titanic_raw`, `titanic_processed`
  - **Formato**: CSV (tabla externa) y Managed table
  - **Consultas**: 4 an√°lisis de negocio (supervivencia, edades)
- **Fuente de datos**: S3 p√∫blico
  - **titanic.csv**: 891 registros (pasajeros del Titanic)
  - **URL**: https://data-engineer-edvai-public.s3.amazonaws.com/titanic.csv

---

## üõ†Ô∏è Tecnolog√≠as Utilizadas

- **Hadoop** - Framework de procesamiento distribuido
- **HDFS** - Sistema de archivos distribuido
- **Sqoop** - Herramienta de transferencia de datos
- **NiFi** - Procesamiento de flujos de datos
- **Apache Spark** - Motor de procesamiento distribuido
- **PySpark** - API de Python para Spark
- **Pandas** - Librer√≠a de manipulaci√≥n de datos en Python
- **Apache Airflow** - Orquestaci√≥n de workflows
- **Apache Hive** - Data warehouse y consultas SQL
- **Google Cloud Storage** - Almacenamiento de objetos en la nube
- **gsutil** - Herramienta CLI de Google Cloud
- **Storage Transfer Service** - Migraci√≥n de datos en GCP
- **PostgreSQL** - Base de datos relacional
- **Bash** - Scripting y automatizaci√≥n
- **Docker** - Contenedores
- **Parquet** - Formato de datos columnar
- **Avro** - Formato de serializaci√≥n de datos
- **SQL** - Consultas estructuradas

---

## üìù Notas Importantes

- Los scripts incluyen manejo de errores y mensajes informativos
- Se realizan limpiezas autom√°ticas de archivos temporales
- Los directorios se crean autom√°ticamente si no existen
- Las conexiones JDBC requieren configuraci√≥n de red entre contenedores
- NiFi requiere configuraci√≥n adicional de librer√≠as HDFS para versiones 2.0+
- Los ejercicios incluyen resultados reales y m√©tricas de rendimiento
- GCP requiere autenticaci√≥n y configuraci√≥n de proyecto
- Storage Transfer Service permite migraci√≥n eficiente entre buckets
- Los buckets regionales optimizan latencia, los multiregionales optimizan disponibilidad
- Spark requiere configuraci√≥n de memoria y paralelismo para optimizar rendimiento
- Los archivos Parquet ofrecen mejor compresi√≥n y rendimiento que CSV
- Las consultas SQL en Spark permiten an√°lisis eficiente de grandes vol√∫menes de datos
- Airflow requiere configuraci√≥n de DAGs y monitoreo de tareas
- Hive necesita configuraci√≥n de metastore y permisos de HDFS
- Los DAGs de Airflow permiten orquestaci√≥n compleja de pipelines ETL
- Las tablas externas en Hive facilitan la integraci√≥n con Spark
- Los filtros espec√≠ficos en Spark optimizan el procesamiento de grandes datasets
- NiFi permite crear flujos visuales de procesamiento de datos sin c√≥digo
- Pandas es ideal para transformaciones de datos en memoria con datasets peque√±os/medianos
- Los archivos de configuraci√≥n de Hadoop deben estar accesibles para que NiFi se conecte a HDFS
- La combinaci√≥n NiFi + Airflow permite separar ingesta (NiFi) de procesamiento (Airflow)

---

## üë®‚Äçüíª Autor

**Enzo** - Pr√°cticas de Data Engineering

---

## üìö Recursos Adicionales

- [Documentaci√≥n de Hadoop](https://hadoop.apache.org/docs/)
- [Gu√≠a de Sqoop](https://sqoop.apache.org/docs/)
- [Docker para Data Engineering](https://docs.docker.com/)