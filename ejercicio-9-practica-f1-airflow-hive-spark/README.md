# Ejercicio 9 - PrÃ¡ctica F1: Airflow + Hive + Spark

Este ejercicio integra **Apache Airflow** para orquestaciÃ³n de workflows, **Apache Hive** para almacenamiento de datos estructurados y **Apache Spark** para procesamiento distribuido, utilizando datos reales de Formula 1 World Championship (1950-2020).

## ğŸ¯ Objetivos

- Crear tablas externas en Hive para almacenamiento de datos de Formula 1
- Desarrollar scripts de automatizaciÃ³n para ingesta de datos desde S3
- Procesar datos con Spark para generar resultados especÃ­ficos
- Orquestar todo el pipeline con Apache Airflow
- Implementar un flujo completo de ETL automatizado

## ğŸ“‹ Ejercicios Incluidos

### 1ï¸âƒ£ **ConfiguraciÃ³n de Base de Datos Hive**
- Crear base de datos `f1` en Hive
- Definir tabla externa `driver_results` con esquema especÃ­fico
- Definir tabla externa `constructor_results` con esquema especÃ­fico
- Verificar estructura y metadatos de las tablas

### 2ï¸âƒ£ **VerificaciÃ³n de Esquemas**
- Mostrar el esquema de `driver_results`
- Mostrar el esquema de `constructor_results`

### 3ï¸âƒ£ **Script de Ingesta Automatizada**
- Crear script bash para descarga de archivos CSV desde S3
- Implementar validaciones de conectividad y servicios
- Configurar subida automÃ¡tica a HDFS
- Incluir limpieza de archivos temporales

### 4ï¸âƒ£ **Procesamiento con Spark**
- Desarrollar script Python para procesamiento de datos
- Insertar en `driver_results` los corredores con mayor cantidad de puntos en la historia
- Insertar en `constructor_results` quienes obtuvieron mÃ¡s puntos en el Spanish Grand Prix en 1991

### 5ï¸âƒ£ **OrquestaciÃ³n con Airflow**
- Crear DAG para automatizaciÃ³n del pipeline
- Configurar tareas secuenciales y dependencias
- Implementar verificaciÃ³n de resultados
- Monitorear ejecuciÃ³n del workflow

## ğŸ“ Estructura del Ejercicio

```
ejercicio-9-practica-f1-airflow-hive-spark/
â”œâ”€â”€ README.md                    # DocumentaciÃ³n principal
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ f1_download_and_ingest.sh # Script de descarga e ingesta desde S3
â”‚   â”œâ”€â”€ process_f1_data.py       # Procesamiento con Spark
â”‚   â””â”€â”€ README.md               # DocumentaciÃ³n de scripts
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ f1_processing.py        # DAG de Airflow
â”‚   â””â”€â”€ README.md               # DocumentaciÃ³n de Airflow
â”œâ”€â”€ hive/
â”‚   â”œâ”€â”€ f1-setup.sql            # Scripts SQL de Hive
â”‚   â””â”€â”€ README.md               # DocumentaciÃ³n de Hive
â”œâ”€â”€ images/                     # Capturas de pantalla
â””â”€â”€ ejercicios-resueltos.md     # Resultados completos
```

## ğŸš€ TecnologÃ­as Utilizadas

- **Apache Airflow** - OrquestaciÃ³n de workflows
- **Apache Hive** - Data warehouse y consultas SQL
- **Apache Spark** - Procesamiento distribuido de datos
- **PySpark** - API de Python para Spark
- **HDFS** - Sistema de archivos distribuido
- **CSV** - Formato de datos delimitado
- **Bash Scripting** - AutomatizaciÃ³n de procesos
- **AWS S3** - Almacenamiento de datos fuente

## ğŸ“Š Dataset Utilizado

- **Fuente**: Formula 1 World Championship (1950-2020)
- **Archivos**: 
  - `results.csv` - Resultados de carreras
  - `drivers.csv` - InformaciÃ³n de corredores
  - `constructors.csv` - InformaciÃ³n de constructores
  - `races.csv` - InformaciÃ³n de carreras
- **Diccionario**: [Kaggle - Formula 1 Dataset](https://www.kaggle.com/datasets/rohanrao/formula-1-world-championship-1950-2020)

## ğŸ”— URLs de Datos

Los archivos CSV estÃ¡n disponibles en S3:

- `results.csv`: https://data-engineer-edvai-public.s3.amazonaws.com/results.csv
- `drivers.csv`: https://data-engineer-edvai-public.s3.amazonaws.com/drivers.csv
- `constructors.csv`: https://data-engineer-edvai-public.s3.amazonaws.com/constructors.csv
- `races.csv`: https://data-engineer-edvai-public.s3.amazonaws.com/races.csv

## ğŸ”§ Requisitos Previos

- Contenedor de Hadoop ejecutÃ¡ndose
- Apache Hive configurado y funcionando
- Apache Spark disponible en el ambiente
- Apache Airflow instalado y configurado
- Acceso a internet para descarga de archivos desde S3
- Conocimientos bÃ¡sicos de SQL, Python y Bash

## ğŸ“– GuÃ­as Adicionales

- **ConfiguraciÃ³n de Hive**: `hive/README.md`
- **Scripts de AutomatizaciÃ³n**: `scripts/README.md` (por crear)
- **ConfiguraciÃ³n de Airflow**: `airflow/README.md` (por crear)
- **Resultados de Ejercicios**: `ejercicios-resueltos.md` (por crear)

## ğŸ¯ Resultados Esperados

Al completar este ejercicio, habrÃ¡s:

1. âœ… Configurado una base de datos Hive con tablas externas para F1
2. âœ… Automatizado la descarga e ingesta de datos CSV desde S3 a HDFS
3. âœ… Procesado datos con Spark para encontrar top corredores y constructores
4. âœ… Orquestado todo el pipeline con Apache Airflow
5. âœ… Verificado la integridad de los datos procesados

## ğŸ“ Notas Importantes

- Las tablas externas apuntan a ubicaciones especÃ­ficas en HDFS
- Los datos deben ser procesados antes de insertarse en las tablas (JOINs entre CSV)
- Los scripts de Spark deben generar los archivos CSV con el formato correcto
- El DAG de Airflow debe ejecutarse en el orden correcto: descarga â†’ procesamiento â†’ verificaciÃ³n

