# ğŸ“Š Data Engineering Exercises and Practices

Este repositorio contiene ejercicios prÃ¡cticos de **ingenierÃ­a de datos** enfocados en la ingesta de datos usando **Hadoop**, **HDFS** y **Sqoop**. Incluye scripts automatizados para diferentes escenarios de ingesta.

---

## ğŸ¯ Objetivos

- Practicar la ingesta de datos desde fuentes externas hacia HDFS
- Automatizar procesos de ETL usando scripts bash
- Conectar bases de datos relacionales (PostgreSQL) con Hadoop usando Sqoop
- Gestionar archivos temporales y limpieza de datos

---

## ğŸ“ Estructura del Repositorio

```
data-engineering-exercises-and-practices/
â”œâ”€â”€ ejercicio-3-practica-de-ingest-local/     # Ingesta desde fuentes externas
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ landing.sh                        # Script de ingesta local
â”‚   â”œâ”€â”€ images/                               # Capturas de pantalla
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ ejercicio-4-practica-ingest-sqoop/        # Ingesta desde PostgreSQL
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ sqoop.sh                          # Script de comandos Sqoop
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ ejercicio-5-practica-sqoop-nifi/          # PrÃ¡ctica completa Sqoop + NiFi
â”‚   â”œâ”€â”€ practica-sqoop/                       # Ejercicios de Sqoop
â”‚   â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”‚   â””â”€â”€ sqoop-exercises.sh            # Script completo de Sqoop
â”‚   â”‚   â”œâ”€â”€ ejercicios-resueltos.md           # Resultados de ejercicios
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ practica-nifi/                        # Ejercicios de NiFi
â”‚   â”‚   â”œâ”€â”€ nifi-flow-diagram.md              # ExplicaciÃ³n del flujo
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ images/                               # Diagramas de flujo
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ ejercicio-6-practica-ingest-gcp/          # PrÃ¡ctica de ingesta en GCP
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ upload_csvs.sh                    # Script de subida a GCS
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ images/                               # Capturas de buckets y transfer
â”‚   â”‚   â”œâ”€â”€ bucket-multiregional-us.jpg
â”‚   â”‚   â”œâ”€â”€ bucket-regional-finlandia.jpg
â”‚   â”‚   â””â”€â”€ storage-transfer-service.jpg
â”‚   â”œâ”€â”€ ejercicios-resueltos.md               # Resultados de ejercicios
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ ejercicio-7-practica-nifi-spark/          # PrÃ¡ctica NiFi + Spark
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ download_parquet.sh               # Script de descarga
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ nifi/
â”‚   â”‚   â”œâ”€â”€ nifi-flow-guide.md                # GuÃ­a del flujo NiFi
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ spark/
â”‚   â”‚   â”œâ”€â”€ spark-analysis.md                 # AnÃ¡lisis con PySpark
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ images/                               # Capturas de flujo NiFi
â”‚   â”‚   â””â”€â”€ nifi-process.jpg
â”‚   â”œâ”€â”€ ejercicios-resueltos.md               # Resultados completos
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ ejercicio-8-practica-airflow-hive-spark/  # PrÃ¡ctica Airflow + Hive + Spark
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ download_and_ingest.sh            # Script de descarga e ingesta
â”‚   â”‚   â”œâ”€â”€ process_airport_trips.py          # Procesamiento con Spark
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â”œâ”€â”€ airport_trips_processing.py       # DAG de Airflow
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ hive/
â”‚   â”‚   â”œâ”€â”€ hive-setup.sql                    # Scripts SQL de Hive
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ images/                               # Capturas de pantalla
â”‚   â”‚   â”œâ”€â”€ ejercicio1-create-tripdata.jpg
â”‚   â”‚   â”œâ”€â”€ ejercicio2-describe-airporttrips.jpg
â”‚   â”‚   â”œâ”€â”€ ejercicio5-airflow.jpg
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ ejercicios-resueltos.md               # Resultados completos
â”‚   â””â”€â”€ README.md
â””â”€â”€ README.md                                 # Este archivo
```

---

## ğŸš€ Ejercicios Incluidos

### 1ï¸âƒ£ **Ejercicio 3: Ingesta Local con Hadoop**

**DescripciÃ³n:** Descarga un archivo CSV desde GitHub, lo mueve a HDFS y limpia archivos temporales.

**CaracterÃ­sticas:**
- âœ… Descarga automÃ¡tica desde GitHub
- âœ… GestiÃ³n de directorios temporales
- âœ… Subida a HDFS
- âœ… Limpieza automÃ¡tica de archivos

**Archivo:** `ejercicio-3-practica-de-ingest-local/scripts/landing.sh`

### 2ï¸âƒ£ **Ejercicio 4: Ingesta con Sqoop**

**DescripciÃ³n:** Conecta PostgreSQL con Hadoop usando Sqoop para importar datos.

**CaracterÃ­sticas:**
- âœ… ConexiÃ³n JDBC con PostgreSQL
- âœ… Listado de tablas
- âœ… Consultas SQL directas
- âœ… ImportaciÃ³n completa y filtrada
- âœ… Formato Parquet

**Archivo:** `ejercicio-4-practica-ingest-sqoop/scripts/sqoop.sh`

### 3ï¸âƒ£ **Ejercicio 5: PrÃ¡ctica Completa Sqoop + NiFi**

**DescripciÃ³n:** PrÃ¡ctica integral que combina Sqoop para ingesta de datos y NiFi para procesamiento de flujos de datos.

**CaracterÃ­sticas:**
- âœ… **Sqoop**: 4 ejercicios completos con resultados reales
- âœ… **NiFi**: ConfiguraciÃ³n y flujo de procesamiento
- âœ… **Scripts automatizados**: Comandos listos para ejecutar
- âœ… **DocumentaciÃ³n detallada**: GuÃ­as paso a paso
- âœ… **Diagramas visuales**: Flujos de NiFi explicados

**Archivos principales:**
- `ejercicio-5-practica-sqoop-nifi/practica-sqoop/scripts/sqoop-exercises.sh`
- `ejercicio-5-practica-sqoop-nifi/practica-nifi/README.md`
- `ejercicio-5-practica-sqoop-nifi/images/nifi-flow-diagram.jpg`

### 4ï¸âƒ£ **Ejercicio 6: PrÃ¡ctica Ingest GCP**

**DescripciÃ³n:** PrÃ¡ctica de ingesta de datos en Google Cloud Platform utilizando Google Cloud Storage y Storage Transfer Service.

**CaracterÃ­sticas:**
- âœ… **Buckets GCP**: CreaciÃ³n de buckets regionales y multiregionales
- âœ… **gsutil CLI**: Subida automatizada de archivos CSV
- âœ… **Storage Transfer**: MigraciÃ³n entre buckets
- âœ… **Scripts automatizados**: Procesos de subida masiva
- âœ… **Capturas visuales**: Evidencia de resultados

**Archivos principales:**
- `ejercicio-6-practica-ingest-gcp/scripts/upload_csvs.sh`
- `ejercicio-6-practica-ingest-gcp/ejercicios-resueltos.md`
- `ejercicio-6-practica-ingest-gcp/images/` (capturas de pantalla)

### 5ï¸âƒ£ **Ejercicio 7: PrÃ¡ctica NiFi + Spark**

**DescripciÃ³n:** PrÃ¡ctica integral que combina Apache NiFi para procesamiento de flujos de datos y Apache Spark para anÃ¡lisis de datos, utilizando un dataset real de taxis de NYC.

**CaracterÃ­sticas:**
- âœ… **NiFi**: Flujo de procesamiento con GetFile y PutHDFS
- âœ… **Spark**: AnÃ¡lisis de datos con PySpark y SQL
- âœ… **Dataset real**: Datos de taxis amarillos de NYC
- âœ… **Scripts automatizados**: Descarga y procesamiento
- âœ… **6 consultas de anÃ¡lisis**: Insights de negocio

**Archivos principales:**
- `ejercicio-7-practica-nifi-spark/scripts/download_parquet.sh`
- `ejercicio-7-practica-nifi-spark/nifi/nifi-flow-guide.md`
- `ejercicio-7-practica-nifi-spark/spark/spark-analysis.md`
- `ejercicio-7-practica-nifi-spark/ejercicios-resueltos.md`

### 6ï¸âƒ£ **Ejercicio 8: PrÃ¡ctica Airflow + Hive + Spark**

**DescripciÃ³n:** PrÃ¡ctica integral que integra Apache Airflow para orquestaciÃ³n de workflows, Apache Hive para almacenamiento de datos estructurados y Apache Spark para procesamiento distribuido, utilizando datos reales de taxis de Nueva York.

**CaracterÃ­sticas:**
- âœ… **Hive**: Base de datos y tabla externa con formato Parquet
- âœ… **Spark**: Procesamiento distribuido con filtros especÃ­ficos
- âœ… **Airflow**: OrquestaciÃ³n completa del pipeline ETL
- âœ… **Dataset real**: NYC Taxi Data (2.7M registros)
- âœ… **Pipeline automatizado**: Descarga â†’ Procesamiento â†’ VerificaciÃ³n
- âœ… **Filtros especÃ­ficos**: Aeropuertos + pago en efectivo

**Archivos principales:**
- `ejercicio-8-practica-airflow-hive-spark/scripts/download_and_ingest.sh`
- `ejercicio-8-practica-airflow-hive-spark/scripts/process_airport_trips.py`
- `ejercicio-8-practica-airflow-hive-spark/airflow/airport_trips_processing.py`
- `ejercicio-8-practica-airflow-hive-spark/hive/hive-setup.sql`
- `ejercicio-8-practica-airflow-hive-spark/ejercicios-resueltos.md`

---

## ğŸ”§ Requisitos Previos

### Entorno de Desarrollo
- **Docker** con contenedores de Hadoop y PostgreSQL
- **Bash** para ejecutar scripts
- Acceso a terminal del contenedor Hadoop

### Contenedores Necesarios
```bash
# Contenedor Hadoop
docker exec -it edvai_hadoop bash
su hadoop

# Verificar PostgreSQL
docker inspect edvai_postgres
```

---

## ğŸ“‹ CÃ³mo Ejecutar

### Ejercicio 3: Ingesta Local
```bash
# 1. Navegar al directorio
cd ejercicio-3-practica-de-ingest-local/scripts/

# 2. Dar permisos de ejecuciÃ³n
chmod +x landing.sh

# 3. Ejecutar el script
./landing.sh

# 4. Verificar en HDFS
hdfs dfs -ls /ingest
```

### Ejercicio 4: Ingesta con Sqoop
```bash
# 1. Navegar al directorio
cd ejercicio-4-practica-ingest-sqoop/scripts/

# 2. Dar permisos de ejecuciÃ³n
chmod +x sqoop.sh

# 3. Ejecutar el script
./sqoop.sh
```

### Ejercicio 5: PrÃ¡ctica Completa Sqoop + NiFi
```bash
# 1. Ejecutar ejercicios de Sqoop
cd ejercicio-5-practica-sqoop-nifi/practica-sqoop/scripts/
chmod +x sqoop-exercises.sh
./sqoop-exercises.sh

# 2. Configurar NiFi (ver documentaciÃ³n)
cd ../../practica-nifi/
# Seguir guÃ­a en README.md para configuraciÃ³n de NiFi
```

### Ejercicio 6: PrÃ¡ctica Ingest GCP
```bash
# 1. Ejecutar script de subida de archivos
cd ejercicio-6-practica-ingest-gcp/scripts/
chmod +x upload_csvs.sh
./upload_csvs.sh

# 2. Verificar archivos en buckets
gsutil ls gs://data-bucket-demo-1/
gsutil ls gs://demo-bucket-edvai/

# 3. Ver documentaciÃ³n completa
cd ../
# Revisar README.md y ejercicios-resueltos.md
```

### Ejercicio 7: PrÃ¡ctica NiFi + Spark
```bash
# 1. Ejecutar script de descarga
cd ejercicio-7-practica-nifi-spark/scripts/
chmod +x download_parquet.sh
./download_parquet.sh

# 2. Configurar flujo en NiFi (ver documentaciÃ³n)
cd ../nifi/
# Seguir guÃ­a en nifi-flow-guide.md

# 3. Ejecutar anÃ¡lisis con Spark
cd ../spark/
# Seguir guÃ­a en spark-analysis.md

# 4. Ver resultados completos
cd ../
# Revisar ejercicios-resueltos.md
```

### Ejercicio 8: PrÃ¡ctica Airflow + Hive + Spark
```bash
# 1. Configurar Hive (base de datos y tabla)
cd ejercicio-8-practica-airflow-hive-spark/hive/
# Seguir guÃ­a en README.md para configuraciÃ³n de Hive

# 2. Ejecutar scripts de procesamiento
cd ../scripts/
chmod +x download_and_ingest.sh
chmod +x process_airport_trips.py
./download_and_ingest.sh
spark-submit process_airport_trips.py

# 3. Configurar DAG de Airflow
cd ../airflow/
# Copiar airport_trips_processing.py a /home/hadoop/airflow/dags/

# 4. Ejecutar DAG en Airflow
# Acceder a interfaz web de Airflow y ejecutar DAG manualmente

# 5. Ver resultados completos
cd ../
# Revisar ejercicios-resueltos.md
```

---

## ğŸ“Š Datos Utilizados

### Ejercicio 3
- **Fuente:** [starwars.csv](https://github.com/fpineyro/homework-0/blob/master/starwars.csv)
- **Destino:** `/ingest/` en HDFS
- **Formato:** CSV

### Ejercicio 4
- **Fuente:** Base de datos Northwind (PostgreSQL)
- **Tabla:** `region`
- **Destino:** `/sqoop/ingest/` en HDFS
- **Formato:** Parquet

### Ejercicio 5
- **Sqoop**: Base de datos Northwind (PostgreSQL)
  - **Tablas:** `orders`, `products`, `customers`
  - **Destino:** `/sqoop/ingest/` en HDFS
  - **Formato:** Parquet
- **NiFi**: Archivo `starwars.csv`
  - **Procesamiento:** CSV â†’ Avro
  - **Destino:** `/nifi/` en HDFS
  - **Formato:** Avro

### Ejercicio 6
- **GCP Storage**: Archivos CSV locales
  - **Fuente**: Directorio local con 5 archivos CSV
  - **Destino**: `gs://data-bucket-demo-1/` (US multiregional)
  - **Formato:** CSV
- **Storage Transfer**: MigraciÃ³n entre buckets
  - **Origen**: `gs://data-bucket-demo-1/`
  - **Destino**: `gs://demo-bucket-edvai/` (Finlandia regional)
  - **Formato:** CSV

### Ejercicio 7
- **NiFi**: Archivo Parquet de NYC Taxi Data
  - **Fuente**: S3 (https://data-engineer-edvai-public.s3.amazonaws.com/)
  - **Procesamiento**: GetFile â†’ PutHDFS
  - **Destino**: `/nifi/` en HDFS
  - **Formato:** Parquet
- **Spark**: AnÃ¡lisis de datos
  - **Fuente**: HDFS `/nifi/yellow_tripdata_2021-01.parquet`
  - **Procesamiento**: PySpark + SQL
  - **Resultados**: 6 consultas de anÃ¡lisis
  - **Formato:** Parquet

### Ejercicio 8
- **Hive**: Base de datos y tabla externa
  - **Base de datos**: `tripdata`
  - **Tabla**: `airport_trips` (EXTERNAL TABLE)
  - **Formato**: Parquet
  - **UbicaciÃ³n**: `/user/hive/warehouse/tripdata.db/airport_trips`
- **Spark**: Procesamiento de datos NYC Taxi
  - **Fuente**: HDFS `/user/hadoop/tripdata/raw/` (2 archivos Parquet)
  - **Procesamiento**: PySpark con filtros especÃ­ficos
  - **Filtros**: Aeropuertos + pago en efectivo (payment_type = 2)
  - **Resultado**: 1 registro filtrado e insertado
  - **Formato:** Parquet
- **Airflow**: OrquestaciÃ³n del pipeline
  - **DAG**: `airport_trips_processing`
  - **Tareas**: Descarga â†’ Procesamiento â†’ VerificaciÃ³n
  - **Resultado**: Pipeline automatizado completo

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **Hadoop** - Framework de procesamiento distribuido
- **HDFS** - Sistema de archivos distribuido
- **Sqoop** - Herramienta de transferencia de datos
- **NiFi** - Procesamiento de flujos de datos
- **Apache Spark** - Motor de procesamiento distribuido
- **PySpark** - API de Python para Spark
- **Apache Airflow** - OrquestaciÃ³n de workflows
- **Apache Hive** - Data warehouse y consultas SQL
- **Google Cloud Storage** - Almacenamiento de objetos en la nube
- **gsutil** - Herramienta CLI de Google Cloud
- **Storage Transfer Service** - MigraciÃ³n de datos en GCP
- **PostgreSQL** - Base de datos relacional
- **Bash** - Scripting y automatizaciÃ³n
- **Docker** - Contenedores
- **Parquet** - Formato de datos columnar
- **Avro** - Formato de serializaciÃ³n de datos
- **SQL** - Consultas estructuradas

---

## ğŸ“ Notas Importantes

- Los scripts incluyen manejo de errores y mensajes informativos
- Se realizan limpiezas automÃ¡ticas de archivos temporales
- Los directorios se crean automÃ¡ticamente si no existen
- Las conexiones JDBC requieren configuraciÃ³n de red entre contenedores
- NiFi requiere configuraciÃ³n adicional de librerÃ­as HDFS para versiones 2.0+
- Los ejercicios incluyen resultados reales y mÃ©tricas de rendimiento
- GCP requiere autenticaciÃ³n y configuraciÃ³n de proyecto
- Storage Transfer Service permite migraciÃ³n eficiente entre buckets
- Los buckets regionales optimizan latencia, los multiregionales optimizan disponibilidad
- Spark requiere configuraciÃ³n de memoria y paralelismo para optimizar rendimiento
- Los archivos Parquet ofrecen mejor compresiÃ³n y rendimiento que CSV
- Las consultas SQL en Spark permiten anÃ¡lisis eficiente de grandes volÃºmenes de datos
- Airflow requiere configuraciÃ³n de DAGs y monitoreo de tareas
- Hive necesita configuraciÃ³n de metastore y permisos de HDFS
- Los DAGs de Airflow permiten orquestaciÃ³n compleja de pipelines ETL
- Las tablas externas en Hive facilitan la integraciÃ³n con Spark
- Los filtros especÃ­ficos en Spark optimizan el procesamiento de grandes datasets

---

## ğŸ‘¨â€ğŸ’» Autor

**Enzo** - PrÃ¡cticas de Data Engineering

---

## ğŸ“š Recursos Adicionales

- [DocumentaciÃ³n de Hadoop](https://hadoop.apache.org/docs/)
- [GuÃ­a de Sqoop](https://sqoop.apache.org/docs/)
- [Docker para Data Engineering](https://docs.docker.com/)