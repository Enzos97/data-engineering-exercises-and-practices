# Scripts - Ejercicio 8

Esta carpeta contiene los scripts necesarios para automatizar el pipeline de procesamiento de datos NYC Taxi.

## ğŸ“ Archivos Incluidos

### 1ï¸âƒ£ **download_and_ingest.sh**
Script de bash para descarga e ingesta automÃ¡tica de archivos Parquet.

**Funcionalidades:**
- âœ… VerificaciÃ³n de servicios HDFS
- âœ… Descarga de archivos desde URLs pÃºblicas
- âœ… ValidaciÃ³n de conectividad
- âœ… Subida automÃ¡tica a HDFS
- âœ… Limpieza de archivos temporales
- âœ… Logging detallado del proceso

**Uso:**
```bash
chmod +x download_and_ingest.sh
./download_and_ingest.sh
```

### 2ï¸âƒ£ **process_airport_trips.py**
Script de Python para procesamiento de datos con Spark.

**Funcionalidades:**
- âœ… Lectura de archivos Parquet desde HDFS
- âœ… UniÃ³n de datasets de enero y febrero 2021
- âœ… Filtrado de viajes a aeropuertos pagados en efectivo
- âœ… InserciÃ³n de resultados en tabla Hive
- âœ… EstadÃ­sticas y validaciones de datos

**Uso:**
```bash
spark-submit process_airport_trips.py
```

## ğŸ”§ ConfiguraciÃ³n Requerida

### Variables de Entorno
```bash
export HADOOP_HOME=/home/hadoop/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

### Servicios Necesarios
- **HDFS**: NameNode y DataNode ejecutÃ¡ndose
- **Hive**: Metastore configurado
- **Spark**: Disponible en el PATH
- **Internet**: Para descarga de archivos

## ğŸ“Š URLs de Datos

- **Enero 2021**: `https://data-engineer-edvai-public.s3.amazonaws.com/yellow_tripdata_2021-01.parquet`
- **Febrero 2021**: `https://data-engineer-edvai-public.s3.amazonaws.com/yellow_tripdata_2021-02.parquet`

## ğŸ¯ Resultados Esperados

### Descarga e Ingesta
- **Archivos descargados**: 2 archivos Parquet (~41.5 MB total)
- **UbicaciÃ³n HDFS**: `/user/hadoop/tripdata/raw/`
- **Tiempo estimado**: 30-60 segundos

### Procesamiento Spark
- **Registros procesados**: ~2.7 millones
- **Filtros aplicados**: Viajes a aeropuertos + pago efectivo
- **Resultado final**: 1 registro insertado en Hive
- **Tiempo estimado**: 1-2 minutos

## ğŸš¨ Troubleshooting

### Error: "Servicios HDFS no detectados"
```bash
# Verificar servicios
jps | grep -E "NameNode|DataNode"

# Iniciar servicios si es necesario
start-dfs.sh
```

### Error: "No hay conexiÃ³n a internet"
```bash
# Verificar conectividad
ping google.com
wget --spider https://data-engineer-edvai-public.s3.amazonaws.com/yellow_tripdata_2021-01.parquet
```

### Error: "Spark session failed"
```bash
# Verificar configuraciÃ³n de Spark
spark-submit --version
# Verificar configuraciÃ³n de Hive
hive --version
```

## ğŸ“ Logs y Monitoreo

Los scripts incluyen logging detallado que muestra:
- âœ… Estado de cada paso del proceso
- ğŸ“Š MÃ©tricas de rendimiento
- âŒ Errores especÃ­ficos con cÃ³digos de salida
- ğŸ“… Timestamps de inicio y finalizaciÃ³n

## ğŸ”„ IntegraciÃ³n con Airflow

Estos scripts estÃ¡n diseÃ±ados para ser ejecutados como tareas de Airflow:
- **BashOperator**: Para `download_and_ingest.sh`
- **BashOperator**: Para `process_airport_trips.py`
- **Dependencias**: Secuenciales con validaciones
