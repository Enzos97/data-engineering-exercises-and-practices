# Ejercicio Final 2 - Car Rental Analytics: Airflow + PySpark + Hive

Este ejercicio implementa un pipeline ETL completo para an√°lisis de alquileres de autom√≥viles usando **Apache Airflow** con patr√≥n Padre-Hijo, **PySpark** para transformaciones y JOINs complejos, y **Apache Hive** para almacenamiento y an√°lisis SQL de datos de car rental.

## üéØ Objetivos

- Crear Data Warehouse en Hive con schema espec√≠fico
- Ingestar 2 datasets desde S3 (car rental + georef USA)
- Desarrollar script PySpark con transformaciones y JOIN
- Orquestar con Airflow usando patr√≥n Padre-Hijo (2 DAGs)
- Aplicar an√°lisis de negocio con 6 consultas SQL
- Elaborar conclusiones y proponer arquitectura alternativa

## üìã Ejercicios Incluidos

### 1Ô∏è‚É£ **Crear Hive Database y Tabla (Punto 1)**
- Base de datos: `car_rental_db`
- Tabla: `car_rental_analytics`
- Schema con 11 columnas espec√≠ficas

### 2Ô∏è‚É£ **Script de Ingest (Punto 2)**
- Descargar `CarRentalData.csv` desde S3
- Descargar `georef-united-states-of-america-state.csv`
- Subir ambos archivos a HDFS `/car_rental/raw`

### 3Ô∏è‚É£ **Script Spark con Transformaciones (Punto 3)**
- Renombrar columnas (sin espacios, sin puntos)
- Redondear `rating` float ‚Üí int
- JOIN car_rental + georef por state code
- Eliminar registros con rating NULL
- Convertir `fuelType` a min√∫sculas
- Excluir el estado de Texas
- Insertar resultado en Hive

### 4Ô∏è‚É£ **Orquestaci√≥n Airflow (Punto 4)**
- **DAG Padre**: Ingesta archivos ‚Üí Dispara DAG hijo
- **DAG Hijo**: Procesa datos ‚Üí Carga en Hive
- Patr√≥n Parent-Child con `TriggerDagRunOperator`

### 5Ô∏è‚É£ **Consultas de Negocio (Punto 5)**

#### 5a. Alquileres ecol√≥gicos con rating >= 4
- Filtro: hybrid o electric con rating ‚â• 4

#### 5b. Top 5 estados con menor cantidad de alquileres
- GROUP BY + ORDER BY ASC

#### 5c. Top 10 modelos m√°s rentados (con marca)
- GROUP BY make, model

#### 5d. Alquileres por a√±o (2010-2015)
- An√°lisis temporal con estad√≠sticas

#### 5e. Top 5 ciudades con m√°s alquileres ecol√≥gicos
- Filtro: hybrid o electric

#### 5f. Promedio de reviews por tipo de combustible
- Segmentaci√≥n por fuelType

### 6Ô∏è‚É£ **Conclusiones y Recomendaciones (Punto 6)**
- An√°lisis de resultados
- Insights de negocio
- Recomendaciones estrat√©gicas

### 7Ô∏è‚É£ **Arquitectura Alternativa (Punto 7)**
- Propuesta on-premise o cloud
- Comparativa de tecnolog√≠as

## üìÅ Estructura del Proyecto

```
ejercicios-Finales/ejercicio-2/
‚îú‚îÄ‚îÄ README.md                         # Este archivo
‚îú‚îÄ‚îÄ GUIA_EJECUCION.md                 # Gu√≠a paso a paso completa
‚îú‚îÄ‚îÄ INICIO_RAPIDO.md                  # Quick start guide
‚îú‚îÄ‚îÄ RESUMEN_PROYECTO.md               # Resumen ejecutivo
‚îú‚îÄ‚îÄ CONCLUSIONES_Y_ARQUITECTURA.md    # Puntos 6 y 7
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ download_data.sh             # Script bash para descarga e ingest
‚îÇ   ‚îî‚îÄ‚îÄ process_car_rental.py        # Script PySpark de transformaci√≥n
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îú‚îÄ‚îÄ car_rental_parent_dag.py     # DAG Padre (ingesta)
‚îÇ   ‚îú‚îÄ‚îÄ car_rental_child_dag.py      # DAG Hijo (procesamiento)
‚îÇ   ‚îî‚îÄ‚îÄ README.md                     # Documentaci√≥n de DAGs
‚îú‚îÄ‚îÄ hive/
‚îÇ   ‚îú‚îÄ‚îÄ car_rental_setup.sql         # CREATE DATABASE + TABLE
‚îÇ   ‚îú‚îÄ‚îÄ queries.sql                  # Consultas de negocio (Punto 5)
‚îÇ   ‚îî‚îÄ‚îÄ README.md                     # Documentaci√≥n de Hive
‚îî‚îÄ‚îÄ images/                           # Capturas de pantalla
```

## üöÄ Tecnolog√≠as Utilizadas

- **Apache Spark (PySpark)** - Procesamiento distribuido y JOIN
- **Apache Airflow** - Orquestaci√≥n con patr√≥n Padre-Hijo
- **Apache Hive** - Data warehouse y consultas SQL
- **HDFS** - Sistema de archivos distribuido
- **Python** - Lenguaje de programaci√≥n
- **Bash** - Scripting y automatizaci√≥n
- **Docker** - Contenedorizaci√≥n

## üìä Datasets Utilizados

### CarRentalData.csv
- **URL**: https://data-engineer-edvai-public.s3.amazonaws.com/CarRentalData.csv
- **Registros**: ~10,000 alquileres
- **Formato**: CSV con estructura JSON anidada
- **Campos principales**: fuelType, rating, renterTripsTaken, reviewCount, location (city, state, lat, lng), vehicle (make, model, year), rate (daily), owner (id)

### georef-united-states-of-america-state.csv
- **URL**: https://data-engineer-edvai-public.s3.amazonaws.com/georef-united-states-of-america-state.csv
- **Registros**: 51 estados USA
- **Formato**: CSV con delimitador `;`
- **Campos principales**: Official Code State, Official Name State, United States Postal Service state abbreviation

### Schema Final: car_rental_analytics

| Campo | Tipo | Descripci√≥n |
|-------|------|-------------|
| fuelType | STRING | Tipo de combustible (diesel, electric, gasoline, hybrid, other) |
| rating | INT | Rating del veh√≠culo (1-5, redondeado) |
| renterTripsTaken | INT | Cantidad de viajes del arrendatario |
| reviewCount | INT | Cantidad de rese√±as del veh√≠culo |
| city | STRING | Ciudad donde se encuentra el veh√≠culo |
| state_name | STRING | Nombre completo del estado (ej: California) |
| owner_id | INT | ID del propietario del veh√≠culo |
| rate_daily | INT | Tarifa diaria de alquiler |
| make | STRING | Marca del veh√≠culo (ej: Toyota, Honda) |
| model | STRING | Modelo del veh√≠culo (ej: Camry, Accord) |
| year | INT | A√±o de fabricaci√≥n del veh√≠culo |

## üîß Requisitos Previos

- Contenedor Hadoop/Hive ejecut√°ndose
- Apache Spark instalado y configurado
- Apache Airflow instalado y funcionando
- Python 3.8+ con PySpark
- HDFS accesible (hdfs://172.17.0.2:9000)
- Java 11 instalado
- Acceso a internet para descarga de datos

## üöÄ Pipeline Completo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         PASO 1: HIVE SETUP (Crear DB y Tabla)               ‚îÇ
‚îÇ  CREATE DATABASE car_rental_db;                              ‚îÇ
‚îÇ  CREATE TABLE car_rental_analytics (...);                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         PASO 2: INGEST (Descarga + HDFS)                     ‚îÇ
‚îÇ  CSV Files (S3) ‚Üí /tmp/car_rental ‚Üí HDFS:/car_rental/raw   ‚îÇ
‚îÇ  ‚Ä¢ CarRentalData.csv (~600 KB)                              ‚îÇ
‚îÇ  ‚Ä¢ georef_usa_states.csv (~12 KB)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         PASO 3: PROCESAMIENTO (PySpark)                      ‚îÇ
‚îÇ  process_car_rental.py                                       ‚îÇ
‚îÇ  1. Leer CSV desde HDFS                                      ‚îÇ
‚îÇ  2. Renombrar columnas (location.city ‚Üí city)               ‚îÇ
‚îÇ  3. Redondear rating (float ‚Üí int)                          ‚îÇ
‚îÇ  4. Eliminar rating NULL                                     ‚îÇ
‚îÇ  5. fuelType a min√∫sculas                                    ‚îÇ
‚îÇ  6. Excluir Texas (state != 'TX')                           ‚îÇ
‚îÇ  7. JOIN con georef (state_code ‚Üí state_name)               ‚îÇ
‚îÇ  8. Seleccionar 11 columnas finales                         ‚îÇ
‚îÇ  9. Escribir a Hive (.saveAsTable)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         PASO 4: ORQUESTACI√ìN (Airflow)                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ  ‚îÇ   DAG PADRE (car_rental_parent_dag) ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ   inicio ‚Üí crear_tabla ‚Üí download   ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ        ‚Üí verificar ‚Üí trigger_hijo    ‚îÇ                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ                 ‚îÇ TriggerDagRunOperator                      ‚îÇ
‚îÇ                 ‚ñº                                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ  ‚îÇ   DAG HIJO (car_rental_child_dag)   ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ   inicio ‚Üí spark_process ‚Üí verificar‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ        ‚Üí estadisticas ‚Üí fin          ‚îÇ                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         PASO 5: AN√ÅLISIS (Hive SQL - 6 Consultas)           ‚îÇ
‚îÇ  5a. Alquileres ecol√≥gicos rating >= 4                      ‚îÇ
‚îÇ  5b. Top 5 estados con menos alquileres                     ‚îÇ
‚îÇ  5c. Top 10 modelos m√°s rentados                            ‚îÇ
‚îÇ  5d. Alquileres por a√±o (2010-2015)                         ‚îÇ
‚îÇ  5e. Top 5 ciudades ecol√≥gicas                              ‚îÇ
‚îÇ  5f. Promedio reviews por combustible                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìñ Gu√≠as de Uso

### Ejecuci√≥n Paso a Paso (Como se Ejecut√≥ en Consola)

#### PASO 1: Crear Tabla en Hive

```bash
# Terminal 1: Acceder al contenedor
docker exec -it edvai_hadoop bash
su hadoop

# Entrar a Hive
hive
```

Dentro de Hive CLI:

```sql
CREATE DATABASE IF NOT EXISTS car_rental_db;

USE car_rental_db;

CREATE TABLE IF NOT EXISTS car_rental_analytics (
    fuelType STRING,
    rating INT,
    renterTripsTaken INT,
    reviewCount INT,
    city STRING,
    state_name STRING,
    owner_id INT,
    rate_daily INT,
    make STRING,
    model STRING,
    year INT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
TBLPROPERTIES ("skip.header.line.count"="0");

SHOW TABLES;

DESCRIBE FORMATTED car_rental_analytics;

SELECT COUNT(*) as total_registros FROM car_rental_analytics;

exit;
```

#### PASO 2: Crear y Ejecutar Script de Ingest

```bash
# Terminal 2: Abrir otra terminal y acceder al contenedor
docker exec -it edvai_hadoop bash
su hadoop
cd /home/hadoop/scripts

# Crear el archivo con nano
nano download_data.sh
```

**Copiar el contenido completo del script desde:**  
`ejercicios-Finales/ejercicio-2/scripts/download_data.sh`

**Guardar y salir de nano:**
```
Ctrl + O ‚Üí Enter ‚Üí Ctrl + X
```

**Dar permisos y ejecutar:**

```bash
chmod +x download_data.sh
./download_data.sh
```

**Salida esperada:**
```
==========================================
üöó CAR RENTAL DATA DOWNLOAD
==========================================
‚úÖ Directorio creado: /tmp/car_rental
üì• Paso 2: Descargando CarRentalData.csv...
‚úÖ Descarga exitosa: CarRentalData.csv
üì• Paso 3: Descargando georef USA states...
‚úÖ Descarga exitosa: georef_usa_states.csv
‚¨ÜÔ∏è  Paso 8: Subiendo archivos a HDFS...
‚úÖ DESCARGA COMPLETADA EXITOSAMENTE
```

#### PASO 3: Crear y Ejecutar Script de Procesamiento Spark

```bash
# En la misma terminal (o abrir otra)
docker exec -it edvai_hadoop bash
su hadoop
cd /home/hadoop/scripts

# Crear el archivo con nano
nano process_car_rental.py
```

**Copiar el contenido completo del script desde:**  
`ejercicios-Finales/ejercicio-2/scripts/process_car_rental.py`

**Guardar y salir de nano:**
```
Ctrl + O ‚Üí Enter ‚Üí Ctrl + X
```

**Dar permisos y ejecutar:**

```bash
chmod +x process_car_rental.py
spark-submit ./process_car_rental.py
```

**Salida esperada:**
```
============================================================
üöó CAR RENTAL DATA PROCESSING
============================================================
‚úÖ Sesi√≥n de Spark creada exitosamente
‚úÖ Datos cargados: 10085 registros
‚úÖ Transformaciones aplicadas
‚úÖ JOIN completado
‚úÖ Datos insertados en Hive
‚úÖ PROCESAMIENTO COMPLETADO EXITOSAMENTE
```

#### PASO 4: Crear DAGs de Airflow

```bash
# Acceder al contenedor
docker exec -it edvai_hadoop bash
su hadoop
cd /home/hadoop/airflow/dags

# Crear DAG Padre
nano car_rental_parent_dag.py
```

**Copiar el contenido completo del script desde:**  
`ejercicios-Finales/ejercicio-2/airflow/car_rental_parent_dag.py`

**Guardar:** `Ctrl + O ‚Üí Enter ‚Üí Ctrl + X`

```bash
# Crear DAG Hijo
nano car_rental_child_dag.py
```

**Copiar el contenido completo del script desde:**  
`ejercicios-Finales/ejercicio-2/airflow/car_rental_child_dag.py`

**Guardar:** `Ctrl + O ‚Üí Enter ‚Üí Ctrl + X`

**Dar permisos:**

```bash
chmod +x car_rental_parent_dag.py
chmod +x car_rental_child_dag.py
```

#### PASO 5: Ejecutar DAGs en Airflow

**Opci√≥n A: Desde la UI de Airflow**

1. Acceder a `http://localhost:8080`
2. Buscar `car_rental_parent_dag`
3. Activar el toggle
4. Click en "Trigger DAG"
5. Monitorear ejecuci√≥n (el DAG hijo se dispara autom√°ticamente)

**Opci√≥n B: Desde CLI**

```bash
# Reiniciar scheduler (si es necesario)
pkill -f "airflow scheduler"
sleep 3
nohup airflow scheduler > /tmp/scheduler.log 2>&1 &

# Activar DAGs
airflow dags unpause car_rental_parent_dag
airflow dags unpause car_rental_child_dag

# Verificar que est√°n activos
airflow dags list | grep car_rental

# Ejecutar DAG padre (dispara el hijo autom√°ticamente)
airflow dags trigger car_rental_parent_dag
```

#### PASO 6: Verificar Datos en Hive

```bash
# Volver a Hive para verificar
hive -e "USE car_rental_db; SELECT COUNT(*) FROM car_rental_analytics;"

# Ver muestra de datos
hive -e "USE car_rental_db; SELECT * FROM car_rental_analytics LIMIT 5;"
```

## üéØ Resultados Obtenidos (Reales)

### Datos Procesados
- **Total registros procesados**: 4,844 alquileres
- **Estados √∫nicos**: 50 (sin Texas)
- **Tipos de combustible**: 4 (diesel, electric, gasoline, hybrid)
- **Rating m√≠nimo**: 1
- **Rating m√°ximo**: 5
- **A√±os de veh√≠culos**: 1990-2024

### An√°lisis de Negocio

#### **Punto 5a - Alquileres ecol√≥gicos con rating >= 4**

**Total: 771 alquileres ecol√≥gicos**

| Tipo | Cantidad | Rating Promedio | Total Viajes |
|------|----------|-----------------|--------------|
| Electric | 542 | 4.99 | 17,601 |
| Hybrid | 229 | 4.99 | 9,348 |

**Insight:** Los veh√≠culos el√©ctricos representan el 70% de los alquileres ecol√≥gicos con excelente rating.

---

#### **Punto 5b - Top 5 estados con menos alquileres**

| Estado | Total Alquileres | Rating Promedio | Tarifa Diaria Promedio |
|--------|------------------|-----------------|------------------------|
| 1. Montana | 1 | 5.0 | $74.00 |
| 2. West Virginia | 3 | 5.0 | $59.33 |
| 3. New Hampshire | 3 | 5.0 | $83.00 |
| 4. Delaware | 4 | 5.0 | $54.50 |
| 5. Mississippi | 4 | 5.0 | $41.75 |

**Insight:** Estados con baja densidad poblacional tienen pocos alquileres pero ratings perfectos (5.0).

---

#### **Punto 5c - Top 10 modelos m√°s rentados (con marca)**

| Posici√≥n | Marca | Modelo | Alquileres | Rating | Tarifa/D√≠a | Total Viajes |
|----------|-------|--------|------------|--------|------------|--------------|
| 1 | Tesla | Model 3 | 288 | 4.98 | $128.01 | 9,794 |
| 2 | Ford | Mustang | 136 | 4.96 | $74.87 | 5,882 |
| 3 | Tesla | Model S | 122 | 4.98 | $135.42 | 3,952 |
| 4 | Jeep | Wrangler | 108 | 4.99 | $78.25 | 4,762 |
| 5 | Tesla | Model X | 103 | 4.99 | $192.70 | 3,638 |
| 6 | Toyota | Corolla | 78 | 4.96 | $35.55 | 4,676 |
| 7 | Mercedes-Benz | C-Class | 78 | 4.96 | $79.27 | 2,818 |
| 8 | BMW | 3 Series | 76 | 4.99 | $62.62 | 3,293 |
| 9 | Chevrolet | Corvette | 68 | 4.99 | $176.21 | 4,164 |
| 10 | Chevrolet | Camaro | 61 | 5.0 | $87.02 | 2,797 |

**Insights clave:**
- üöó **Tesla domina el mercado**: 3 modelos en el top 5 (513 alquileres, 35% del top 10)
- üí∞ **Econom√≠a vs. Premium**: Corolla ($35.55) vs. Model X ($192.70)
- ‚≠ê **Excelentes ratings**: Todos los modelos >4.96, Camaro con 5.0 perfecto

---

#### **Punto 5d - Alquileres por a√±o (2010-2015)**

| A√±o | Alquileres | Rating Promedio | Tarifa/D√≠a Promedio | Marcas √önicas | Total Viajes |
|-----|------------|-----------------|---------------------|---------------|--------------|
| 2010 | 144 | 4.97 | $61.01 | 30 | 6,754 |
| 2011 | 200 | 4.98 | $69.72 | 30 | 8,141 |
| 2012 | 225 | 4.97 | $60.83 | 28 | 9,999 |
| 2013 | 305 | 4.97 | $78.30 | 35 | 12,328 |
| 2014 | 382 | 4.98 | $84.48 | 36 | 15,477 |
| 2015 | 532 | 4.98 | $94.53 | 37 | 18,799 |
| **TOTAL** | **1,788** | **4.97** | **$75.81** | **46** | **71,498** |

**Estad√≠sticas del periodo:**
- üìà **Crecimiento sostenido**: De 144 (2010) a 532 (2015) alquileres/a√±o (+269%)
- üíµ **Aumento de tarifas**: De $61 (2010) a $94.53 (2015) por d√≠a (+55%)
- üè≠ **Diversidad**: 46 marcas diferentes, 302 modelos distintos

---

#### **Punto 5e - Top 5 ciudades con m√°s alquileres ecol√≥gicos**

| Ciudad | Estado | Total Ecol√≥gicos | Rating | H√≠bridos | El√©ctricos | Tarifa/D√≠a |
|--------|--------|------------------|--------|----------|------------|------------|
| 1. San Diego | California | 44 | 5.0 | 13 | 31 | $105.68 |
| 2. Las Vegas | Nevada | 34 | 4.97 | 2 | 32 | $145.47 |
| 3. Portland | Oregon | 20 | 5.0 | 4 | 16 | $115.00 |
| 4. Phoenix | Arizona | 17 | 5.0 | 9 | 8 | $90.82 |
| 5. San Jose | California | 15 | 5.0 | 4 | 11 | $90.53 |

**Insights:**
- üå¥ **California lidera**: 2 ciudades en el top 5 (San Diego + San Jose)
- ‚ö° **Preferencia el√©ctrica**: 88 el√©ctricos vs 32 h√≠bridos (73% vs 27%)
- ‚≠ê **Ratings perfectos**: 4 de 5 ciudades con rating 5.0
- üí∞ **Tarifas premium**: Las Vegas m√°s cara ($145.47/d√≠a)

---

#### **Punto 5f - Promedio de reviews por tipo de combustible**

| Tipo Combustible | Veh√≠culos | Promedio Reviews | M√≠n | M√°x | Rating | Total Reviews |
|------------------|-----------|------------------|-----|-----|--------|---------------|
| 1. **Hybrid** | 229 | **34.87** | 1 | 193 | 4.99 | 7,986 |
| 2. **Gasoline** | 4,015 | **31.93** | 1 | 321 | 4.98 | 128,187 |
| 3. **Electric** | 542 | **28.34** | 1 | 248 | 4.99 | 15,360 |
| 4. **Diesel** | 58 | **17.50** | 1 | 103 | 4.98 | 1,015 |

**Insights clave:**
- üå± **H√≠bridos m√°s comentados**: 34.87 reviews promedio (mayor engagement)
- ‚õΩ **Gasolina domina volumen**: 4,015 veh√≠culos (83% del total)
- ‚ö° **El√©ctricos en crecimiento**: 542 veh√≠culos, rating 4.99
- üöú **Diesel minoritario**: Solo 58 veh√≠culos (1.2% del mercado)

**Conclusi√≥n General:**
- Rating promedio general: **4.98/5.0** (excelente satisfacci√≥n)
- Veh√≠culos ecol√≥gicos (hybrid + electric): **771 unidades** (15.9% del total)
- Tendencia clara hacia electrificaci√≥n de la flota

## üìù Notas Importantes

### Transformaciones Cr√≠ticas

**1. Manejo de Columnas Anidadas**

El archivo `CarRentalData.csv` tiene estructura JSON anidada:
```python
# Problema: location.city, location.state
# Soluci√≥n: Usar backticks y alias
col("`location.city`").alias("city")
col("`location.state`").alias("state")
```

**2. Mapeo del State Code**

```python
# Problema: FIPS code vs USPS abbreviation
# Soluci√≥n: Usar columna correcta para JOIN
df_states.select(
    col("`United States Postal Service state abbreviation`").alias("state_code")
)
```

**3. Exclusi√≥n de Texas**

```python
# Aplicar ANTES del JOIN para eficiencia
df_rental_sin_texas = df_rental.filter(col("state") != "TX")
```

### Patr√≥n Airflow Padre-Hijo

**DAG Padre**:
- Responsabilidad: Ingesta de datos
- Dispara: DAG Hijo usando `TriggerDagRunOperator`
- Configuraci√≥n: `wait_for_completion=False`

**DAG Hijo**:
- Responsabilidad: Procesamiento y carga
- Se ejecuta solo cuando es disparado por el padre
- Configuraci√≥n: `schedule_interval=None`

## üîß Troubleshooting

### Problema: "cannot resolve 'location.city'"
**Causa**: Columnas anidadas no reconocidas  
**Soluci√≥n**: Usar backticks `` `location.city` ``

### Problema: "DataFrame est√° vac√≠o despu√©s de JOIN"
**Causa**: Mismatch en state codes (FIPS vs USPS)  
**Soluci√≥n**: Usar `United States Postal Service state abbreviation`

### Problema: "DAG hijo no se ejecuta"
**Causa**: DAG hijo pausado o no detectado  
**Soluci√≥n**:
```bash
airflow dags list | grep car_rental
airflow dags unpause car_rental_child_dag
```

### Problema: "JAVA_HOME incorrecto"
**Causa**: Script apunta a Java 8  
**Soluci√≥n**:
```bash
# En download_data.sh, cambiar a:
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

## üîó Referencias

- [PySpark SQL Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)
- [Hive Language Manual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual)
- [Airflow TriggerDagRunOperator](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/trigger_dagrun.html)
- [Cornell Car Rental Dataset](https://www.kaggle.com/datasets/kushleshkumar/cornell-car-rental-dataset)

## üìß Contacto

Para consultas sobre el pipeline de car rental analytics, contactar al equipo de Data Engineering de Edvai.

---

**Cliente**: Car Rental Analytics Company  
**Autor**: Data Engineering Team - Edvai  
**Fecha**: 2025-11-24  
**Versi√≥n**: 2.0 (PySpark + Airflow Parent-Child Pattern)  
**Tecnolog√≠a Principal**: Apache Spark + Airflow + Hive
