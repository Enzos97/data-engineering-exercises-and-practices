# revision de warehouse (por simple curiosidad)

enzo@Nitro-Enzo:~$ docker exec -it edvai_hadoop bash
root@cd7195a9ebf6:/# su hadoop
hadoop@cd7195a9ebf6:/$  hdfs dfs -ls /user/hive/warehouse/
Found 3 items
drwxrwxr-x   - hadoop supergroup          0 2022-05-02 14:06 /user/hive/warehouse/emp.db
drwxrwxr-x   - hadoop supergroup          0 2022-05-09 18:00 /user/hive/warehouse/tables
drwxrwxr-x   - hadoop supergroup          0 2025-10-22 20:12 /user/hive/warehouse/tripdata.db

# 1. Crear la siguientes tablas externas en la base de datos f1 en hive: 

# iniciamos Hive
hadoop@cd7195a9ebf6:/$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/hadoop/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/hadoop/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/home/hadoop/hive/lib/hive-common-2.3.9.jar!/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.hadoop.hive.common.StringInternUtils (file:/home/hadoop/hive/lib/hive-common-2.3.9.jar) to field java.net.URI.string
WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hive.common.StringInternUtils
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release

# Crear base de datos f1 si no existe

hive> CREATE DATABASE IF NOT EXISTS f1;
OK
Time taken: 1.749 seconds

# Usar la base de datos f1

hive> USE f1;
OK
Time taken: 0.081 seconds

#Tabla externa driver_results

hive> CREATE EXTERNAL TABLE IF NOT EXISTS driver_results (
    >     driver_forename STRING,
    >     driver_surname STRING,
    >     driver_nationality STRING,
    >     points DOUBLE
    > )
    > ROW FORMAT DELIMITED
    > FIELDS TERMINATED BY ','
    > STORED AS TEXTFILE
    > LOCATION '/user/hive/warehouse/f1.db/driver_results'
    > TBLPROPERTIES ('skip.header.line.count'='1');
OK
Time taken: 0.422 seconds

# Tabla externa constructor_results

hive> CREATE EXTERNAL TABLE IF NOT EXISTS constructor_results (
    >     constructorRef STRING,
    >     cons_name STRING,
    >     cons_nationality STRING,
    >     url STRING,
    >     points DOUBLE
    > )
    > ROW FORMAT DELIMITED
    > FIELDS TERMINATED BY ','
    > STORED AS TEXTFILE
    > LOCATION '/user/hive/warehouse/f1.db/constructor_results'
    > TBLPROPERTIES ('skip.header.line.count'='1');
OK
Time taken: 0.137 seconds

# ejercicio 2: VERIFICACIONES

#Mostrar todas las tablas en la base de datos

hive> SHOW TABLES;
OK
constructor_results
driver_results
Time taken: 0.137 seconds, Fetched: 2 row(s)

#Describir la estructura de driver_results

hive> DESCRIBE driver_results;
OK
driver_forename         string
driver_surname          string
driver_nationality      string
points                  double
Time taken: 0.121 seconds, Fetched: 4 row(s)

# Describir la estructura de constructor_results

hive> DESCRIBE constructor_results;
OK
constructorref          string
cons_name               string
cons_nationality        string
url                     string
points                  double
Time taken: 0.097 seconds, Fetched: 5 row(s)

#Describir informaciÃ³n detallada de driver_results

hive> DESCRIBE FORMATTED driver_results;
OK
# col_name              data_type               comment

driver_forename         string
driver_surname          string
driver_nationality      string
points                  double

# Detailed Table Information
Database:               f1
Owner:                  hadoop
CreateTime:             Mon Nov 03 00:47:16 ART 2025
LastAccessTime:         UNKNOWN
Retention:              0
Location:               hdfs://172.17.0.2:9000/user/hive/warehouse/f1.db/driver_results
Table Type:             EXTERNAL_TABLE
Table Parameters:
        EXTERNAL                TRUE
        skip.header.line.count  1
        transient_lastDdlTime   1762141636

# Storage Information
SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat:            org.apache.hadoop.mapred.TextInputFormat
OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
Compressed:             No
Num Buckets:            -1
Bucket Columns:         []
Sort Columns:           []
Storage Desc Params:
        field.delim             ,
        serialization.format    ,
Time taken: 0.131 seconds, Fetched: 31 row(s)

# Describir informaciÃ³n detallada de constructor_results

hive> DESCRIBE FORMATTED constructor_results;
OK
# col_name              data_type               comment

constructorref          string
cons_name               string
cons_nationality        string
url                     string
points                  double

# Detailed Table Information
Database:               f1
Owner:                  hadoop
CreateTime:             Mon Nov 03 00:47:41 ART 2025
LastAccessTime:         UNKNOWN
Retention:              0
Location:               hdfs://172.17.0.2:9000/user/hive/warehouse/f1.db/constructor_results
Table Type:             EXTERNAL_TABLE
Table Parameters:
        EXTERNAL                TRUE
        skip.header.line.count  1
        transient_lastDdlTime   1762141661

# Storage Information
SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat:            org.apache.hadoop.mapred.TextInputFormat
OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
Compressed:             No
Num Buckets:            -1
Bucket Columns:         []
Sort Columns:           []
Storage Desc Params:
        field.delim             ,
        serialization.format    ,
Time taken: 0.087 seconds, Fetched: 32 row(s)
hive>

# Ejercicio 3

hadoop@cd7195a9ebf6:/$ cd /home/hadoop/scripts
hadoop@cd7195a9ebf6:~/scripts$ ls
derby.log               ingest.sh   process_airport_trips.py  spark-warehouse    transformation.py
download_and_ingest.sh  landing.sh  pyspark_jupyter.sh        start-services.sh
hadoop@cd7195a9ebf6:~/scripts$ nano f1_download_and_ingest.sh


// start script//

#!/bin/bash

# Script: f1_download_and_ingest.sh
# DescripciÃ³n: Descarga archivos CSV de Formula 1 desde S3 y los sube a HDFS

echo "=== INICIANDO DESCARGA E INGESTA A HDFS ==="
echo "Fecha: $(date)"

# --- CONFIGURACIÃ“N DEL ENTORNO ---
# Si Airflow no carga el entorno de Hadoop, lo forzamos manualmente
export HADOOP_HOME=/home/hadoop/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# Comando HDFS con ruta absoluta por seguridad
HDFS_CMD="$HADOOP_HOME/bin/hdfs"

# Directorio HDFS para los archivos
HDFS_RAW_DIR="/user/hadoop/f1/raw"

# --- 1. Verificar servicios HDFS ---
echo "1. Verificando servicios HDFS..."
jps | grep -E "NameNode|DataNode" > /dev/null
if [ $? -ne 0 ]; then
  echo "âŒ Servicios HDFS no detectados"
  exit 1
fi

# --- 2. Verificar/Crear directorio en HDFS ---
echo "2. Verificando directorio HDFS: $HDFS_RAW_DIR"
$HDFS_CMD dfs -mkdir -p $HDFS_RAW_DIR

# --- 3. URLs y nombres locales ---
URL1="https://data-engineer-edvai-public.s3.amazonaws.com/results.csv"
URL2="https://data-engineer-edvai-public.s3.amazonaws.com/drivers.csv"
URL3="https://data-engineer-edvai-public.s3.amazonaws.com/constructors.csv"
URL4="https://data-engineer-edvai-public.s3.amazonaws.com/races.csv"
FILE1="results.csv"
FILE2="drivers.csv"
FILE3="constructors.csv"
FILE4="races.csv"

# --- 4. Prueba de conectividad ---
echo "3. Probando conectividad con URLs..."
wget --spider $URL1 --timeout=30
if [ $? -ne 0 ]; then
  echo "âŒ No hay conexiÃ³n a internet o el servidor no responde"
  exit 1
fi
echo "âœ… Conectividad OK"

# --- 5. Descarga de archivos ---
echo "4. Descargando archivos..."
wget --tries=3 --timeout=60 -O $FILE1 $URL1
DOWNLOAD1=$?
wget --tries=3 --timeout=60 -O $FILE2 $URL2
DOWNLOAD2=$?
wget --tries=3 --timeout=60 -O $FILE3 $URL3
DOWNLOAD3=$?
wget --tries=3 --timeout=60 -O $FILE4 $URL4
DOWNLOAD4=$?

if [ $DOWNLOAD1 -eq 0 ] && [ $DOWNLOAD2 -eq 0 ] && [ $DOWNLOAD3 -eq 0 ] && [ $DOWNLOAD4 -eq 0 ]; then
  echo "âœ… Archivos descargados correctamente"
  echo "   - $FILE1: $(ls -lh $FILE1 | awk '{print $5}')"
  echo "   - $FILE2: $(ls -lh $FILE2 | awk '{print $5}')"
  echo "   - $FILE3: $(ls -lh $FILE3 | awk '{print $5}')"
  echo "   - $FILE4: $(ls -lh $FILE4 | awk '{print $5}')"
else
  echo "âŒ Error en la descarga: cÃ³digos $DOWNLOAD1 / $DOWNLOAD2 / $DOWNLOAD3 / $DOWNLOAD4"
  exit 1
fi

# --- 6. Subir a HDFS ---
echo "5. Subiendo archivos a HDFS..."
$HDFS_CMD dfs -put -f $FILE1 $HDFS_RAW_DIR/
UPLOAD1=$?
$HDFS_CMD dfs -put -f $FILE2 $HDFS_RAW_DIR/
UPLOAD2=$?
$HDFS_CMD dfs -put -f $FILE3 $HDFS_RAW_DIR/
UPLOAD3=$?
$HDFS_CMD dfs -put -f $FILE4 $HDFS_RAW_DIR/
UPLOAD4=$?

if [ $UPLOAD1 -eq 0 ] && [ $UPLOAD2 -eq 0 ] && [ $UPLOAD3 -eq 0 ] && [ $UPLOAD4 -eq 0 ]; then
  echo "âœ… Archivos subidos correctamente a HDFS"
else
  echo "âŒ Error subiendo archivos a HDFS"
  exit 1
fi

# --- 7. VerificaciÃ³n ---
echo "6. Verificando carga en HDFS..."
$HDFS_CMD dfs -ls -h $HDFS_RAW_DIR/

# --- 8. Limpieza local ---
echo "7. Limpiando archivos locales..."
rm -f $FILE1 $FILE2 $FILE3 $FILE4

echo "=== PROCESO COMPLETADO EXITOSAMENTE ==="
echo "âœ… Archivos disponibles en HDFS: $HDFS_RAW_DIR"
echo "ðŸ“… Fecha finalizaciÃ³n: $(date)"

ðŸ’¾ Guarda en nano:
Ctrl + O â†’ Enter â†’ Ctrl + X

hadoop@cd7195a9ebf6:~/scripts$ chmod +x f1_download_and_ingest.sh
hadoop@cd7195a9ebf6:~/scripts$  ls -la f1_download_and_ingest.sh
-rwxrwxr-x 1 hadoop hadoop 3176 Nov  3 01:25 f1_download_and_ingest.sh
hadoop@cd7195a9ebf6:~/scripts$ ./f1_download_and_ingest.sh
=== INICIANDO DESCARGA E INGESTA A HDFS ===
Fecha: Mon Nov  3 01:27:25 -03 2025
1. Verificando servicios HDFS...
2. Verificando directorio HDFS: /user/hadoop/f1/raw
3. Probando conectividad con URLs...
Spider mode enabled. Check if remote file exists.
--2025-11-03 01:27:28--  https://data-engineer-edvai-public.s3.amazonaws.com/results.csv
Resolving data-engineer-edvai-public.s3.amazonaws.com (data-engineer-edvai-public.s3.amazonaws.com)... 16.15.186.220, 54.231.230.209, 54.231.196.89, ...
Connecting to data-engineer-edvai-public.s3.amazonaws.com (data-engineer-edvai-public.s3.amazonaws.com)|16.15.186.220|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1721961 (1.6M) [text/csv]
Remote file exists.

âœ… Conectividad OK
4. Descargando archivos...
--2025-11-03 01:27:29--  https://data-engineer-edvai-public.s3.amazonaws.com/results.csv
Resolving data-engineer-edvai-public.s3.amazonaws.com (data-engineer-edvai-public.s3.amazonaws.com)... 16.15.186.220, 54.231.230.209, 54.231.196.89, ...
Connecting to data-engineer-edvai-public.s3.amazonaws.com (data-engineer-edvai-public.s3.amazonaws.com)|16.15.186.220|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1721961 (1.6M) [text/csv]
Saving to: 'results.csv'

results.csv                            100%[============================================================================>]   1.64M  1.76MB/s    in 0.9s

2025-11-03 01:27:30 (1.76 MB/s) - 'results.csv' saved [1721961/1721961]

--2025-11-03 01:27:30--  https://data-engineer-edvai-public.s3.amazonaws.com/drivers.csv
Resolving data-engineer-edvai-public.s3.amazonaws.com (data-engineer-edvai-public.s3.amazonaws.com)... 16.15.186.220, 54.231.230.209, 54.231.196.89, ...
Connecting to data-engineer-edvai-public.s3.amazonaws.com (data-engineer-edvai-public.s3.amazonaws.com)|16.15.186.220|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 94367 (92K) [text/csv]
Saving to: 'drivers.csv'

drivers.csv                            100%[============================================================================>]  92.16K   277KB/s    in 0.3s

2025-11-03 01:27:31 (277 KB/s) - 'drivers.csv' saved [94367/94367]

--2025-11-03 01:27:31--  https://data-engineer-edvai-public.s3.amazonaws.com/constructors.csv
Resolving data-engineer-edvai-public.s3.amazonaws.com (data-engineer-edvai-public.s3.amazonaws.com)... 16.15.186.220, 54.231.230.209, 54.231.196.89, ...
Connecting to data-engineer-edvai-public.s3.amazonaws.com (data-engineer-edvai-public.s3.amazonaws.com)|16.15.186.220|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 17478 (17K) [text/csv]
Saving to: 'constructors.csv'

constructors.csv                       100%[============================================================================>]  17.07K  --.-KB/s    in 0.002s

2025-11-03 01:27:32 (7.31 MB/s) - 'constructors.csv' saved [17478/17478]

--2025-11-03 01:27:32--  https://data-engineer-edvai-public.s3.amazonaws.com/races.csv
Resolving data-engineer-edvai-public.s3.amazonaws.com (data-engineer-edvai-public.s3.amazonaws.com)... 16.15.186.220, 54.231.230.209, 54.231.196.89, ...
Connecting to data-engineer-edvai-public.s3.amazonaws.com (data-engineer-edvai-public.s3.amazonaws.com)|16.15.186.220|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 164344 (160K) [text/csv]
Saving to: 'races.csv'

races.csv                              100%[============================================================================>] 160.49K   358KB/s    in 0.4s

2025-11-03 01:27:33 (358 KB/s) - 'races.csv' saved [164344/164344]

âœ… Archivos descargados correctamente
   - results.csv: 1.7M
   - drivers.csv: 93K
   - constructors.csv: 18K
   - races.csv: 161K
5. Subiendo archivos a HDFS...
âœ… Archivos subidos correctamente a HDFS
6. Verificando carga en HDFS...
Found 4 items
-rw-r--r--   1 hadoop supergroup     17.1 K 2025-11-03 01:27 /user/hadoop/f1/raw/constructors.csv
-rw-r--r--   1 hadoop supergroup     92.2 K 2025-11-03 01:27 /user/hadoop/f1/raw/drivers.csv
-rw-r--r--   1 hadoop supergroup    160.5 K 2025-11-03 01:27 /user/hadoop/f1/raw/races.csv
-rw-r--r--   1 hadoop supergroup      1.6 M 2025-11-03 01:27 /user/hadoop/f1/raw/results.csv
7. Limpiando archivos locales...
=== PROCESO COMPLETADO EXITOSAMENTE ===
âœ… Archivos disponibles en HDFS: /user/hadoop/f1/raw
ðŸ“… Fecha finalizaciÃ³n: Mon Nov  3 01:27:45 -03 2025

Ejercicio 4

hadoop@cd7195a9ebf6:~/scripts$ ls
derby.log               f1_download_and_ingest.sh  landing.sh                pyspark_jupyter.sh  start-services.sh
download_and_ingest.sh  ingest.sh                  process_airport_trips.py  spark-warehouse     transformation.py
hadoop@cd7195a9ebf6:~/scripts$ nano process_f1_data.py

// start script//

#!/usr/bin/env python3
"""
Script: process_f1_data.py
DescripciÃ³n: Procesa datos de Formula 1 con Spark y genera archivos para tablas Hive.
- Punto 4a: Corredores con mayor cantidad de puntos en la historia
- Punto 4b: Constructores con mÃ¡s puntos en Spanish Grand Prix 1991
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as spark_sum, desc

def main():
    print("=== INICIANDO PROCESAMIENTO DE DATOS F1 CON SPARK ===")

    # Inicializar Spark Session con soporte para Hive
    spark = (
        SparkSession.builder
        .appName("F1DataProcessing")
        .config("spark.sql.adaptive.enabled", "true")
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
        .enableHiveSupport()
        .getOrCreate()
    )

    spark.sparkContext.setLogLevel("WARN")

    try:
        # Ruta en HDFS para los archivos CSV
        hdfs_raw_path = "hdfs://172.17.0.2:9000/user/hadoop/f1/raw/"
        
        # Rutas de destino para las tablas externas
        driver_results_path = "hdfs://172.17.0.2:9000/user/hive/warehouse/f1.db/driver_results/"
        constructor_results_path = "hdfs://172.17.0.2:9000/user/hive/warehouse/f1.db/constructor_results/"

        print("1. ðŸ“‚ Leyendo archivos CSV desde HDFS...")
        
        # Leer archivos CSV con header
        df_results = spark.read.option("header", "true").option("inferSchema", "true").csv(hdfs_raw_path + "results.csv")
        df_drivers = spark.read.option("header", "true").option("inferSchema", "true").csv(hdfs_raw_path + "drivers.csv")
        df_constructors = spark.read.option("header", "true").option("inferSchema", "true").csv(hdfs_raw_path + "constructors.csv")
        df_races = spark.read.option("header", "true").option("inferSchema", "true").csv(hdfs_raw_path + "races.csv")

        print(f"   âœ… results.csv: {df_results.count():,} registros")
        print(f"   âœ… drivers.csv: {df_drivers.count():,} registros")
        print(f"   âœ… constructors.csv: {df_constructors.count():,} registros")
        print(f"   âœ… races.csv: {df_races.count():,} registros")

        # Mostrar esquemas
        print("\n2. ðŸ“Š Esquemas de datos:")
        print("   Esquema de results:")
        df_results.printSchema()
        print("   Esquema de drivers:")
        df_drivers.printSchema()
        print("   Esquema de constructors:")
        df_constructors.printSchema()
        print("   Esquema de races:")
        df_races.printSchema()

        # ============================================
        # PUNTO 4a: Corredores con mayor cantidad de puntos en la historia
        # ============================================
        print("\n3. ðŸŽï¸ Procesando punto 4a: Corredores con mayor cantidad de puntos...")
        
        # Hacer JOIN entre results y drivers para obtener informaciÃ³n de corredores
        df_driver_points = df_results.join(
            df_drivers,
            df_results.driverId == df_drivers.driverId,
            "inner"
        ).groupBy(
            df_drivers.forename.alias("driver_forename"),
            df_drivers.surname.alias("driver_surname"),
            df_drivers.nationality.alias("driver_nationality")
        ).agg(
            spark_sum(df_results.points).alias("points")
        ).orderBy(desc("points"))

        print(f"   âœ… Total de corredores Ãºnicos: {df_driver_points.count():,}")
        print("   ðŸ“‹ Top 10 corredores por puntos:")
        df_driver_points.show(10, truncate=False)

        # Seleccionar columnas para la tabla driver_results
        df_driver_results = df_driver_points.select(
            col("driver_forename"),
            col("driver_surname"),
            col("driver_nationality"),
            col("points")
        )

        print("\n4. ðŸ’¾ Guardando driver_results en HDFS...")
        df_driver_results.coalesce(1).write.mode("overwrite").option("header", "true").csv(driver_results_path)
        print(f"   âœ… Datos guardados en: {driver_results_path}")

        # ============================================
        # PUNTO 4b: Constructores con mÃ¡s puntos en Spanish Grand Prix 1991
        # ============================================
        print("\n5. ðŸ Procesando punto 4b: Constructores con mÃ¡s puntos en Spanish Grand Prix 1991...")
        
        # Filtrar races por Spanish Grand Prix y aÃ±o 1991
        df_spanish_gp_1991 = df_races.filter(
            (col("name").like("%Spanish Grand Prix%")) & 
            (col("year") == 1991)
        )
        
        print(f"   âœ… Carreras encontradas: {df_spanish_gp_1991.count()}")

        # Hacer JOIN entre results, constructors y races
        df_constructor_points = df_results.join(
            df_constructors,
            df_results.constructorId == df_constructors.constructorId,
            "inner"
        ).join(
            df_spanish_gp_1991,
            df_results.raceId == df_spanish_gp_1991.raceId,
            "inner"
        ).groupBy(
            df_constructors.constructorRef.alias("constructorRef"),
            df_constructors.name.alias("cons_name"),
            df_constructors.nationality.alias("cons_nationality"),
            df_constructors.url.alias("url")
        ).agg(
            spark_sum(df_results.points).alias("points")
        ).orderBy(desc("points"))

        print(f"   âœ… Total de constructores: {df_constructor_points.count():,}")
        print("   ðŸ“‹ Resultados de constructores en Spanish GP 1991:")
        df_constructor_points.show(truncate=False)

        # Seleccionar columnas para la tabla constructor_results
        df_constructor_results = df_constructor_points.select(
            col("constructorRef"),
            col("cons_name"),
            col("cons_nationality"),
            col("url"),
            col("points")
        )

        print("\n6. ðŸ’¾ Guardando constructor_results en HDFS...")
        df_constructor_results.coalesce(1).write.mode("overwrite").option("header", "true").csv(constructor_results_path)
        print(f"   âœ… Datos guardados en: {constructor_results_path}")

        # VerificaciÃ³n final
        print("\n7. âœ… VerificaciÃ³n de datos guardados:")
        print("   ðŸ“Š Resumen de driver_results:")
        df_driver_results.describe().show()
        print("   ðŸ“Š Resumen de constructor_results:")
        df_constructor_results.describe().show()

        print("\nâœ… PROCESAMIENTO COMPLETADO EXITOSAMENTE")
        print(f"ðŸ“Š Corredores procesados: {df_driver_results.count():,}")
        print(f"ðŸ“Š Constructores procesados: {df_constructor_results.count():,}")

    except Exception as e:
        print(f"âŒ ERROR durante el procesamiento: {str(e)}")
        import traceback
        traceback.print_exc()
        raise e

    finally:
        spark.stop()
        print("ðŸ›‘ SesiÃ³n de Spark cerrada")

if __name__ == "__main__":
    main()

ðŸ’¾ Guarda en nano:
Ctrl + O â†’ Enter â†’ Ctrl + X

hadoop@cd7195a9ebf6:~/scripts$ chmod +x process_f1_data.py
hadoop@cd7195a9ebf6:~/scripts$ ls -la process_f1_data.py
-rwxrwxr-x 1 hadoop hadoop 6863 Nov  3 14:52 process_f1_data.py
hadoop@cd7195a9ebf6:~/scripts$ spark-submit process_f1_data.py

1. ðŸ“‚ Leyendo archivos CSV desde HDFS...
   âœ… results.csv: 26,759 registros
   âœ… drivers.csv: 861 registros
   âœ… constructors.csv: 212 registros
   âœ… races.csv: 1,125 registros

2. ðŸ“Š Esquemas de datos:
   Esquema de results:
root
 |-- resultId: integer (nullable = true)
 |-- raceId: integer (nullable = true)
 |-- driverId: integer (nullable = true)
 |-- constructorId: integer (nullable = true)
 |-- number: string (nullable = true)
 |-- grid: integer (nullable = true)
 |-- position: string (nullable = true)
 |-- positionText: string (nullable = true)
 |-- positionOrder: integer (nullable = true)
 |-- points: double (nullable = true)
 |-- laps: integer (nullable = true)
 |-- time: string (nullable = true)
 |-- milliseconds: string (nullable = true)
 |-- fastestLap: string (nullable = true)
 |-- rank: string (nullable = true)
 |-- fastestLapTime: string (nullable = true)
 |-- fastestLapSpeed: string (nullable = true)
 |-- statusId: integer (nullable = true)

   Esquema de drivers:
root
 |-- driverId: integer (nullable = true)
 |-- driverRef: string (nullable = true)
 |-- number: string (nullable = true)
 |-- code: string (nullable = true)
 |-- forename: string (nullable = true)
 |-- surname: string (nullable = true)
 |-- dob: string (nullable = true)
 |-- nationality: string (nullable = true)
 |-- url: string (nullable = true)

   Esquema de constructors:
root
 |-- constructorId: integer (nullable = true)
 |-- constructorRef: string (nullable = true)
 |-- name: string (nullable = true)
 |-- nationality: string (nullable = true)
 |-- url: string (nullable = true)

   Esquema de races:
root
 |-- raceId: integer (nullable = true)
 |-- year: integer (nullable = true)
 |-- round: integer (nullable = true)
 |-- circuitId: integer (nullable = true)
 |-- name: string (nullable = true)
 |-- date: string (nullable = true)
 |-- time: string (nullable = true)
 |-- url: string (nullable = true)
 |-- fp1_date: string (nullable = true)
 |-- fp1_time: string (nullable = true)
 |-- fp2_date: string (nullable = true)
 |-- fp2_time: string (nullable = true)
 |-- fp3_date: string (nullable = true)
 |-- fp3_time: string (nullable = true)
 |-- quali_date: string (nullable = true)
 |-- quali_time: string (nullable = true)
 |-- sprint_date: string (nullable = true)
 |-- sprint_time: string (nullable = true)


3. ðŸŽï¸ Procesando punto 4a: Corredores con mayor cantidad de puntos...
   âœ… Total de corredores Ãºnicos: 861
   ðŸ“‹ Top 10 corredores por puntos:
+---------------+--------------+------------------+------+
|driver_forename|driver_surname|driver_nationality|points|
+---------------+--------------+------------------+------+
|Lewis          |Hamilton      |British           |4820.5|
|Sebastian      |Vettel        |German            |3098.0|
|Max            |Verstappen    |Dutch             |2912.5|
|Fernando       |Alonso        |Spanish           |2329.0|
|Kimi           |RÃ¤ikkÃ¶nen     |Finnish           |1873.0|
|Valtteri       |Bottas        |Finnish           |1788.0|
|Nico           |Rosberg       |German            |1594.5|
|Sergio         |PÃ©rez         |Mexican           |1585.0|
|Michael        |Schumacher    |German            |1566.0|
|Charles        |Leclerc       |Monegasque        |1363.0|
+---------------+--------------+------------------+------+
only showing top 10 rows


4. ðŸ’¾ Guardando driver_results en HDFS...
   âœ… Datos guardados en: hdfs://172.17.0.2:9000/user/hive/warehouse/f1.db/driver_results/

5. ðŸ Procesando punto 4b: Constructores con mÃ¡s puntos en Spanish Grand Prix 1991...
   âœ… Carreras encontradas: 1
   âœ… Total de constructores: 17
   ðŸ“‹ Resultados de constructores en Spanish GP 1991:
+--------------+------------+----------------+-----------------------------------------------------------------+------+
|constructorRef|cons_name   |cons_nationality|url                                                              |points|
+--------------+------------+----------------+-----------------------------------------------------------------+------+
|williams      |Williams    |British         |http://en.wikipedia.org/wiki/Williams_Grand_Prix_Engineering     |14.0  |
|ferrari       |Ferrari     |Italian         |http://en.wikipedia.org/wiki/Scuderia_Ferrari                    |9.0   |
|mclaren       |McLaren     |British         |http://en.wikipedia.org/wiki/McLaren                             |2.0   |
|benetton      |Benetton    |Italian         |http://en.wikipedia.org/wiki/Benetton_Formula                    |1.0   |
|fondmetal     |Fondmetal   |Italian         |http://en.wikipedia.org/wiki/Fondmetal                           |0.0   |
|tyrrell       |Tyrrell     |British         |http://en.wikipedia.org/wiki/Tyrrell_Racing                      |0.0   |
|leyton        |Leyton House|British         |http://en.wikipedia.org/wiki/Leyton_House                        |0.0   |
|brabham       |Brabham     |British         |http://en.wikipedia.org/wiki/Brabham                             |0.0   |
|lola          |Lola        |British         |http://en.wikipedia.org/wiki/MasterCard_Lola                     |0.0   |
|ags           |AGS         |French          |http://en.wikipedia.org/wiki/Automobiles_Gonfaronnaises_Sportives|0.0   |
|minardi       |Minardi     |Italian         |http://en.wikipedia.org/wiki/Minardi                             |0.0   |
|footwork      |Footwork    |British         |http://en.wikipedia.org/wiki/Footwork_Arrows                     |0.0   |
|ligier        |Ligier      |French          |http://en.wikipedia.org/wiki/Ligier                              |0.0   |
|dallara       |Dallara     |Italian         |http://en.wikipedia.org/wiki/Dallara                             |0.0   |
|team_lotus    |Team Lotus  |British         |http://en.wikipedia.org/wiki/Team_Lotus                          |0.0   |
|lambo         |Lambo       |Italian         |http://en.wikipedia.org/wiki/Modena_(racing_team)                |0.0   |
|jordan        |Jordan      |Irish           |http://en.wikipedia.org/wiki/Jordan_Grand_Prix                   |0.0   |
+--------------+------------+----------------+-----------------------------------------------------------------+------+


6. ðŸ’¾ Guardando constructor_results en HDFS...
   âœ… Datos guardados en: hdfs://172.17.0.2:9000/user/hive/warehouse/f1.db/constructor_results/

7. âœ… VerificaciÃ³n de datos guardados:
   ðŸ“Š Resumen de driver_results:
+-------+---------------+--------------+------------------+-----------------+
|summary|driver_forename|driver_surname|driver_nationality|           points|
+-------+---------------+--------------+------------------+-----------------+
|  count|            861|           861|               861|              861|
|   mean|           null|          null|              null|61.77357723577236|
| stddev|           null|          null|              null|294.2850380118803|
|    min|          Adolf|         Abate|          American|              0.0|
|    max|          Ã“scar|     Ã‰tancelin|        Venezuelan|           4820.5|
+-------+---------------+--------------+------------------+-----------------+

   ðŸ“Š Resumen de constructor_results:
+-------+--------------+---------+----------------+--------------------+------------------+
|summary|constructorRef|cons_name|cons_nationality|                 url|            points|
+-------+--------------+---------+----------------+--------------------+------------------+
|  count|            17|       17|              17|                  17|                17|
|   mean|          null|     null|            null|                null|1.5294117647058822|
| stddev|          null|     null|            null|                null|3.8909774970247444|
|    min|           ags|      AGS|         British|http://en.wikiped...|               0.0|
|    max|      williams| Williams|         Italian|http://en.wikiped...|              14.0|
+-------+--------------+---------+----------------+--------------------+------------------+


âœ… PROCESAMIENTO COMPLETADO EXITOSAMENTE
ðŸ“Š Corredores procesados: 861
ðŸ“Š Constructores procesados: 17
ðŸ›‘ SesiÃ³n de Spark cerrada
hadoop@cd7195a9ebf6:~/scripts$

Ejercicio 5


// start script//

hadoop@cd7195a9ebf6:/$ cd ~/airflow/dags
hadoop@cd7195a9ebf6:~/airflow/dags$ ls
__pycache__  example-DAG.py  ingest-transform.py
hadoop@cd7195a9ebf6:~/airflow/dags$ hadoop@cd7195a9ebf6:~/airflow/dags$ nano f1_processing.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from datetime import timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.dummy import DummyOperator
from airflow.utils.dates import days_ago

default_args = {
    'owner': 'hadoop',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    dag_id='f1_processing',
    default_args=default_args,
    description='Orquesta la descarga, ingestiÃ³n y procesamiento de datos de Formula 1',
    schedule_interval=None,
    start_date=days_ago(1),
    catchup=False,
    tags=['spark', 'hive', 'etl', 'f1'],
) as dag:

    inicio = DummyOperator(task_id='inicio')

    ingest = BashOperator(
        task_id='ingesta_datos_f1',
        bash_command='bash -c \'bash /home/hadoop/scripts/f1_download_and_ingest.sh\'',
    )

    process = BashOperator(
        task_id='procesa_spark_f1',
        bash_command='bash -c \'spark-submit /home/hadoop/scripts/process_f1_data.py\'',
    )

    verify_drivers = BashOperator(
        task_id='verifica_driver_results',
        bash_command='bash -c \'beeline -u jdbc:hive2://localhost:10000 -e "USE f1; SELECT COUNT(*) AS total_drivers FROM driver_results;"\'',
    )

    verify_constructors = BashOperator(
        task_id='verifica_constructor_results',
        bash_command='bash -c \'beeline -u jdbc:hive2://localhost:10000 -e "USE f1; SELECT COUNT(*) AS total_constructors FROM constructor_results;"\'',
    )

    fin = DummyOperator(task_id='fin_proceso')

    inicio >> ingest >> process >> verify_drivers >> verify_constructors >> fin

ðŸ’¾ Guarda en nano:
Ctrl + O â†’ Enter â†’ Ctrl + X

hadoop@cd7195a9ebf6:~/airflow/dags$ chmod -x f1_processing.py
hadoop@cd7195a9ebf6:~/airflow/dags$ ls -la f1_processing.py
-rw-rw-r-- 1 hadoop hadoop 1690 Nov  3 15:22 f1_processing.py
hadoop@cd7195a9ebf6:~/airflow/dags$ ls
__pycache__  airport_trips_processing.py  example-DAG.py  f1_processing.py  ingest-transform.py
hadoop@cd7195a9ebf6:~/airflow/dags$





