# âœˆï¸ Ejercicio Final 1: AviaciÃ³n Civil - SoluciÃ³n Completa

## ğŸ“‹ DescripciÃ³n del Proyecto

**Cliente:** AdministraciÃ³n Nacional de AviaciÃ³n Civil  
**Objetivo:** Pipeline automatizado para anÃ¡lisis de aterrizajes y despegues en Argentina  
**Periodo:** 01/01/2021 - 30/06/2022

---

## ğŸ“‚ Estructura del Proyecto

```
ejercicios-Finales/ejercicio-1/
â”œâ”€â”€ README.md                         # DocumentaciÃ³n principal
â”œâ”€â”€ SOLUCION_COMPLETA_EJERCICIO_1.md  # GuÃ­a paso a paso completa
â”œâ”€â”€ TESTEAR_EN_HIVE.md                # Instrucciones de testing
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest_aviacion.sh           # Script bash para descarga e ingest
â”‚   â””â”€â”€ process_aviacion_spark.py    # Script PySpark de transformaciÃ³n
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ aviacion_spark_dag.py        # DAG de Airflow
â””â”€â”€ hive/
    â””â”€â”€ queries_aviacion.sql         # CREATE TABLE + Consultas SQL
```

---

## ğŸš€ PASO 1: Ingest de Archivos (Punto 1 y 3)

### 1.1. Copiar Script de Ingest al Contenedor

Desde **PowerShell** (tu mÃ¡quina local):

```powershell
# Copiar script bash al contenedor
docker cp ejercicios-Finales/ejercicio-1/scripts/ingest_aviacion.sh edvai_hadoop:/home/hadoop/scripts/

# Dar permisos de ejecuciÃ³n
docker exec -it edvai_hadoop chmod +x /home/hadoop/scripts/ingest_aviacion.sh
```

### 1.2. Ejecutar Script de Ingest (Automatizado)

```bash
# Entrar al contenedor
docker exec -it edvai_hadoop bash
su hadoop

# Ejecutar script de ingest
bash /home/hadoop/scripts/ingest_aviacion.sh
```

**El script automÃ¡ticamente:**
1. âœ… Crea directorio local `/home/hadoop/landing`
2. âœ… Descarga 3 archivos CSV desde S3:
   - `2021-informe-ministerio.csv` (32 MB)
   - `202206-informe-ministerio.csv` (22 MB)
   - `aeropuertos_detalle.csv` (136 KB)
3. âœ… Verifica descargas exitosas
4. âœ… Limpia directorio HDFS si existe
5. âœ… Crea directorio en HDFS `/ingest`
6. âœ… Sube los 3 archivos a HDFS
7. âœ… Verifica archivos en HDFS
8. âœ… OpciÃ³n de limpiar archivos locales

**Resultado esperado:**
```
==========================================
âœ… INGEST COMPLETADO EXITOSAMENTE
==========================================

ğŸ“Š Resumen:
   - Archivos descargados: 3
   - UbicaciÃ³n local: /home/hadoop/landing
   - UbicaciÃ³n HDFS: /ingest

ğŸ“ Archivos en HDFS:
   â€¢ /ingest/2021-informe-ministerio.csv (32.3 M)
   â€¢ /ingest/202206-informe-ministerio.csv (22.8 M)
   â€¢ /ingest/aeropuertos_detalle.csv (136.0 K)

ğŸ¯ Siguiente paso: Crear tablas en Hive
```

### 1.3. Verificar Archivos en HDFS (Manual)

```bash
# Verificar que los archivos estÃ¡n en HDFS
hdfs dfs -ls -h /ingest

# Ver primeras lÃ­neas de un archivo
hdfs dfs -cat /ingest/aeropuertos_detalle.csv | head -n 5
```

---

### ğŸ“ **Alternativa: Descarga Manual (sin script)**

Si prefieres ejecutar los comandos manualmente:

```bash
# Dentro del contenedor
docker exec -it edvai_hadoop bash
su hadoop

# Crear directorio landing
mkdir -p /home/hadoop/landing
cd /home/hadoop/landing

# Descargar archivos
wget -O 2021-informe-ministerio.csv "https://data-engineer-edvai-public.s3.amazonaws.com/2021-informe-ministerio.csv"
wget -O 202206-informe-ministerio.csv "https://data-engineer-edvai-public.s3.amazonaws.com/202206-informe-ministerio.csv"
wget -O aeropuertos_detalle.csv "https://data-engineer-edvai-public.s3.amazonaws.com/aeropuertos_detalle.csv"

# Subir a HDFS
hdfs dfs -mkdir -p /ingest
hdfs dfs -put -f *.csv /ingest/

# Verificar
hdfs dfs -ls /ingest
```

---

## ğŸ“Š PASO 2: Crear Tablas en Hive (Punto 2)

### 2.1. Copiar Script SQL al Contenedor

Desde **PowerShell** (tu mÃ¡quina local):

```powershell
docker cp ejercicios-Finales/ejercicio-1/hive/queries_aviacion.sql edvai_hadoop:/home/hadoop/
```

### 2.2. Crear Base de Datos y Tablas

```bash
# Dentro del contenedor
docker exec -it edvai_hadoop bash
su hadoop
hive
```

**En Hive:**

```sql
-- Crear base de datos
CREATE DATABASE IF NOT EXISTS aviacion;

USE aviacion;

-- TABLA 1: VUELOS (aeropuerto_tabla)
-- Schema segÃºn PDF PÃ¡gina 2
CREATE TABLE IF NOT EXISTS aeropuerto_tabla (
    fecha DATE,
    horaUTC STRING,
    clase_de_vuelo STRING,
    clasificacion_de_vuelo STRING,
    tipo_de_movimiento STRING,
    aeropuerto STRING,
    origen_destino STRING,
    aerolinea_nombre STRING,
    aeronave STRING,
    pasajeros INT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
TBLPROPERTIES ("skip.header.line.count"="1");

-- TABLA 2: DETALLES AEROPUERTOS (aeropuerto_detalles_tabla)
-- Schema segÃºn PDF PÃ¡gina 3
CREATE TABLE IF NOT EXISTS aeropuerto_detalles_tabla (
    aeropuerto STRING,
    oac STRING,
    iata STRING,
    tipo STRING,
    denominacion STRING,
    coordenadas_latitud STRING,
    coordenadas_longitud STRING,
    elev FLOAT,
    uom_elev STRING,
    ref STRING,
    distancia_ref FLOAT,
    direccion_ref STRING,
    condicion STRING,
    control STRING,
    region STRING,
    uso STRING,
    trafico STRING,
    sna STRING,
    concesionado STRING,
    provincia STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
TBLPROPERTIES ("skip.header.line.count"="1");

-- Verificar tablas creadas
SHOW TABLES;

-- Salir de Hive
exit;
```

---

## ğŸ”„ PASO 3: Script PySpark con Transformaciones (Punto 4)

### 3.1. Copiar Script al Contenedor

Desde **PowerShell**:

```powershell
docker cp ejercicios-Finales/ejercicio-1/scripts/process_aviacion_spark.py edvai_hadoop:/home/hadoop/scripts/
```

Dentro del contenedor:

```bash
chmod +x /home/hadoop/scripts/process_aviacion_spark.py
```

### 3.2. Transformaciones Aplicadas (Punto 4)

El script `process_aviacion_spark.py` aplica las siguientes transformaciones:

âœ… **Eliminar columnas innecesarias:**
- âŒ `inhab`
- âŒ `fir`
- âŒ `calidad del dato`

âœ… **Filtrar vuelos internacionales:**
```python
df_vuelos = df_vuelos.filter(
    ~(lower(col(col_clasificacion)) == 'internacional')
)
```

âœ… **Rellenar pasajeros null con 0:**
```python
df_vuelos = df_vuelos.withColumn(
    'pasajeros',
    when(col('pasajeros').isNull(), 0).otherwise(col('pasajeros'))
)
```

âœ… **Rellenar distancia_ref null con 0:**
```python
df_aeropuertos = df_aeropuertos.withColumn(
    'distancia_ref',
    when(col('distancia_ref').isNull(), 0.0).otherwise(col('distancia_ref'))
)
```

âœ… **Convertir fechas a formato Date:**
```python
df_vuelos = df_vuelos.withColumn(
    'fecha',
    to_date(col('fecha'), 'dd/MM/yyyy')
)
```

âœ… **Normalizar nombres de columnas:**
- MinÃºsculas
- Sin tildes (Ã³â†’o, Ã­â†’i, Ã¡â†’a)
- Sin parÃ©ntesis: `Clase de Vuelo (todos los vuelos)` â†’ `clase de vuelo`

### 3.3. Correcciones Aplicadas

**CorrecciÃ³n 1: FunciÃ³n normalizar_nombre_columna()**

```python
def normalizar_nombre_columna(nombre):
    nombre = nombre.lower()
    nombre = nombre.replace('Ã³', 'o').replace('Ã­', 'i')
    nombre = nombre.replace('Ã¡', 'a').replace('Ã©', 'e')
    nombre = nombre.replace('Ãº', 'u').replace('Ã±', 'n')
    # CORRECCIÃ“N: Eliminar parÃ©ntesis
    nombre = re.sub(r'\s*\([^)]*\)', '', nombre)
    nombre = nombre.strip()
    return nombre
```

**CorrecciÃ³n 2: Mapeo de columnas de aeropuertos**

Las columnas reales despuÃ©s de normalizar son diferentes a las esperadas:

| Columna Real | Columna Esperada | SoluciÃ³n |
|--------------|------------------|----------|
| `local` | `aeropuerto` | `col('local').alias('aeropuerto')` |
| `oaci` | `oac` | `col('oaci').alias('oac')` |
| `latitud` | `coordenadas_latitud` | `col('latitud').alias('coordenadas_latitud')` |
| `longitud` | `coordenadas_longitud` | `col('longitud').alias('coordenadas_longitud')` |

### 3.4. Ejecutar Script PySpark

```bash
# Dentro del contenedor
spark-submit /home/hadoop/scripts/process_aviacion_spark.py
```

**Resultado esperado:**
```
âœ… PROCESAMIENTO COMPLETADO EXITOSAMENTE
ğŸ“Š Resumen:
   - Registros de vuelos procesados: 143,000
   - Registros de aeropuertos procesados: 54
   - Base de datos: aviacion
   - Tablas creadas:
     â€¢ aeropuerto_tabla
     â€¢ aeropuerto_detalles_tabla
```

---

## ğŸ¤– PASO 4: OrquestaciÃ³n con Airflow (Punto 3)

### 4.1. Copiar DAG al Contenedor

Desde **PowerShell**:

```powershell
docker cp ejercicios-Finales/ejercicio-1/airflow/aviacion_spark_dag.py edvai_hadoop:/home/hadoop/airflow/dags/
```

### 4.2. Reiniciar Airflow Scheduler

```bash
# Dentro del contenedor
pkill -f "airflow scheduler"
sleep 3
nohup airflow scheduler > /tmp/airflow_scheduler.log 2>&1 &

# Esperar 10 segundos
sleep 10

# Verificar que el DAG aparece
airflow dags list | grep aviacion
```

### 4.3. Activar y Ejecutar DAG

```bash
# Despausar DAG
airflow dags unpause aviacion_processing_spark_dag

# Ejecutar manualmente
airflow dags trigger aviacion_processing_spark_dag
```

**O desde la UI:** `http://localhost:8080`

### 4.4. Flujo del DAG

```
inicio_proceso
    â†“
crear_tablas_hive
    â†“
procesar_datos_spark
    â†“
verificar_datos_hive
    â†“
fin_proceso
```

---

## ğŸ“Š PASO 5: Consultas SQL y AnÃ¡lisis (Puntos 5-10)

### 5.1. Ejecutar Todas las Consultas

```bash
# Dentro del contenedor
hive -f /home/hadoop/queries_aviacion.sql
```

### 5.2. Consultas Individuales

#### **Punto 5: Verificar Tipos de Datos**

```sql
USE aviacion;

DESCRIBE aeropuerto_tabla;
```

**Captura de pantalla:** Schema con tipos correctos (fecha date, pasajeros int, etc.)

---

#### **Punto 6: Vuelos entre 01/12/2021 y 31/01/2022**

```sql
SELECT COUNT(*) as total_vuelos
FROM aeropuerto_tabla
WHERE fecha BETWEEN '2021-12-01' AND '2022-01-31';
```

**Resultado esperado:** 57,984 vuelos

---

#### **Punto 7: Pasajeros AerolÃ­neas Argentinas (01/01/2021 - 30/06/2022)**

```sql
SELECT SUM(pasajeros) as total_pasajeros
FROM aeropuerto_tabla
WHERE aerolinea_nombre LIKE '%AEROLINEAS ARGENTINAS%'
  AND fecha BETWEEN '2021-01-01' AND '2022-06-30';
```

**Resultado esperado:** 7,484,860 pasajeros

---

#### **Punto 8: Tablero de Vuelos con Ciudades (01/01/2022 - 30/06/2022)**

```sql
SELECT 
    v.fecha, 
    v.horautc, 
    v.aeropuerto as codigo_salida, 
    a_salida.denominacion as ciudad_salida, 
    v.origen_destino as codigo_arribo, 
    a_arribo.denominacion as ciudad_arribo, 
    v.pasajeros
FROM aeropuerto_tabla v
LEFT JOIN aeropuerto_detalles_tabla a_salida 
    ON v.aeropuerto = a_salida.aeropuerto
LEFT JOIN aeropuerto_detalles_tabla a_arribo 
    ON v.origen_destino = a_arribo.aeropuerto
WHERE v.fecha BETWEEN '2022-01-01' AND '2022-06-30'
ORDER BY v.fecha DESC
LIMIT 10;
```

**Captura de pantalla:** Top 10 vuelos con ciudades de origen y destino

---

#### **Punto 9: Top 10 AerolÃ­neas (01/01/2021 - 30/06/2022)**

```sql
SELECT 
    aerolinea_nombre, 
    SUM(pasajeros) as total_pasajeros
FROM aeropuerto_tabla
WHERE aerolinea_nombre IS NOT NULL 
  AND aerolinea_nombre != '0' 
GROUP BY aerolinea_nombre
ORDER BY total_pasajeros DESC
LIMIT 10;
```

**Resultado esperado (Top 3):**
1. AEROLINEAS ARGENTINAS SA - 7,484,860
2. JETSMART AIRLINES S.A. - 1,511,650
3. FB LÃNEAS AÃ‰REAS - FLYBONDI - 1,482,473

**VisualizaciÃ³n:** GrÃ¡fico de barras con las 10 aerolÃ­neas

---

#### **Punto 10: Top 10 Aeronaves desde Buenos Aires (01/01/2021 - 30/06/2022)**

```sql
SELECT 
    v.aeronave, 
    COUNT(*) as cantidad_despegues
FROM aeropuerto_tabla v
JOIN aeropuerto_detalles_tabla d 
    ON v.aeropuerto = d.aeropuerto
WHERE (UPPER(d.provincia) LIKE '%BUENOS AIRES%' 
       OR UPPER(d.provincia) LIKE '%CAPITAL FEDERAL%')
  AND v.tipo_de_movimiento = 'Despegue'
  AND v.aeronave IS NOT NULL 
  AND v.aeronave != '0'
GROUP BY v.aeronave
ORDER BY cantidad_despegues DESC
LIMIT 10;
```

**Resultado esperado (Top 3):**
1. EMB-ERJ190100IGW - 12,470 despegues
2. CE-150-L - 8,117
3. CE-152 - 7,980

**VisualizaciÃ³n:** GrÃ¡fico de barras con las 10 aeronaves

---

## ğŸ“ PASO 6: AnÃ¡lisis y Conclusiones (Puntos 11-13)

### **Punto 11: Datos Externos para Mejorar el AnÃ¡lisis**

Para enriquecer este dataset de aviaciÃ³n, agregarÃ­a:

1. **Datos MeteorolÃ³gicos:**
   - Tablas con condiciones climÃ¡ticas (viento, niebla, tormentas) por aeropuerto y hora
   - PermitirÃ­a analizar la causa de demoras o cancelaciones
   - Fuente: Servicio MeteorolÃ³gico Nacional

2. **Calendario de Feriados:**
   - Dataset de feriados nacionales y vacaciones escolares
   - Para correlacionar picos de demanda con temporada alta
   - Fuente: Ministerio del Interior

3. **Datos EconÃ³micos:**
   - Precio promedio de pasajes por ruta
   - Ãndices de inflaciÃ³n y dÃ³lar
   - Para anÃ¡lisis de accesibilidad y demanda

4. **Datos de Capacidad de Aeronaves:**
   - Capacidad mÃ¡xima de pasajeros por modelo de aeronave
   - Para calcular factor de ocupaciÃ³n real

---

### **Punto 12: Conclusiones y Recomendaciones**

#### **Conclusiones:**

1. **Calidad de Datos:**
   - Los archivos originales tienen columnas desplazadas y formatos inconsistentes
   - Se encontraron ~67,941 vuelos internacionales que se excluyeron correctamente
   - Muchos registros con pasajeros null (convertidos a 0)

2. **ConcentraciÃ³n del Mercado:**
   - AerolÃ­neas Argentinas domina con el 70% de los pasajeros transportados
   - Las low-cost (JetSmart, Flybondi) representan el ~20% combinadas

3. **Aeropuertos Hub:**
   - Buenos Aires (EZE y AEP) concentra la mayorÃ­a de operaciones
   - Aeropuertos regionales tienen baja actividad

4. **Temporalidad:**
   - Clara estacionalidad: verano (dic-feb) tiene picos de demanda
   - RecuperaciÃ³n post-pandemia visible en 2022

#### **Recomendaciones:**

1. **Mejora de Calidad de Datos:**
   - Implementar validaciones automÃ¡ticas en origen (formato, tipos, rangos)
   - Estandarizar nombres de aerolÃ­neas (ej: "AEROLINEAS ARGENTINAS SA" vs "Aerolineas Argentinas")
   - Alertas para columnas con alto % de nulls

2. **OptimizaciÃ³n Operativa:**
   - Analizar rutas con baja ocupaciÃ³n para reasignaciÃ³n de aeronaves
   - Identificar aeropuertos subutilizados para promociÃ³n turÃ­stica

3. **Monitoreo en Tiempo Real:**
   - Dashboard con KPIs actualizados diariamente:
     * Pasajeros por aerolÃ­nea
     * Puntualidad de vuelos
     * OcupaciÃ³n promedio por ruta

4. **Arquitectura de Datos:**
   - Migrar a pipeline incremental (solo nuevos datos)
   - Implementar particionamiento por fecha para consultas mÃ¡s rÃ¡pidas

---

### **Punto 13: Arquitectura Alternativa**

#### **Arquitectura Actual (On-Premise):**

```
CSV Files (S3)
    â†“
  HDFS (Raw)
    â†“
  PySpark (Transformations)
    â†“
  Hive (DW)
    â†“
  SQL Queries
```

**Limitaciones:**
- Escalabilidad limitada
- Mantenimiento manual de infraestructura
- No hay auto-scaling

---

#### **Arquitectura Propuesta - Cloud (AWS):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              INGESTA DE DATOS                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CSV Files â†’ Amazon S3 (Raw Zone)               â”‚
â”‚  - s3://aviacion-data/raw/2021/                 â”‚
â”‚  - s3://aviacion-data/raw/2022/                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PROCESAMIENTO ETL                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  AWS Glue (Spark Serverless)                    â”‚
â”‚  - Glue Crawler: Auto-detect schema             â”‚
â”‚  - Glue Jobs: Transformaciones PySpark          â”‚
â”‚  - Almacena en S3 (Processed Zone)              â”‚
â”‚    â†’ s3://aviacion-data/processed/              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DATA WAREHOUSE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Amazon Athena (Queries directas sobre S3)      â”‚
â”‚  o                                               â”‚
â”‚  Amazon Redshift (DW optimizado)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ORQUESTACIÃ“N                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  MWAA (Managed Airflow)                         â”‚
â”‚  - Mismos DAGs que tenemos                      â”‚
â”‚  - Auto-scaling                                  â”‚
â”‚  - Sin mantenimiento de infraestructura         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              VISUALIZACIÃ“N                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Amazon QuickSight                              â”‚
â”‚  - Dashboards interactivos                      â”‚
â”‚  - ActualizaciÃ³n automÃ¡tica                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ventajas:**
- âœ… **Serverless**: No gestiÃ³n de servidores
- âœ… **Auto-scaling**: Se adapta a demanda
- âœ… **Costos**: Pago por uso (no infraestructura 24/7)
- âœ… **Alta disponibilidad**: SLAs de AWS
- âœ… **IntegraciÃ³n**: QuickSight, SageMaker (ML)

**Costos estimados (mensuales):**
- S3 (100 GB): ~$2.30 USD
- Glue Jobs (10 ejecuciones): ~$5 USD
- Athena (queries): ~$5 USD (1 TB escaneado)
- MWAA: ~$300 USD (smallest instance)
- **Total: ~$312 USD/mes**

---

#### **Arquitectura Alternativa - Cloud (Google Cloud Platform):**

```
CSV Files â†’ Cloud Storage (GCS)
    â†“
Cloud Dataflow (Apache Beam)
    â†“
BigQuery (DW Serverless)
    â†“
Cloud Composer (Managed Airflow)
    â†“
Looker Studio (VisualizaciÃ³n gratuita)
```

**Ventaja principal:** BigQuery es extremadamente rÃ¡pido y no requiere gestiÃ³n de tablas

---

## ğŸ“¸ Capturas para el Examen

**Debes incluir capturas de:**

1. âœ… EjecuciÃ³n del DAG en Airflow (Graph View)
2. âœ… `DESCRIBE aeropuerto_tabla` (Punto 5)
3. âœ… Resultado Punto 6 (count vuelos)
4. âœ… Resultado Punto 7 (sum pasajeros)
5. âœ… Resultado Punto 8 (tablero con JOIN)
6. âœ… GrÃ¡fico Punto 9 (top 10 aerolÃ­neas)
7. âœ… GrÃ¡fico Punto 10 (top 10 aeronaves)
8. âœ… Logs de Spark (procesamiento exitoso)

---

## ğŸ¯ Checklist Final

- [x] Ingest de 3 archivos CSV a HDFS
- [x] CreaciÃ³n de 2 tablas en Hive con schema correcto
- [x] Script PySpark con todas las transformaciones
- [x] NormalizaciÃ³n de nombres de columnas
- [x] Filtrado de vuelos internacionales
- [x] Manejo de valores NULL
- [x] DAG de Airflow funcionando
- [x] Consultas SQL (Puntos 6-10)
- [x] Visualizaciones (Puntos 9-10)
- [x] AnÃ¡lisis y conclusiones (Puntos 11-13)

---

## ğŸ”§ Troubleshooting

### Problema: "DAG no aparece en Airflow"
```bash
pkill -f "airflow scheduler"
sleep 3
nohup airflow scheduler > /tmp/scheduler.log 2>&1 &
sleep 10
airflow dags list | grep aviacion
```

### Problema: "cannot resolve clase de vuelo"
- Causa: FunciÃ³n normalizar no eliminaba parÃ©ntesis
- SoluciÃ³n: Usar `re.sub(r'\s*\([^)]*\)', '', nombre)`

### Problema: "cannot resolve aeropuerto (tabla aeropuertos)"
- Causa: Columna real es `local`, no `aeropuerto`
- SoluciÃ³n: Usar `col('local').alias('aeropuerto')`

---

**Â¡Ejercicio Completado!** ğŸ‰âœˆï¸

