# Airflow - Ejercicio 9: Formula 1

Esta carpeta contiene la configuraci√≥n y documentaci√≥n del DAG de Apache Airflow para orquestar el pipeline de procesamiento de datos de Formula 1.

## üìÅ Archivos Incluidos

### 1Ô∏è‚É£ **f1_processing.py**
DAG principal de Airflow que orquesta todo el pipeline de procesamiento de datos F1.

## üéØ Configuraci√≥n del DAG

### Informaci√≥n B√°sica
- **DAG ID**: `f1_processing`
- **Descripci√≥n**: "Orquesta la descarga, ingesti√≥n y procesamiento de datos de Formula 1"
- **Propietario**: `hadoop`
- **Programaci√≥n**: Manual (sin schedule)
- **Tags**: `['spark', 'hive', 'etl', 'f1']`

### Argumentos por Defecto
```python
default_args = {
    'owner': 'hadoop',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}
```

## üîÑ Tareas del DAG

### 1Ô∏è‚É£ **inicio** (DummyOperator)
- **Tipo**: DummyOperator
- **Prop√≥sito**: Marca el inicio del pipeline
- **Dependencias**: Ninguna

### 2Ô∏è‚É£ **ingesta_datos_f1** (BashOperator)
- **Tipo**: BashOperator
- **Comando**: `bash -c 'bash /home/hadoop/scripts/f1_download_and_ingest.sh'`
- **Prop√≥sito**: Descarga archivos CSV de Formula 1 e ingesta en HDFS
- **Dependencias**: `inicio`

### 3Ô∏è‚É£ **procesa_spark_f1** (BashOperator)
- **Tipo**: BashOperator
- **Comando**: `bash -c 'spark-submit /home/hadoop/scripts/process_f1_data.py'`
- **Prop√≥sito**: Procesa datos con Spark y genera resultados para tablas Hive
- **Dependencias**: `ingesta_datos_f1`

### 4Ô∏è‚É£ **verifica_driver_results** (BashOperator)
- **Tipo**: BashOperator
- **Comando**: `beeline -u jdbc:hive2://localhost:10000 -e "USE f1; SELECT COUNT(*) AS total_drivers FROM driver_results;"`
- **Prop√≥sito**: Verifica que los datos de corredores se procesaron correctamente
- **Dependencias**: `procesa_spark_f1`

### 5Ô∏è‚É£ **verifica_constructor_results** (BashOperator)
- **Tipo**: BashOperator
- **Comando**: `beeline -u jdbc:hive2://localhost:10000 -e "USE f1; SELECT COUNT(*) AS total_constructors FROM constructor_results;"`
- **Prop√≥sito**: Verifica que los datos de constructores se procesaron correctamente
- **Dependencias**: `verifica_driver_results`

### 6Ô∏è‚É£ **fin_proceso** (DummyOperator)
- **Tipo**: DummyOperator
- **Prop√≥sito**: Marca el final del pipeline
- **Dependencias**: `verifica_constructor_results`

## üîó Flujo de Dependencias

```
inicio ‚Üí ingesta_datos_f1 ‚Üí procesa_spark_f1 ‚Üí verifica_driver_results ‚Üí verifica_constructor_results ‚Üí fin_proceso
```

## üöÄ Instalaci√≥n y Configuraci√≥n

### 1. Copiar DAG a la carpeta de Airflow
```bash
cp f1_processing.py /home/hadoop/airflow/dags/
```

### 2. Verificar permisos
```bash
chmod +x /home/hadoop/airflow/dags/f1_processing.py
```

### 3. Reiniciar Airflow (si es necesario)
```bash
# Reiniciar scheduler
airflow scheduler --daemon

# Reiniciar webserver
airflow webserver --daemon
```

## üìä Monitoreo del DAG

### Vista de Grafo
- Muestra el flujo secuencial de tareas
- Indica el estado de cada tarea (SUCCESS, FAILED, RUNNING)
- Permite ver logs individuales de cada tarea

### M√©tricas de Ejecuci√≥n
- **Tiempo total estimado**: ~5-10 minutos
- **Tareas totales**: 6
- **Archivos procesados**: 4 CSV (results, drivers, constructors, races)

## üîç Logs y Debugging

### Acceso a Logs
1. Ir a la interfaz web de Airflow
2. Seleccionar el DAG `f1_processing`
3. Hacer clic en la tarea espec√≠fica
4. Ver logs detallados de ejecuci√≥n

### Logs Importantes
- **ingesta_datos_f1**: Logs de descarga y subida a HDFS de 4 archivos CSV
- **procesa_spark_f1**: Logs de procesamiento con Spark (JOINs, agrupaciones, filtros)
- **verifica_driver_results**: Resultado de conteo de corredores
- **verifica_constructor_results**: Resultado de conteo de constructores

## ‚ö†Ô∏è Troubleshooting

### Error: "Script not found"
```bash
# Verificar que los scripts existen
ls -la /home/hadoop/scripts/f1_download_and_ingest.sh
ls -la /home/hadoop/scripts/process_f1_data.py

# Verificar permisos
chmod +x /home/hadoop/scripts/f1_download_and_ingest.sh
chmod +x /home/hadoop/scripts/process_f1_data.py
```

### Error: "Hive connection failed"
```bash
# Verificar que Hive est√° ejecut√°ndose
jps | grep HiveServer2

# Verificar conectividad
beeline -u jdbc:hive2://localhost:10000

# Verificar que la base de datos f1 existe
beeline -u jdbc:hive2://localhost:10000 -e "SHOW DATABASES;"
```

### Error: "Spark job failed"
```bash
# Verificar configuraci√≥n de Spark
spark-submit --version

# Verificar logs de Spark
tail -f /home/hadoop/spark/logs/spark-*.log

# Verificar que los archivos CSV est√°n en HDFS
hdfs dfs -ls /user/hadoop/f1/raw/
```

### Error: "Tabla no encontrada en Hive"
```bash
# Verificar que las tablas externas existen
beeline -u jdbc:hive2://localhost:10000 -e "USE f1; SHOW TABLES;"

# Verificar esquemas de las tablas
beeline -u jdbc:hive2://localhost:10000 -e "USE f1; DESCRIBE driver_results;"
beeline -u jdbc:hive2://localhost:10000 -e "USE f1; DESCRIBE constructor_results;"
```

## üìà M√©tricas de Rendimiento

### Tiempos de Ejecuci√≥n Estimados
- **inicio**: ~1 segundo
- **ingesta_datos_f1**: ~1-3 minutos (dependiendo de velocidad de conexi√≥n)
- **procesa_spark_f1**: ~2-5 minutos (dependiendo del tama√±o de datos)
- **verifica_driver_results**: ~10 segundos
- **verifica_constructor_results**: ~10 segundos
- **fin_proceso**: ~1 segundo

### Recursos Utilizados
- **CPU**: Moderado durante procesamiento Spark
- **Memoria**: ~2-4 GB durante procesamiento
- **Red**: Descarga de archivos CSV desde S3
- **Almacenamiento**: Archivos CSV en HDFS

## üéØ Resultados Esperados

Al ejecutar el DAG exitosamente, deber√≠as ver:

1. ‚úÖ **Todas las tareas en estado SUCCESS**
2. ‚úÖ **4 archivos CSV descargados y procesados**
3. ‚úÖ **Datos procesados en tablas externas de Hive:**
   - `driver_results`: Corredores con mayor cantidad de puntos
   - `constructor_results`: Constructores en Spanish Grand Prix 1991
4. ‚úÖ **Verificaci√≥n exitosa con beeline mostrando conteos**
5. ‚úÖ **Pipeline completo automatizado**

## üìã Consultas de Verificaci√≥n

Despu√©s de ejecutar el DAG, puedes verificar los resultados manualmente:

### Verificar driver_results
```sql
USE f1;
SELECT * FROM driver_results ORDER BY points DESC LIMIT 10;
```

### Verificar constructor_results
```sql
USE f1;
SELECT * FROM constructor_results ORDER BY points DESC;
```

## üîÑ Automatizaci√≥n Adicional

### Programaci√≥n Autom√°tica
Para ejecutar el DAG autom√°ticamente, modificar:
```python
schedule_interval='@daily',  # Ejecutar diariamente
# o
schedule_interval='0 2 * * *',  # Ejecutar a las 2 AM todos los d√≠as
```

### Notificaciones
Para agregar notificaciones por email:
```python
default_args = {
    'email_on_failure': True,
    'email_on_retry': True,
    'email': ['admin@company.com'],
}
```

## üì∏ Capturas de Pantalla

Despu√©s de ejecutar el DAG, captura:
1. **Vista del DAG** en Airflow mostrando todas las tareas en SUCCESS
2. **Resultados en Hive** mostrando los conteos y datos de las tablas

## üîó Referencias

- **Documentaci√≥n de scripts**: `../scripts/README.md`
- **Configuraci√≥n de Hive**: `../hive/README.md`
- **Diccionario de datos F1**: [Kaggle - Formula 1 Dataset](https://www.kaggle.com/datasets/rohanrao/formula-1-world-championship-1950-2020)

