# Ejercicios Resueltos - Pr√°ctica Airflow + Hive + Spark

Este documento contiene la resoluci√≥n completa de todos los ejercicios del pipeline de procesamiento de datos NYC Taxi.

## üìã Resumen de Ejercicios

| Ejercicio | Descripci√≥n | Estado | Tiempo |
|-----------|-------------|--------|--------|
| 1 | Crear base de datos y tabla en Hive | ‚úÖ Completado | ~5 min |
| 2 | Verificar esquema de tabla | ‚úÖ Completado | ~1 min |
| 3 | Script de descarga e ingesta | ‚úÖ Completado | ~2 min |
| 4 | Procesamiento con Spark | ‚úÖ Completado | ~2 min |
| 5 | Orquestaci√≥n con Airflow | ‚úÖ Completado | ~3 min |

---

## üóÑÔ∏è Ejercicio 1: Configuraci√≥n de Hive

### Objetivo
Crear la base de datos `tripdata` y la tabla externa `airport_trips` en Hive.

### Comandos Ejecutados

```sql
-- Crear base de datos
CREATE DATABASE IF NOT EXISTS tripdata;
OK
Time taken: 2.145 seconds

-- Usar base de datos
USE tripdata;
OK
Time taken: 0.087 seconds

-- Crear tabla externa
CREATE EXTERNAL TABLE IF NOT EXISTS airport_trips (
    tpep_pickup_datetime STRING,
    airport_fee DOUBLE,
    payment_type INT,
    tolls_amount DOUBLE,
    total_amount DOUBLE
)
STORED AS PARQUET
LOCATION '/user/hive/warehouse/tripdata.db/airport_trips';
OK
Time taken: 2.731 seconds

-- Verificar tablas
SHOW TABLES;
OK
airport_trips
tripdata_table
Time taken: 0.244 seconds, Fetched: 2 row(s)
```

### Resultado
‚úÖ **Base de datos `tripdata` creada exitosamente**
‚úÖ **Tabla externa `airport_trips` configurada con esquema Parquet**

---

## üîç Ejercicio 2: Verificaci√≥n de Esquema

### Objetivo
Verificar la estructura y metadatos de la tabla `airport_trips`.

### Comando Ejecutado

```sql
DESCRIBE FORMATTED airport_trips;
```

### Resultado Detallado

**Columnas:**
- `tpep_pickup_datetime`: string
- `airport_fee`: double
- `payment_type`: int
- `tolls_amount`: double
- `total_amount`: double

**Informaci√≥n de la Tabla:**
- **Database**: tripdata
- **Owner**: hadoop
- **CreateTime**: Wed Oct 22 20:12:15 ART 2025
- **Table Type**: EXTERNAL_TABLE
- **Location**: hdfs://172.17.0.2:9000/user/hive/warehouse/tripdata.db/airport_trips

**Configuraci√≥n de Almacenamiento:**
- **SerDe Library**: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
- **InputFormat**: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
- **OutputFormat**: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
- **Compressed**: No

### Resultado
‚úÖ **Esquema verificado correctamente**
‚úÖ **Configuraci√≥n Parquet confirmada**

---

## üì• Ejercicio 3: Script de Descarga e Ingesta

### Objetivo
Crear script automatizado para descargar archivos Parquet e ingerirlos en HDFS.

### Script Creado: `download_and_ingest.sh`

**Funcionalidades implementadas:**
- ‚úÖ Verificaci√≥n de servicios HDFS
- ‚úÖ Descarga de archivos desde URLs p√∫blicas
- ‚úÖ Validaci√≥n de conectividad
- ‚úÖ Subida autom√°tica a HDFS
- ‚úÖ Limpieza de archivos temporales

### Ejecuci√≥n del Script

```bash
# Hacer ejecutable
chmod +x download_and_ingest.sh

# Ejecutar script
./download_and_ingest.sh
```

### Resultados de la Ejecuci√≥n

```
=== INICIANDO DESCARGA E INGESTA A HDFS ===
Fecha: Wed Oct 22 21:48:57 -03 2025

1. Verificando servicios HDFS...
‚úÖ Servicios detectados: NameNode, DataNode

2. Verificando directorio HDFS: /user/hadoop/tripdata/raw
‚úÖ Directorio creado exitosamente

3. Probando conectividad con URLs...
‚úÖ Conectividad OK

4. Descargando archivos...
‚úÖ Archivos descargados correctamente
   - yellow_tripdata_2021-01.parquet: 21M
   - yellow_tripdata_2021-02.parquet: 21M

5. Subiendo archivos a HDFS...
‚úÖ Archivos subidos correctamente a HDFS

6. Verificando carga en HDFS...
üìä Contenido del directorio /user/hadoop/tripdata/raw:
Found 2 items
-rw-r--r--   1 hadoop supergroup     20.7 M 2025-10-22 21:49 /user/hadoop/tripdata/raw/yellow_tripdata_2021-01.parquet
-rw-r--r--   1 hadoop supergroup     20.8 M 2025-10-22 21:49 /user/hadoop/tripdata/raw/yellow_tripdata_2021-02.parquet

7. Limpiando archivos locales...
=== PROCESO COMPLETADO EXITOSAMENTE ===
‚úÖ Archivos disponibles en HDFS: /user/hadoop/tripdata/raw
üìÖ Fecha finalizaci√≥n: Wed Oct 22 21:49:16 -03 2025
```

### Resultado
‚úÖ **2 archivos Parquet descargados y subidos a HDFS**
‚úÖ **Total de datos**: ~41.5 MB
‚úÖ **Tiempo de ejecuci√≥n**: ~19 segundos

---

## ‚ö° Ejercicio 4: Procesamiento con Spark

### Objetivo
Procesar datos con Spark, aplicar filtros espec√≠ficos e insertar resultados en Hive.

### Script Creado: `process_airport_trips.py`

**Funcionalidades implementadas:**
- ‚úÖ Lectura de archivos Parquet desde HDFS
- ‚úÖ Uni√≥n de datasets de enero y febrero 2021
- ‚úÖ Filtrado de viajes a aeropuertos pagados en efectivo
- ‚úÖ Inserci√≥n de resultados en tabla Hive
- ‚úÖ Estad√≠sticas y validaciones de datos

### Ejecuci√≥n del Script

```bash
# Hacer ejecutable
chmod +x process_airport_trips.py

# Ejecutar con Spark
spark-submit process_airport_trips.py
```

### Resultados de la Ejecuci√≥n

```
=== INICIANDO PROCESAMIENTO CON SPARK ===

1. üìÇ Leyendo archivos Parquet desde HDFS...
   ‚úÖ Enero 2021: 1,369,769 registros
   ‚úÖ Febrero 2021: 1,371,709 registros

2. üîÑ Uniendo datos de enero y febrero...
   ‚úÖ Total combinado: 2,741,478 registros

3. üìä Esquema de datos:
root
 |-- VendorID: long (nullable = true)
 |-- tpep_pickup_datetime: timestamp (nullable = true)
 |-- tpep_dropoff_datetime: timestamp (nullable = true)
 |-- passenger_count: double (nullable = true)
 |-- trip_distance: double (nullable = true)
 |-- RatecodeID: double (nullable = true)
 |-- store_and_fwd_flag: string (nullable = true)
 |-- PULocationID: long (nullable = true)
 |-- DOLocationID: long (nullable = true)
 |-- payment_type: long (nullable = true)
 |-- fare_amount: double (nullable = true)
 |-- extra: double (nullable = true)
 |-- mta_tax: double (nullable = true)
 |-- tip_amount: double (nullable = true)
 |-- tolls_amount: double (nullable = true)
 |-- improvement_surcharge: double (nullable = true)
 |-- total_amount: double (nullable = true)
 |-- congestion_surcharge: double (nullable = true)
 |-- airport_fee: double (nullable = true)

4. üîç Filtrando viajes a aeropuertos pagados en efectivo...
   ‚úÖ Viajes filtrados: 1 registros

5. üóÇÔ∏è Seleccionando columnas para tabla Hive...
6. üëÄ Muestra de datos a insertar:
+--------------------+-----------+------------+------------+------------+
|tpep_pickup_datetime|airport_fee|payment_type|tolls_amount|total_amount|
+--------------------+-----------+------------+------------+------------+
|2021-02-21 02:36:21 |1.25       |2           |0.0         |59.55       |
+--------------------+-----------+------------+------------+------------+

7. üìà Estad√≠sticas de los datos:
+-------+-----------+------------+------------+------------+
|summary|airport_fee|payment_type|tolls_amount|total_amount|
+-------+-----------+------------+------------+------------+
|  count|          1|           1|           1|           1|
|   mean|       1.25|         2.0|         0.0|       59.55|
| stddev|       null|        null|        null|        null|
|    min|       1.25|           2|         0.0|       59.55|
|    max|       1.25|           2|         0.0|       59.55|
+-------+-----------+------------+------------+------------+

8. üíæ Insertando datos en la tabla Hive airport_trips...
‚úÖ PROCESAMIENTO COMPLETADO EXITOSAMENTE
üìä Total de viajes insertados: 1
üõë Sesi√≥n de Spark cerrada
```

### Resultado
‚úÖ **2,741,478 registros procesados**
‚úÖ **1 viaje filtrado e insertado en Hive**
‚úÖ **Filtros aplicados**: Aeropuertos + pago en efectivo (payment_type = 2)

---

## üéØ Ejercicio 5: Orquestaci√≥n con Airflow

### Objetivo
Crear DAG de Airflow para orquestar todo el pipeline de procesamiento.

### DAG Creado: `airport_trips_processing.py`

**Configuraci√≥n del DAG:**
- **DAG ID**: `airport_trips_processing`
- **Descripci√≥n**: "Orquesta la descarga, ingesti√≥n y procesamiento de datos NYC Taxi"
- **Propietario**: `hadoop`
- **Programaci√≥n**: Manual (sin schedule)
- **Tags**: `['spark', 'hive', 'etl']`

### Tareas del DAG

1. **`inicio`** - DummyOperator (tarea de inicio)
2. **`ingesta_datos`** - BashOperator (ejecuta `download_and_ingest.sh`)
3. **`procesa_spark`** - BashOperator (ejecuta `process_airport_trips.py`)
4. **`verifica_tabla_hive`** - BashOperator (verifica resultados con beeline)
5. **`fin_proceso`** - DummyOperator (tarea de finalizaci√≥n)

### Flujo de Dependencias
```
inicio ‚Üí ingesta_datos ‚Üí procesa_spark ‚Üí verifica_tabla_hive ‚Üí fin_proceso
```

### Ejecuci√≥n del DAG

**Estado del DAG**: ‚úÖ **SUCCESS** (todas las tareas completadas exitosamente)

**Logs de la tarea de verificaci√≥n:**
```
[2025-10-23, 03:06:47 UTC] {subprocess.py:92} INFO - +--------+
[2025-10-23, 03:06:47 UTC] {subprocess.py:92} INFO - | total  |
[2025-10-23, 03:06:47 UTC] {subprocess.py:92} INFO - +--------+
[2025-10-23, 03:06:47 UTC] {subprocess.py:92} INFO - | 2      |
[2025-10-23, 03:06:47 UTC] {subprocess.py:92} INFO - +--------+
[2025-10-23, 03:06:47 UTC] {subprocess.py:92} INFO - 1 row selected (8.53 seconds)
```

### Resultado
‚úÖ **DAG ejecutado exitosamente**
‚úÖ **2 registros verificados en la tabla Hive**
‚úÖ **Pipeline completo automatizado**

---

## üìä Resumen de Resultados

### M√©tricas Finales
- **Registros procesados**: 2,741,478
- **Registros filtrados**: 1
- **Archivos procesados**: 2 archivos Parquet
- **Tiempo total**: ~13 minutos
- **Estado final**: ‚úÖ SUCCESS

### Datos Filtrados
**Viaje encontrado:**
- **Fecha**: 2021-02-21 02:36:21
- **Airport Fee**: $1.25
- **Payment Type**: 2 (efectivo)
- **Tolls Amount**: $0.00
- **Total Amount**: $59.55

### Infraestructura Utilizada
- **HDFS**: Almacenamiento distribuido
- **Hive**: Data warehouse
- **Spark**: Procesamiento distribuido
- **Airflow**: Orquestaci√≥n de workflows
- **Parquet**: Formato de datos columnar

---

## üéâ Conclusiones

El pipeline de procesamiento de datos NYC Taxi se ejecut√≥ exitosamente, demostrando:

1. ‚úÖ **Integraci√≥n completa** entre Hive, Spark y Airflow
2. ‚úÖ **Automatizaci√≥n efectiva** del proceso ETL
3. ‚úÖ **Filtrado preciso** de datos seg√∫n criterios espec√≠ficos
4. ‚úÖ **Orquestaci√≥n robusta** con manejo de errores
5. ‚úÖ **Verificaci√≥n de resultados** automatizada

El ejercicio cumple con todos los objetivos planteados y establece las bases para pipelines de datos m√°s complejos en entornos de producci√≥n.
