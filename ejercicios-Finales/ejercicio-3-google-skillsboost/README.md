# Ejercicio Final 3 - Google Cloud Dataprep

## ğŸ“‹ Google Skills Boost LAB

**TÃ­tulo:** Creating a Data Transformation Pipeline with Cloud Dataprep  
**DuraciÃ³n:** 1 hora 15 minutos  
**Costo:** 5 CrÃ©ditos  
**Estado:** âœ… Completado

---

## ğŸ“ Respuestas a Preguntas

### 1. Â¿Para quÃ© se utiliza Data Prep?

Data Prep es un servicio de preparaciÃ³n de datos **sin cÃ³digo (no-code)** para limpiar, transformar y enriquecer datos de forma visual antes de cargarlos en BigQuery o Cloud Storage.

---

### 2. Â¿QuÃ© cosas se pueden realizar con DataPrep?

- âœ… **Limpieza de datos**: Eliminar duplicados, valores nulos, outliers
- âœ… **Transformaciones**: Renombrar columnas, cambiar tipos de datos, crear columnas calculadas
- âœ… **Enriquecimiento**: JOIN de mÃºltiples fuentes, agregar columnas derivadas
- âœ… **Filtrado**: Filtrar filas por condiciones especÃ­ficas
- âœ… **Agregaciones**: GROUP BY, SUM, AVG, COUNT
- âœ… **NormalizaciÃ³n**: Estandarizar formatos (fechas, strings, nÃºmeros)
- âœ… **DetecciÃ³n automÃ¡tica**: Identificar anomalÃ­as y sugerir transformaciones

---

### 3. Â¿Por quÃ© otra/s herramientas lo podrÃ­as reemplazar? Â¿Por quÃ©?

| Herramienta | RazÃ³n de Reemplazo |
|-------------|-------------------|
| **Apache Spark/PySpark** | Mayor control, procesamiento masivo (TB+), transformaciones complejas |
| **dbt (Data Build Tool)** | Transformaciones SQL versionadas, CI/CD, testing automatizado |
| **Cloud Dataflow** | Procesamiento en streaming, pipelines en tiempo real |
| **Pandas (Python)** | Scripts personalizados, integraciÃ³n con ML, flexibilidad total |
| **Talend / Informatica** | ETL empresarial, conectores legacy, auditorÃ­a |

**Â¿Por quÃ© reemplazar?**
- Si necesitas **cÃ³digo versionado** â†’ dbt o PySpark
- Si necesitas **streaming** â†’ Dataflow
- Si necesitas **control total** â†’ PySpark/Pandas
- Si el equipo **sabe programar** â†’ PySpark (mÃ¡s barato)

**Â¿Por quÃ© NO reemplazar?**
- Usuarios de negocio sin conocimientos tÃ©cnicos
- Prototipado rÃ¡pido
- Presupuesto limitado (pago por uso)

---

### 4. Â¿CuÃ¡les son los casos de uso comunes de Data Prep de GCP?

1. **PreparaciÃ³n de datos para BigQuery**: Limpiar CSVs antes de cargar en DW
2. **IntegraciÃ³n de fuentes mÃºltiples**: JOIN de Cloud SQL + CSV + JSON
3. **Limpieza de logs**: Parsear logs de aplicaciones, normalizar campos
4. **PreparaciÃ³n para ML**: Feature engineering, one-hot encoding, normalizaciÃ³n
5. **MigraciÃ³n de datos**: Transformar datos de legacy a formato moderno
6. **ValidaciÃ³n de calidad**: Detectar datos faltantes, duplicados, inconsistencias
7. **Enriquecimiento de datos**: Agregar columnas calculadas, lookups

---

### 5. Â¿CÃ³mo se cargan los datos en Data Prep de GCP?

**Fuentes soportadas:**
- âœ… **Cloud Storage (GCS)**: CSV, JSON, Avro, Parquet
- âœ… **BigQuery**: Tablas existentes
- âœ… **Cloud SQL**: MySQL, PostgreSQL
- âœ… **Google Sheets**: Hojas de cÃ¡lculo
- âœ… **URL HTTP**: Archivos pÃºblicos

**Proceso:**
1. Conectar fuente de datos
2. Seleccionar archivo/tabla
3. Data Prep infiere schema automÃ¡ticamente
4. Preview de datos (primeras 10k filas)
5. Iniciar transformaciones

---

### 6. Â¿QuÃ© tipos de datos se pueden preparar en Data Prep de GCP?

**Formatos:**
- CSV, TSV (delimitados)
- JSON (estructurado y semi-estructurado)
- Avro
- Parquet (columnar)
- Excel (.xlsx)
- Text files

**Tipos de datos:**
- String, Integer, Float, Boolean, Date, Timestamp
- Arrays, Structs (JSON anidado)
- Binarios (conversiÃ³n a base64)

**TamaÃ±o mÃ¡ximo:** Hasta **2 TB** por dataset

---

### 7. Â¿QuÃ© pasos se pueden seguir para limpiar y transformar datos en Data Prep de GCP?

**Pipeline tÃ­pico:**

```
1. IMPORTAR datos â†’ Preview automÃ¡tico
2. DETECTAR anomalÃ­as â†’ Sugerencias automÃ¡ticas
3. ELIMINAR columnas innecesarias â†’ Drop
4. RENOMBRAR columnas â†’ Nombres descriptivos
5. CAMBIAR tipos de datos â†’ String â†’ Date, Int â†’ Float
6. MANEJAR nulos â†’ Reemplazar con 0, media, o eliminar filas
7. ELIMINAR duplicados â†’ Distinct
8. FILTRAR filas â†’ Condiciones WHERE
9. CREAR columnas calculadas â†’ Derivaciones
10. JOIN con otras fuentes â†’ LEFT/INNER/OUTER JOIN
11. AGREGAR (GROUP BY) â†’ SUM, AVG, COUNT
12. EXPORTAR a BigQuery o GCS
```

**Interfaz visual:** Cada paso se registra como "receta" replicable.

---

### 8. Â¿CÃ³mo se pueden automatizar tareas de preparaciÃ³n de datos en Data Prep de GCP?

**MÃ©todos de automatizaciÃ³n:**

1. **Programar ejecuciÃ³n (Scheduling)**:
   - Diario, semanal, mensual
   - Trigger por llegada de nuevo archivo

2. **API de Dataprep**:
   ```python
   # Ejecutar receta via API
   POST /v4/jobGroups
   {
     "wrangledDataset": {"id": 12345},
     "runParameters": {"overrides": {...}}
   }
   ```

3. **IntegraciÃ³n con Cloud Composer (Airflow)**:
   ```python
   from airflow.providers.google.cloud.operators.dataprep import DataprepRunJobOperator
   
   run_dataprep = DataprepRunJobOperator(
       task_id='run_dataprep_recipe',
       recipe_id=12345
   )
   ```

4. **Cloud Functions + Pub/Sub**:
   - Trigger automÃ¡tico cuando llega archivo a GCS
   - Cloud Function ejecuta receta Dataprep

5. **ParÃ¡metros dinÃ¡micos**:
   - Variables en receta (ej: fecha actual)
   - Sobrescribir valores en tiempo de ejecuciÃ³n

---

### 9. Â¿QuÃ© tipos de visualizaciones se pueden crear en Data Prep de GCP?

**Visualizaciones incorporadas:**

- ğŸ“Š **Histogramas**: DistribuciÃ³n de valores numÃ©ricos
- ğŸ“ˆ **Column profile**: EstadÃ­sticas por columna (min, max, media, nulos)
- ğŸ¥§ **Value distribution**: Frecuencia de valores categÃ³ricos
- ğŸ”¢ **Data quality score**: Porcentaje de datos vÃ¡lidos
- ğŸ¯ **Missing values chart**: Porcentaje de nulos por columna
- ğŸ“‰ **Outlier detection**: Valores fuera de rango esperado
- ğŸ”— **Relationship matrix**: CorrelaciÃ³n entre columnas

**LimitaciÃ³n:** No es una herramienta de BI completa (solo EDA - Exploratory Data Analysis)

**Para visualizaciones avanzadas:**
- Exportar a **Looker Studio** (Data Studio)
- Exportar a **Looker** (BI empresarial)
- Conectar BigQuery con **Tableau** o **Power BI**

---

### 10. Â¿CÃ³mo se puede garantizar la calidad de los datos en Data Prep de GCP?

**Mecanismos de calidad:**

1. **Data Quality Rules**:
   ```
   - Column NOT NULL
   - Value IN ('A', 'B', 'C')
   - Range BETWEEN 0 AND 100
   - Regex MATCH '^[A-Z]{2}[0-9]{6}$'
   ```

2. **Sugerencias automÃ¡ticas**:
   - Detecta outliers (valores extremos)
   - Identifica formatos inconsistentes
   - Sugiere correcciones

3. **ValidaciÃ³n pre-proceso**:
   - Schema validation (tipos de datos esperados)
   - Row count validation (nÃºmero esperado de filas)

4. **Alertas post-proceso**:
   - Email si calidad < threshold (ej: >10% nulos)
   - Detener pipeline si falla validaciÃ³n crÃ­tica

5. **Monitoreo continuo**:
   - Cloud Logging: registra cada ejecuciÃ³n
   - Cloud Monitoring: mÃ©tricas de calidad
   - BigQuery: audit logs

6. **Testing de recetas**:
   - Ejecutar en subset de datos
   - Comparar output esperado vs real
   - Versionado de recetas (rollback si falla)

**Best practice:** Combinar Data Prep con **dbt tests** para validaciÃ³n exhaustiva.

---

## ğŸ—ï¸ Arquitectura Propuesta: GCP con Data Prep

### Diagrama de Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CAPA DE INGESTA                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  AWS S3      â”‚           â”‚  Cloud SQL   â”‚               â”‚
â”‚  â”‚  (Parquet)   â”‚           â”‚  (PostgreSQL)â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚         â”‚                          â”‚                        â”‚
â”‚         â”‚ Transfer Service         â”‚ Cloud SQL Connector    â”‚
â”‚         â–¼                          â–¼                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚     Cloud Storage (GCS)            â”‚                    â”‚
â”‚  â”‚     gs://car-rental-raw/           â”‚                    â”‚
â”‚  â”‚     â€¢ parquet files (S3)           â”‚                    â”‚
â”‚  â”‚     â€¢ export from Cloud SQL        â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                 â”‚                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CAPA DE PROCESAMIENTO                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚      Cloud Dataprep (Trifacta)         â”‚                â”‚
â”‚  â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚                â”‚
â”‚  â”‚  1ï¸âƒ£ Conectar fuentes:                  â”‚                â”‚
â”‚  â”‚     â€¢ GCS (parquet from S3)            â”‚                â”‚
â”‚  â”‚     â€¢ Cloud SQL (via connector)        â”‚                â”‚
â”‚  â”‚                                         â”‚                â”‚
â”‚  â”‚  2ï¸âƒ£ Transformaciones (sin cÃ³digo):     â”‚                â”‚
â”‚  â”‚     â€¢ Filtrar datos inconsistentes     â”‚                â”‚
â”‚  â”‚     â€¢ Limpiar valores nulos            â”‚                â”‚
â”‚  â”‚     â€¢ JOIN de ambas fuentes            â”‚                â”‚
â”‚  â”‚     â€¢ Renombrar columnas               â”‚                â”‚
â”‚  â”‚     â€¢ Crear columnas calculadas        â”‚                â”‚
â”‚  â”‚     â€¢ Normalizar formatos              â”‚                â”‚
â”‚  â”‚                                         â”‚                â”‚
â”‚  â”‚  3ï¸âƒ£ ValidaciÃ³n de calidad:             â”‚                â”‚
â”‚  â”‚     â€¢ Detectar outliers                â”‚                â”‚
â”‚  â”‚     â€¢ Verificar schema                 â”‚                â”‚
â”‚  â”‚     â€¢ Alertas si calidad < 95%         â”‚                â”‚
â”‚  â”‚                                         â”‚                â”‚
â”‚  â”‚  4ï¸âƒ£ Output:                            â”‚                â”‚
â”‚  â”‚     â€¢ Formato: Avro/Parquet            â”‚                â”‚
â”‚  â”‚     â€¢ Destino: BigQuery                â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                   â”‚                                          â”‚
â”‚                   â”‚ (ejecuta Dataflow jobs)                 â”‚
â”‚                   â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚      Cloud Dataflow (backend)          â”‚                â”‚
â”‚  â”‚      â€¢ Ejecuta transformaciones        â”‚                â”‚
â”‚  â”‚      â€¢ Escalado automÃ¡tico             â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                   â”‚                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CAPA DE ALMACENAMIENTO                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚         BigQuery (DW)                  â”‚                â”‚
â”‚  â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚                â”‚
â”‚  â”‚  Dataset: car_rental_analytics         â”‚                â”‚
â”‚  â”‚  Tables:                                â”‚                â”‚
â”‚  â”‚    â€¢ fact_rentals                      â”‚                â”‚
â”‚  â”‚    â€¢ dim_vehicles                      â”‚                â”‚
â”‚  â”‚    â€¢ dim_customers                     â”‚                â”‚
â”‚  â”‚    â€¢ dim_locations                     â”‚                â”‚
â”‚  â”‚                                         â”‚                â”‚
â”‚  â”‚  Features:                              â”‚                â”‚
â”‚  â”‚    â€¢ Particionado por fecha            â”‚                â”‚
â”‚  â”‚    â€¢ Clustering por location           â”‚                â”‚
â”‚  â”‚    â€¢ CompresiÃ³n automÃ¡tica             â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                   â”‚                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚
        â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CAPA DE BI     â”‚   â”‚   CAPA DE ML     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  â”‚   â”‚                  â”‚
â”‚ Looker Studio    â”‚   â”‚ Vertex AI        â”‚
â”‚ (Data Studio)    â”‚   â”‚ (AutoML)         â”‚
â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚   â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚ â€¢ Dashboards     â”‚   â”‚ â€¢ RegresiÃ³n      â”‚
â”‚ â€¢ KPIs           â”‚   â”‚   Lineal         â”‚
â”‚ â€¢ Filtros        â”‚   â”‚ â€¢ PredicciÃ³n de  â”‚
â”‚ â€¢ Compartir      â”‚   â”‚   demanda        â”‚
â”‚                  â”‚   â”‚ â€¢ AutoML Tables  â”‚
â”‚ Alternativa:     â”‚   â”‚                  â”‚
â”‚ â€¢ Looker         â”‚   â”‚ Alternativa:     â”‚
â”‚ â€¢ Tableau        â”‚   â”‚ â€¢ BigQuery ML    â”‚
â”‚ â€¢ Power BI       â”‚   â”‚ â€¢ Notebooks      â”‚
â”‚                  â”‚   â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   ORQUESTACIÃ“N             â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚  Cloud Composer (Airflow)  â”‚
         â”‚  â€¢ Schedule diario         â”‚
         â”‚  â€¢ Trigger por archivo     â”‚
         â”‚  â€¢ Monitoreo               â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   MONITOREO                â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚  â€¢ Cloud Logging           â”‚
         â”‚  â€¢ Cloud Monitoring        â”‚
         â”‚  â€¢ Alertas (email/Slack)   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Componentes de la Arquitectura

### 1. Ingesta

**AWS S3 â†’ GCS:**
```bash
# Transfer Service (configuraciÃ³n one-time)
gsutil -m cp -r s3://bucket-source/parquet/* gs://car-rental-raw/parquet/
```

**Cloud SQL â†’ GCS:**
```sql
-- Exportar tabla a GCS (desde Cloud SQL)
EXPORT DATA OPTIONS(
  uri='gs://car-rental-raw/sql_export/*.csv',
  format='CSV',
  overwrite=true
) AS
SELECT * FROM rentals WHERE date >= '2024-01-01';
```

---

### 2. Procesamiento (Cloud Dataprep)

**Receta de transformaciÃ³n:**
1. Importar parquet desde GCS
2. Importar export de Cloud SQL
3. JOIN por `rental_id`
4. Filtrar: `WHERE status = 'completed'`
5. Limpiar nulos: `rating` â†’ reemplazar con media
6. Normalizar: `fuelType` â†’ lowercase
7. Crear columna: `revenue = rate_daily * days_rented`
8. Validar: rating BETWEEN 1 AND 5
9. Exportar a BigQuery: `car_rental_analytics.fact_rentals`

---

### 3. Almacenamiento (BigQuery)

```sql
-- Tabla particionada y clusterizada
CREATE TABLE car_rental_analytics.fact_rentals
(
  rental_id INT64,
  rental_date DATE,
  vehicle_id INT64,
  customer_id INT64,
  location STRING,
  fuelType STRING,
  rating INT64,
  revenue FLOAT64
)
PARTITION BY rental_date
CLUSTER BY location, fuelType;
```

---

### 4. BI (Looker Studio)

**Dashboards:**
- ğŸ“Š Revenue por mes/ciudad
- ğŸš— VehÃ­culos mÃ¡s rentados
- â­ SatisfacciÃ³n por tipo de combustible
- ğŸ“ Mapa de calor por ubicaciÃ³n

---

### 5. ML (Vertex AI)

**Modelo de regresiÃ³n lineal:**
```python
# Predecir demanda de alquileres
from google.cloud import aiplatform

# AutoML Tables
dataset = aiplatform.TabularDataset.create(
    display_name="car_rental_demand",
    bq_source="bq://project.car_rental_analytics.fact_rentals"
)

model = aiplatform.AutoMLTabularTrainingJob(
    display_name="rental_demand_forecast",
    optimization_objective="minimize-rmse",
)

model.run(
    dataset=dataset,
    target_column="revenue",
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1,
)
```

**Alternativa: BigQuery ML**
```sql
CREATE OR REPLACE MODEL car_rental_analytics.demand_forecast
OPTIONS(
  model_type='LINEAR_REG',
  input_label_cols=['revenue']
) AS
SELECT
  rental_date,
  location,
  fuelType,
  rating,
  revenue
FROM car_rental_analytics.fact_rentals;
```

---

## ğŸ¯ Ventajas de esta Arquitectura

| Ventaja | DescripciÃ³n |
|---------|-------------|
| âœ… **Sin cÃ³digo** | Equipo de negocio puede transformar datos |
| âœ… **Visual** | Interfaz drag-and-drop, fÃ¡cil de entender |
| âœ… **Escalable** | Dataflow backend maneja TB de datos |
| âœ… **Integrado** | Nativo con BigQuery, Cloud SQL, GCS |
| âœ… **EconÃ³mico** | Pago por uso (vs licencias ETL tradicionales) |
| âœ… **RÃ¡pido** | Prototipado en minutos, no dÃ­as |

---

## ğŸ’° Costos Estimados Mensuales

| Servicio | Uso | Costo USD/mes |
|----------|-----|---------------|
| Cloud Storage (GCS) | 100 GB | $2.00 |
| Cloud Dataprep | 100 GB procesados | $15.00 |
| Cloud Dataflow | 10 GB-hours | $5.00 |
| BigQuery (storage) | 200 GB | $4.00 |
| BigQuery (queries) | 1 TB procesados | $5.00 |
| Looker Studio | Gratis | $0.00 |
| Vertex AI (AutoML) | 1 modelo/mes | $50.00 |
| **TOTAL** | | **~$81/mes** |

---

## ğŸ“š Referencias

- [Cloud Dataprep Documentation](https://cloud.google.com/dataprep/docs)
- [BigQuery ML](https://cloud.google.com/bigquery-ml/docs)
- [Vertex AI](https://cloud.google.com/vertex-ai/docs)
- [Looker Studio](https://lookerstudio.google.com/)

---

**Autor:** Data Engineering Team - Edvai  
**Fecha:** 2025-11-24  
**Ejercicio:** Final 3 - Google Cloud Dataprep

