# Ejercicio Final 1 - Aviaci√≥n Civil: Airflow + PySpark + Hive

Este ejercicio implementa un pipeline ETL completo para procesar datos de aviaci√≥n argentina usando **Apache Airflow** para orquestaci√≥n, **PySpark** para transformaciones distribuidas (sin Pandas), y **Apache Hive** para almacenamiento y an√°lisis SQL de datos de vuelos y aeropuertos.

## üéØ Objetivos

- Implementar ingesta automatizada de archivos CSV desde S3 a HDFS
- Crear tablas estructuradas en Hive con schemas espec√≠ficos
- Desarrollar script PySpark con transformaciones complejas
- Orquestar pipeline con Apache Airflow
- Aplicar an√°lisis de negocio con SQL
- Elaborar conclusiones y proponer arquitectura alternativa

## üìã Ejercicios Incluidos

### 1Ô∏è‚É£ **Ingest de Archivos (Punto 1)**
- Descargar 3 archivos CSV desde S3
- Subir archivos a HDFS `/ingest`
- Verificar archivos en sistema distribuido

### 2Ô∏è‚É£ **Creaci√≥n de Tablas en Hive (Punto 2)**
- Base de datos: `aviacion`
- Tabla 1: `aeropuerto_tabla` (vuelos 2021-2022)
- Tabla 2: `aeropuerto_detalles_tabla` (detalles aeropuertos)
- Aplicar schemas espec√≠ficos seg√∫n PDF

### 3Ô∏è‚É£ **Script PySpark con Transformaciones (Punto 4)**
- Eliminar columnas innecesarias (inhab, fir, calidad del dato)
- Filtrar vuelos internacionales (solo dom√©sticos)
- Rellenar valores NULL con 0 (pasajeros, distancia_ref)
- Convertir fechas a formato Date
- Normalizar nombres de columnas (tildes, par√©ntesis)
- Mapeo correcto de columnas de aeropuertos

### 4Ô∏è‚É£ **Orquestaci√≥n con Airflow (Punto 3)**
- DAG: `aviacion_processing_spark_dag`
- Flujo: crear_tablas ‚Üí procesar_datos ‚Üí verificar
- Ejecuci√≥n automatizada del pipeline
- Integraci√≥n con Spark

### 5Ô∏è‚É£ **Verificar Tipos de Datos (Punto 5)**
- Schema de aeropuerto_tabla
- Validar tipos: fecha DATE, pasajeros INT, etc.

### 6Ô∏è‚É£ **An√°lisis: Vuelos por Periodo (Punto 6)**
- Contar vuelos entre 01/12/2021 y 31/01/2022
- Resultado esperado: ~57,984 vuelos

### 7Ô∏è‚É£ **An√°lisis: Pasajeros por Aerol√≠nea (Punto 7)**
- Suma de pasajeros de Aerol√≠neas Argentinas
- Periodo: 01/01/2021 - 30/06/2022
- Resultado esperado: ~7.4M pasajeros

### 8Ô∏è‚É£ **Tablero de Vuelos con Ciudades (Punto 8)**
- JOIN entre vuelos y aeropuertos
- Mostrar ciudades de origen y destino
- Top 10 vuelos ordenados por fecha

### 9Ô∏è‚É£ **Top 10 Aerol√≠neas (Punto 9)**
- Ranking por pasajeros transportados
- Visualizaci√≥n con gr√°fico de barras
- Periodo: 2021-2022

### üîü **Top 10 Aeronaves desde Buenos Aires (Punto 10)**
- Despegues desde CABA y Buenos Aires
- Conteo por modelo de aeronave
- Visualizaci√≥n con gr√°fico de barras

### 1Ô∏è‚É£1Ô∏è‚É£ **Datos Externos Recomendados (Punto 11)**
- Datos meteorol√≥gicos (clima, vientos, visibilidad)
- Calendario de feriados y eventos
- Datos econ√≥micos (precios, inflaci√≥n, tipo de cambio)
- Capacidad de aeronaves (asientos totales)
- Datos de puntualidad y retrasos
- Informaci√≥n de aeropuertos (capacidad, pistas)

### 1Ô∏è‚É£2Ô∏è‚É£ **Conclusiones y Recomendaciones (Punto 12)**

#### Calidad de Datos
- ‚úÖ Dataset robusto: 143,000 registros de vuelos procesados
- ‚úÖ Cobertura temporal completa: 18 meses (2021-2022)
- ‚ö†Ô∏è Valores NULL en pasajeros (~5%) y distancia_ref (~10%)
- ‚ö†Ô∏è Vuelos internacionales representan 47.5% del total (excluidos)

#### Concentraci√≥n del Mercado
- **Aerol√≠neas Argentinas**: Dominancia del 70% del mercado dom√©stico
- **Top 3 aerol√≠neas**: Concentran >85% de los pasajeros
- **Low-cost**: Crecimiento de Flybondi y JetSmart (competencia)

#### Patrones de Temporalidad
- **Alta temporada**: Diciembre-Enero (verano argentino)
- **Frecuencia**: ~475 vuelos/d√≠a promedio
- **Rutas principales**: Buenos Aires (EZE/AEP) ‚ü∑ Provincias

#### Recomendaciones Operativas
1. **Calidad de Datos**: Implementar validaciones en origen
2. **Diversificaci√≥n**: Incentivar competencia para reducir monopolio
3. **Capacidad**: Optimizar slots en aeropuertos congestionados (AEP)
4. **Sostenibilidad**: Monitorear emisiones por aeronave
5. **Conectividad**: Mejorar rutas interprovinciales directas

üìÑ **Ver documento completo:** `CONCLUSIONES_Y_ARQUITECTURA.md`

### 1Ô∏è‚É£3Ô∏è‚É£ **Arquitectura Alternativa (Punto 13)**

#### Opci√≥n 1: Cloud AWS

**Arquitectura Propuesta:**
```
S3 ‚Üí Glue ETL ‚Üí Athena/Redshift ‚Üí QuickSight
         ‚Üì
    Step Functions (orquestaci√≥n)
```

**Stack Tecnol√≥gico:**
- **Almacenamiento**: S3 (datos raw y procesados)
- **ETL**: AWS Glue (Spark managed)
- **Orquestaci√≥n**: Step Functions / MWAA (Airflow managed)
- **DW**: Redshift Spectrum o Athena
- **BI**: QuickSight o Tableau
- **Gobierno**: Glue Data Catalog

**Ventajas:**
‚úÖ Escalabilidad autom√°tica  
‚úÖ Pago por uso (no infraestructura idle)  
‚úÖ Integraci√≥n nativa entre servicios  
‚úÖ Managed services (menos DevOps)  

**Desventajas:**
‚ùå Costo variable (dif√≠cil presupuesto)  
‚ùå Vendor lock-in  
‚ùå Curva de aprendizaje AWS  

**Costo Estimado:** USD 300-800/mes (143k registros/mes)

---

#### Opci√≥n 2: Cloud GCP

**Arquitectura Propuesta:**
```
GCS ‚Üí Dataproc ‚Üí BigQuery ‚Üí Data Studio
        ‚Üì
  Cloud Composer (Airflow managed)
```

**Stack Tecnol√≥gico:**
- **Almacenamiento**: Cloud Storage
- **ETL**: Dataproc (Spark) o Dataflow
- **Orquestaci√≥n**: Cloud Composer (Airflow managed)
- **DW**: BigQuery (columnar, serverless)
- **BI**: Looker o Data Studio

**Ventajas:**
‚úÖ BigQuery extremadamente r√°pido para anal√≠tica  
‚úÖ Integraci√≥n con TensorFlow (ML futuro)  
‚úÖ Costos m√°s predecibles que AWS  

**Costo Estimado:** USD 250-700/mes

---

#### Opci√≥n 3: On-Premise Mejorado

**Arquitectura Propuesta:**
```
NiFi ‚Üí HDFS ‚Üí Spark (yarn) ‚Üí Hive ‚Üí Superset
              ‚Üì
         Airflow (orchestration)
```

**Mejoras vs Actual:**
- **Alta Disponibilidad**: Cluster multi-nodo (3+ nodes)
- **Monitoreo**: Prometheus + Grafana
- **Backup**: Snapshots HDFS + DR site
- **CI/CD**: Jenkins para deployments
- **Seguridad**: Kerberos + Ranger

**Ventajas:**
‚úÖ Control total de datos (cumplimiento normativo)  
‚úÖ Costo fijo predecible  
‚úÖ Sin latencia de red cloud  
‚úÖ Privacidad de datos sensibles  

**Desventajas:**
‚ùå CAPEX inicial alto (hardware)  
‚ùå Requiere equipo DevOps dedicado  
‚ùå Escalabilidad limitada por hardware  

**Costo Estimado:**  
- **CAPEX**: USD 30,000 (cluster 5 nodos)  
- **OPEX**: USD 2,000/mes (salarios, electricidad, mantenimiento)

---

#### Recomendaci√≥n Final

**Para ANAC (organismo gubernamental):**

üèÜ **H√≠brido: On-Premise + Cloud Backup**

**Justificaci√≥n:**
1. **Datos sensibles**: Informaci√≥n de vuelos puede ser estrat√©gica
2. **Presupuesto estatal**: CAPEX m√°s f√°cil de aprobar que OPEX recurrente
3. **Soberan√≠a de datos**: Cumplimiento normativo argentino
4. **DR en Cloud**: S3 Glacier para backups (bajo costo)

**Configuraci√≥n Recomendada:**
- **Producci√≥n**: On-premise (Hadoop cluster)
- **Backup/DR**: AWS S3 Glacier
- **BI P√∫blico**: Athena + QuickSight (consultas p√∫blicas)
- **ML/Innovaci√≥n**: GCP Dataproc (proyectos piloto)

üìÑ **Ver documento completo:** `CONCLUSIONES_Y_ARQUITECTURA.md` con diagramas, costos y roadmap de implementaci√≥n.

## üìÅ Estructura del Proyecto

```
ejercicios-Finales/ejercicio-1/
‚îú‚îÄ‚îÄ README.md                           # Documentaci√≥n principal
‚îú‚îÄ‚îÄ SOLUCION_COMPLETA_EJERCICIO_1.md    # Gu√≠a paso a paso completa
‚îú‚îÄ‚îÄ TESTEAR_EN_HIVE.md                  # Instrucciones de testing
‚îú‚îÄ‚îÄ CONCLUSIONES_Y_ARQUITECTURA.md      # Puntos 12 y 13 (An√°lisis completo)
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ ingest_aviacion.sh             # Script bash para descarga e ingest
‚îÇ   ‚îî‚îÄ‚îÄ process_aviacion_spark.py      # Script PySpark de transformaci√≥n
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îî‚îÄ‚îÄ aviacion_spark_dag.py          # DAG de Airflow
‚îî‚îÄ‚îÄ hive/
    ‚îî‚îÄ‚îÄ queries_aviacion.sql           # CREATE TABLE + Consultas SQL
```

## üöÄ Tecnolog√≠as Utilizadas

- **Apache Spark (PySpark)** - Procesamiento distribuido de datos
- **Apache Airflow** - Orquestaci√≥n de pipelines
- **Apache Hive** - Data warehouse y consultas SQL
- **HDFS** - Sistema de archivos distribuido
- **Python** - Lenguaje de programaci√≥n
- **Bash** - Scripting y automatizaci√≥n
- **Docker** - Contenedorizaci√≥n

## üìä Datasets Utilizados

### Vuelos 2021
- **URL**: https://data-engineer-edvai-public.s3.amazonaws.com/2021-informe-ministerio.csv
- **Registros**: ~115,000 vuelos
- **Formato**: CSV con delimitador `;`
- **Periodo**: 01/01/2021 - 31/12/2021

### Vuelos 2022 (Primer Semestre)
- **URL**: https://data-engineer-edvai-public.s3.amazonaws.com/202206-informe-ministerio.csv
- **Registros**: ~28,000 vuelos
- **Formato**: CSV con delimitador `;`
- **Periodo**: 01/01/2022 - 30/06/2022

### Detalles de Aeropuertos
- **URL**: https://data-engineer-edvai-public.s3.amazonaws.com/aeropuertos_detalle.csv
- **Registros**: 54 aeropuertos argentinos
- **Formato**: CSV con delimitador `;`

### Schema Tabla 1: aeropuerto_tabla

| Campo | Tipo | Descripci√≥n |
|-------|------|-------------|
| fecha | DATE | Fecha del vuelo (YYYY-MM-DD) |
| horaUTC | STRING | Hora UTC del vuelo |
| clase_de_vuelo | STRING | Tipo de vuelo (Regular, Privado, etc.) |
| clasificacion_de_vuelo | STRING | Dom√©stico/Internacional |
| tipo_de_movimiento | STRING | Aterrizaje/Despegue |
| aeropuerto | STRING | C√≥digo IATA del aeropuerto |
| origen_destino | STRING | C√≥digo del aeropuerto de origen/destino |
| aerolinea_nombre | STRING | Nombre de la aerol√≠nea |
| aeronave | STRING | Modelo de la aeronave |
| pasajeros | INT | Cantidad de pasajeros |

### Schema Tabla 2: aeropuerto_detalles_tabla

| Campo | Tipo | Descripci√≥n |
|-------|------|-------------|
| aeropuerto | STRING | C√≥digo del aeropuerto |
| oac | STRING | C√≥digo OACI |
| iata | STRING | C√≥digo IATA |
| tipo | STRING | Tipo de aeropuerto |
| denominacion | STRING | Nombre completo del aeropuerto |
| coordenadas_latitud | STRING | Latitud geogr√°fica |
| coordenadas_longitud | STRING | Longitud geogr√°fica |
| elev | FLOAT | Elevaci√≥n en metros |
| uom_elev | STRING | Unidad de medida de elevaci√≥n |
| ref | STRING | Referencia geogr√°fica |
| distancia_ref | FLOAT | Distancia a referencia |
| direccion_ref | STRING | Direcci√≥n a referencia |
| condicion | STRING | Condici√≥n operativa |
| control | STRING | Tipo de control |
| region | STRING | Regi√≥n geogr√°fica |
| uso | STRING | Uso del aeropuerto |
| trafico | STRING | Tipo de tr√°fico |
| sna | STRING | Sistema nacional de aeropuertos |
| concesionado | STRING | Estado de concesi√≥n |
| provincia | STRING | Provincia argentina |

## üîß Requisitos Previos

- Contenedor Hadoop/Hive ejecut√°ndose
- Apache Spark instalado y configurado
- Apache Airflow instalado y funcionando
- Python 3.8+ con PySpark
- HDFS accesible (hdfs://172.17.0.2:9000)
- Java 11 instalado
- Acceso a internet para descarga de datos

## üöÄ Pipeline Completo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         PASO 1: INGEST (Descarga + HDFS)                    ‚îÇ
‚îÇ  CSV Files (S3) ‚Üí /home/hadoop/landing ‚Üí HDFS:/ingest      ‚îÇ
‚îÇ  ‚Ä¢ 2021-informe-ministerio.csv (32 MB)                      ‚îÇ
‚îÇ  ‚Ä¢ 202206-informe-ministerio.csv (22 MB)                    ‚îÇ
‚îÇ  ‚Ä¢ aeropuertos_detalle.csv (136 KB)                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         PASO 2: CREACI√ìN DE TABLAS (Hive)                   ‚îÇ
‚îÇ  CREATE DATABASE aviacion;                                   ‚îÇ
‚îÇ  ‚Ä¢ aeropuerto_tabla (10 columnas)                           ‚îÇ
‚îÇ  ‚Ä¢ aeropuerto_detalles_tabla (20 columnas)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         PASO 3: PROCESAMIENTO (PySpark)                      ‚îÇ
‚îÇ  process_aviacion_spark.py                                   ‚îÇ
‚îÇ  ‚Ä¢ Leer CSV desde HDFS                                       ‚îÇ
‚îÇ  ‚Ä¢ Union de 2021 + 2022 (unionByName)                       ‚îÇ
‚îÇ  ‚Ä¢ Normalizar columnas (tildes, par√©ntesis)                 ‚îÇ
‚îÇ  ‚Ä¢ Filtrar vuelos internacionales                           ‚îÇ
‚îÇ  ‚Ä¢ Rellenar NULL con 0                                      ‚îÇ
‚îÇ  ‚Ä¢ Convertir fechas DD/MM/YYYY ‚Üí DATE                       ‚îÇ
‚îÇ  ‚Ä¢ JOIN vuelos + aeropuertos                                ‚îÇ
‚îÇ  ‚Ä¢ Escribir a Hive (.saveAsTable)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         PASO 4: ORQUESTACI√ìN (Airflow DAG)                   ‚îÇ
‚îÇ  aviacion_processing_spark_dag                               ‚îÇ
‚îÇ  inicio ‚Üí crear_tablas ‚Üí procesar_spark ‚Üí verificar ‚Üí fin  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         PASO 5-10: AN√ÅLISIS (Hive SQL)                       ‚îÇ
‚îÇ  ‚Ä¢ Punto 5: DESCRIBE (tipos de datos)                       ‚îÇ
‚îÇ  ‚Ä¢ Punto 6: COUNT vuelos (dic 2021 - ene 2022)              ‚îÇ
‚îÇ  ‚Ä¢ Punto 7: SUM pasajeros (Aerol√≠neas Argentinas)           ‚îÇ
‚îÇ  ‚Ä¢ Punto 8: Tablero con ciudades (JOIN)                     ‚îÇ
‚îÇ  ‚Ä¢ Punto 9: Top 10 aerol√≠neas (GROUP BY + ORDER BY)         ‚îÇ
‚îÇ  ‚Ä¢ Punto 10: Top 10 aeronaves Buenos Aires                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìñ Gu√≠as de Uso

### Ejecuci√≥n Paso a Paso (Como se Ejecut√≥ en Consola)

#### PASO 1: Crear Tablas en Hive

```bash
# Terminal 1: Acceder al contenedor
docker exec -it edvai_hadoop bash
su hadoop

# Entrar a Hive
hive
```

Dentro de Hive CLI:

```sql
CREATE DATABASE IF NOT EXISTS aviacion;

USE aviacion;

-- Tabla 1: Vuelos (Schema seg√∫n PDF P√°gina 2)
CREATE TABLE IF NOT EXISTS aeropuerto_tabla (
    fecha DATE,
    horaUTC STRING,
    clase_de_vuelo STRING,
    clasificacion_de_vuelo STRING,
    tipo_de_movimiento STRING,
    aeropuerto STRING,
    origen_destino STRING,
    aerolinea_nombre STRING,
    aeronave STRING,
    pasajeros INT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
TBLPROPERTIES ("skip.header.line.count"="1");

-- Tabla 2: Detalles Aeropuertos (Schema seg√∫n PDF P√°gina 3)
CREATE TABLE IF NOT EXISTS aeropuerto_detalles_tabla (
    aeropuerto STRING,
    oac STRING,
    iata STRING,
    tipo STRING,
    denominacion STRING,
    coordenadas_latitud STRING,
    coordenadas_longitud STRING,
    elev FLOAT,
    uom_elev STRING,
    ref STRING,
    distancia_ref FLOAT,
    direccion_ref STRING,
    condicion STRING,
    control STRING,
    region STRING,
    uso STRING,
    trafico STRING,
    sna STRING,
    concesionado STRING,
    provincia STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
TBLPROPERTIES ("skip.header.line.count"="1");

SHOW TABLES;

exit;
```

#### PASO 2: Crear y Ejecutar Script de Ingest

```bash
# Terminal 2: Abrir otra terminal y acceder al contenedor
docker exec -it edvai_hadoop bash
su hadoop
cd /home/hadoop/scripts

# Crear el archivo con nano
nano ingest_aviacion.sh
```

**Copiar el contenido completo del script desde:**  
`ejercicios-Finales/ejercicio-1/scripts/ingest_aviacion.sh`

**Guardar y salir de nano:**
```
Ctrl + O ‚Üí Enter ‚Üí Ctrl + X
```

**Dar permisos y ejecutar:**

```bash
chmod +x ingest_aviacion.sh
bash ingest_aviacion.sh
```

**Salida esperada:**
```
==========================================
‚úàÔ∏è  AVIACI√ìN CIVIL - DESCARGA DE DATOS
==========================================
‚úÖ Directorio creado: /home/hadoop/landing
‚úÖ Descarga exitosa: 2021-informe-ministerio.csv
‚úÖ Descarga exitosa: 202206-informe-ministerio.csv
‚úÖ Descarga exitosa: aeropuertos_detalle.csv
‚úÖ Todos los archivos subidos
‚úÖ INGEST COMPLETADO EXITOSAMENTE
```

#### PASO 3: Crear y Ejecutar Script de Procesamiento Spark

```bash
# En la misma terminal (o abrir otra)
docker exec -it edvai_hadoop bash
su hadoop
cd /home/hadoop/scripts

# Crear el archivo con nano
nano process_aviacion_spark.py
```

**Copiar el contenido completo del script desde:**  
`ejercicios-Finales/ejercicio-1/scripts/process_aviacion_spark.py`

**Guardar y salir de nano:**
```
Ctrl + O ‚Üí Enter ‚Üí Ctrl + X
```

**Ejecutar con Spark:**

```bash
spark-submit process_aviacion_spark.py
```

**Salida esperada:**
```
============================================================
‚úàÔ∏è PROCESAMIENTO DE DATOS DE AVIACI√ìN CON PYSPARK
============================================================
‚úÖ Sesi√≥n de Spark creada exitosamente
‚úÖ Datos 2021 cargados: 115000 registros
‚úÖ Datos 2022 cargados: 28000 registros
‚úÖ Uni√≥n de datasets completada
‚úÖ Vuelos internacionales excluidos
‚úÖ Datos insertados en Hive
‚úÖ PROCESAMIENTO COMPLETADO EXITOSAMENTE
```

#### PASO 4: Crear DAG de Airflow

```bash
# Acceder al contenedor
docker exec -it edvai_hadoop bash
su hadoop
cd /home/hadoop/airflow/dags

# Crear DAG
nano aviacion_spark_dag.py
```

**Copiar el contenido completo del script desde:**  
`ejercicios-Finales/ejercicio-1/airflow/aviacion_spark_dag.py`

**Guardar:** `Ctrl + O ‚Üí Enter ‚Üí Ctrl + X`

**Reiniciar Airflow y ejecutar:**

```bash
# Reiniciar scheduler
pkill -f "airflow scheduler"
sleep 3
nohup airflow scheduler > /tmp/scheduler.log 2>&1 &

# Activar DAG
airflow dags unpause aviacion_processing_spark_dag

# Ejecutar DAG
airflow dags trigger aviacion_processing_spark_dag
```

#### PASO 5: Ejecutar Consultas de Negocio

```bash
# Entrar a Hive
hive

# Usar la base de datos
USE aviacion;

-- Punto 6: Vuelos entre 01/12/2021 y 31/01/2022
SELECT COUNT(*) as total_vuelos
FROM aeropuerto_tabla
WHERE fecha BETWEEN '2021-12-01' AND '2022-01-31';

-- Punto 7: Pasajeros de Aerol√≠neas Argentinas
SELECT SUM(pasajeros) as total_pasajeros
FROM aeropuerto_tabla
WHERE aerolinea_nombre = 'AEROLINEAS ARGENTINAS SA'
AND fecha BETWEEN '2021-01-01' AND '2022-06-30';

-- Ver archivo queries_aviacion.sql para m√°s consultas

exit;
```

## üéØ Resultados Esperados

### Datos Procesados
- **Total registros vuelos**: 143,000 (despu√©s de filtrar internacionales)
- **Registros aeropuertos**: 54
- **Vuelos internacionales excluidos**: 67,941
- **Valores NULL tratados**: Pasajeros, distancia_ref
- **Formato de fechas**: DD/MM/YYYY ‚Üí YYYY-MM-DD

### An√°lisis de Negocio

**Punto 6 - Vuelos Diciembre 2021 - Enero 2022**:
- Total: 57,984 vuelos

**Punto 7 - Pasajeros Aerol√≠neas Argentinas (2021-2022)**:
- Total: 7,484,860 pasajeros
- Representa ~70% del mercado

**Punto 9 - Top 3 Aerol√≠neas**:
1. AEROLINEAS ARGENTINAS SA: 7,484,860 pasajeros
2. JETSMART AIRLINES S.A.: 1,511,650 pasajeros
3. FB L√çNEAS A√âREAS - FLYBONDI: 1,482,473 pasajeros

**Punto 10 - Top 3 Aeronaves Buenos Aires**:
1. EMB-ERJ190100IGW: 12,470 despegues
2. CE-150-L: 8,117 despegues
3. CE-152: 7,980 despegues

## üìù Notas Importantes

### Correcciones Aplicadas al Script PySpark

**1. Normalizaci√≥n de Columnas con Par√©ntesis**

Problema: `Clase de Vuelo (todos los vuelos)` no se normalizaba correctamente.

```python
def normalizar_nombre_columna(nombre):
    nombre = nombre.lower()
    nombre = nombre.replace('√≥', 'o').replace('√≠', 'i')
    nombre = nombre.replace('√°', 'a').replace('√©', 'e')
    # CORRECCI√ìN: Eliminar par√©ntesis y su contenido
    nombre = re.sub(r'\s*\([^)]*\)', '', nombre)
    return nombre
```

**2. Mapeo de Columnas de Aeropuertos**

Problema: Las columnas reales no coincid√≠an con las esperadas.

```python
# Correcci√≥n aplicada:
df_aeropuertos_final = df_aeropuertos.select(
    col('local').alias('aeropuerto'),     # local ‚Üí aeropuerto
    col('oaci').alias('oac'),             # oaci ‚Üí oac
    col('latitud').alias('coordenadas_latitud'),
    col('longitud').alias('coordenadas_longitud'),
    # ... resto de columnas
)
```

### Diferencias: Pandas vs PySpark

| Operaci√≥n | Pandas | PySpark |
|-----------|--------|---------|
| Lectura CSV | `pd.read_csv()` | `spark.read.csv()` |
| Concatenar | `pd.concat([df1, df2])` | `df1.unionByName(df2)` |
| Filtrar | `df[df['col'] == val]` | `df.filter(col('col') == val)` |
| NULL a 0 | `df['col'].fillna(0)` | `when(col('col').isNull(), 0)` |
| Fechas | `pd.to_datetime()` | `to_date(col('fecha'), 'dd/MM/yyyy')` |
| Escribir Hive | `LOAD DATA` | `.write.saveAsTable()` |

### Configuraci√≥n de Entorno

```bash
# Variables de entorno necesarias
export HADOOP_HOME=/home/hadoop/hadoop
export SPARK_HOME=/home/hadoop/spark
export HIVE_HOME=/home/hadoop/hive
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export PATH=$SPARK_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$PATH
```

## üîß Troubleshooting

### Problema: "cannot resolve 'clase de vuelo'"
**Causa**: Funci√≥n normalizar no eliminaba par√©ntesis  
**Soluci√≥n**: Usar `re.sub(r'\s*\([^)]*\)', '', nombre)`

### Problema: "cannot resolve 'aeropuerto' (tabla aeropuertos)"
**Causa**: Columna real es `local`, no `aeropuerto`  
**Soluci√≥n**: Usar `col('local').alias('aeropuerto')`

### Problema: "DAG no aparece en Airflow"
**Causa**: Scheduler no detect√≥ cambios  
**Soluci√≥n**:
```bash
pkill -f "airflow scheduler"
nohup airflow scheduler > /tmp/scheduler.log 2>&1 &
sleep 10
airflow dags list | grep aviacion
```

### Problema: "java: command not found"
**Causa**: `JAVA_HOME` incorrecto  
**Soluci√≥n**:
```bash
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

## üîó Referencias

- [PySpark SQL Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)
- [Hive Language Manual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual)
- [Spark + Hive Integration](https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html)
- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Datos Abiertos Argentina - Aviaci√≥n](https://datos.gob.ar/lv/dataset/transporte-aterrizajes-despegues-procesados-por-administracion-nacional-aviacion-civil-anac)

## üìß Contacto

Para consultas sobre el pipeline de aviaci√≥n civil, contactar al equipo de Data Engineering de Edvai.

---

**Cliente**: Administraci√≥n Nacional de Aviaci√≥n Civil  
**Autor**: Data Engineering Team - Edvai  
**Fecha**: 2025-11-24  
**Versi√≥n**: 2.0 (PySpark - Sin Pandas)  
**Tecnolog√≠a Principal**: Apache Spark + Airflow + Hive
